# Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

## Abstract

Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.

迁移学习，在对下游任务进行微调之前，首先对模型进行数据丰富任务的预训练, 已经成为自然语言处理 (NLP) 中一种强大的技术。迁移学习的有效性导致了方法、方法和实践的多样性。在本文中，我们通过引入一个统一的框架来探索 NLP 迁移学习技术的前景，该框架将每个语言问题转换成文本到文本的格式。我们的系统研究比较了几十个语言理解任务的训练前目标、体系结构、未标记数据集、转移方法和其他因素。通过将我们探索的见解与规模和我们新的 “巨大的干净爬行语料库” 相结合，我们在涵盖总结、问答、文本分类等等。为了促进未来 NLP 迁移学习的工作，我们发布了我们的数据集、预训练模型和代码。

## 1 Introduction

Training a machine learning model to perform natural language processing (NLP) tasks often requires that the model can process text in a way that is amenable to downstream learning. This can be loosely viewed as developing general-purpose knowledge that allows the model to “understand” text. This knowledge can range from low-level (e.g. the spelling or meaning of words) to high-level (e.g. that a tuba is too large to fit in most backpacks). In modern machine learning practice, providing this knowledge is rarely done explicitly; instead, it is often “learned” as part of an auxiliary task. For example, a historically common approach is to use “word vectors” [Mikolov et al., 2013b,a; Pennington et al., 2014] to map word identities to a continuous representation where, ideally, similar words map to similar vectors. These vectors are often learned through an objective that, for example, encourages co-occurring words to be positioned nearby in the continuous space [Mikolov et al., 2013b].

训练一个机器学习模型来执行自然语言处理(NLP)任务通常需要该模型能够以一种适合下游学习的方式来处理文本。这可以粗略地看作是开发通用知识，允许模型“理解”文本。这些知识可以是低级的(如单词的拼写或含义)，也可以是高级的(如大号太大，大多数背包装不下)。在现代机器学习实践中，提供这些知识很少是明确的;相反，它经常作为辅助任务的一部分被“学习”。例如，历史上常用的方法是使用“词向量”[Mikolov et al.， 2013b,a;将单词标识映射到一个连续的表示，在理想情况下，相似的单词映射到相似的向量。这些向量通常是通过一个目标来学习的，例如，鼓励同时出现的单词被放置在连续空间附近[Mikolov et al.， 2013b]。

Recently, it has become increasingly common to pre-train the entire model on a data-rich task. Ideally, this pre-training causes the model to develop general-purpose abilities and knowledge that can then be “transferred” to downstream tasks. In applications of transfer learning to computer vision [Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014], pre-training is typically done via supervised learning on a large labeled dataset like ImageNet [Russakovsky et al., 2015; Deng et al., 2009]. In contrast, modern techniques for transfer learning in NLP often pre-train using unsupervised learning on unlabeled data. This approach has recently been used to obtain state-of-the-art results in many of the most common NLP benchmarks [Devlin et al., 2018; Yang
et al., 2019; Dong et al., 2019; Liu et al., 2019c; Lan et al., 2019]. Beyond its empirical strength, unsupervised pre-training for NLP is particularly attractive because unlabeled text data is available en masse thanks to the Internet – for example, the Common Crawl project2 produces about 20TB of text data extracted from web pages each month. This is a natural fit for neural networks, which have been shown to exhibit remarkable scalability, i.e. it is often possible to achieve better performance simply by training a larger model on a larger dataset [Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019; Shazeer et al., 2018, 2017; Huang et al., 2018b].

最近，在一个数据丰富的任务上对整个模型进行预培训变得越来越普遍。理想情况下，这种预训练会使模型发展出通用的能力和知识，然后可以“转移”到下游的任务中。转移学习在计算机视觉中的应用[Oquab等，2014;Jia等，2014;Huh等人，2016;Yosinski等人，2014]，通常是通过在ImageNet等大型标记数据集上的监督学习来完成预训练[Russakovsky等人，2015;邓等人，2009]。与此相反，在NLP中，现代的转移学习技术通常是对未标记数据进行无监督学习的预训练。这种方法最近被用于在许多最常见的NLP基准测试中获得最先进的结果[Devlin et al.， 2018;杨等人，2019年;董等人，2019年;刘等，2019c;Lan等人，2019年]。除了它的经验优势之外，无监督的NLP前培训特别有吸引力，因为未标记的文本数据可以通过互联网获得——例如，常见的爬网项目t2每个月从web页面提取20TB的文本数据。这对于神经网络来说是一种自然的适合，因为神经网络已经被证明具有显著的可扩展性，也就是说，通常可以通过在更大的数据集上训练更大的模型来获得更好的性能[Hestness et al.， 2017;Shazeer等人，2017;Jozefowicz等，2016;马哈詹等人，2018;Radford等人，2019年;Shazeer等人，2018年，2017年;黄等，2018b]。

This synergy has resulted in a great deal of recent work developing transfer learning methodology for NLP, which has produced a wide landscape of pre-training objectives [Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019], unlabeled datasets [Yang et al., 2019; Liu et al., 2019c; Zellers et al., 2019], benchmarks [Wang et al., 2019b, 2018; Conneau and Kiela, 2018], fine-tuning methods [Howard and Ruder, 2018; Houlsby et al., 2019; Peters et al., 2019], and more. The rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning. Motivated by a need for more rigorous understanding,
we present a unified approach to transfer learning that allows us to systematically study different approaches and push the current limits of the field.

这种协同作用导致了最近大量的工作，为NLP开发转移学习方法，产生了广泛的培训前目标景观[Howard和Ruder, 2018;Devlin et al.， 2018;杨等人，2019年;Dong等人，2019年]，未标记数据集[Yang等人，2019年;刘等，2019c;Zellers等人，2019年]，benchmark [Wang等人，2019b, 2018年;Conneau和Kiela, 2018]，微调方法[Howard和Ruder, 2018;Houlsby等人，2019年;彼得斯等人，2019年]，以及更多。在这个新兴领域中，快速的发展速度和技术的多样性使得比较不同的算法、梳理新贡献的效果和理解现有的转移学习方法的空间变得困难。出于对更严格理解的需要，我们提出了一个统一的方法来转移学习，使我们能够系统地研究不同的方法，并推动该领域目前的限制。

The basic idea underlying our work is to treat every NLP problem as a “text-to-text” problem, i.e. taking text as input and producing new text as output. Similar approaches were used as part of the Natural Language Decathlon [McCann et al., 2018] and to test the zero-shot learning capabilities of language models [Radford et al., 2019]. Crucially, our text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task we consider. We leverage this flexibility by evaluating performance on a wide variety of English-based NLP problems, including question answering, document summarization, and sentiment classification, to name a few. With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled datasets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and datasets beyond what has previously been considered. By combining our insights and scale, we obtain state-of-the-art results in many of the tasks we consider.

我们工作的基本思想是把每一个NLP问题当作一个“文本到文本”的问题，即以文本为输入，以生成新文本为输出。类似的方法被用作自然语言十项全能(Natural Language Decathlon, McCann et al.， 2018)的一部分，并被用来测试语言模型的零射击学习能力(zero-shot learning capability, Radford et al.， 2019)。至关重要的是，我们的文本到文本的框架允许我们直接将相同的模型、目标、训练过程和解码过程应用于我们考虑的每一项任务。我们利用这种灵活性来评估各种基于英语的NLP问题的性能，包括问题回答、文档摘要和情绪分类等。通过这种统一的方法，我们可以比较不同的迁移学习目标、未标记的数据集和其他因素的有效性，同时通过扩展模型和数据集来探索NLP迁移学习的极限。通过结合我们的洞察力和规模，我们在许多我们考虑的任务中获得了最先进的结果。

We emphasize that our main goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands. As such, the bulk of our work comprises of a survey, exploration, and empirical comparison of existing techniques. Our approach of casting every problem as a text-to-text task constitutes an additional major contribution. This unifying framework differs from current practice and boasts both simplicity and strong performance. Finally, we also push the field forward by training larger models than have been previously considered (up to 11 billion parameters). In order to perform experiments at this scale, we introduce the “Colossal Clean Crawled Corpus” (C4), a dataset consisting of hundreds of gigabytes of clean English text scraped from the web. Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we also contribute our code, datasets, and pre-trained models.

我们强调，我们的主要目标不是提出新的方法，而是就该领域的立场提供一个全面的观点。因此，我们的大部分工作包括对现有技术的调查、探索和经验比较。我们把每一个问题都当作一个文本到文本的任务的方法构成了另一个重要贡献。这个统一的框架不同于当前的实践，它具有简单性和强大的性能。最后，我们还通过训练比以前考虑的更大的模型(多达110亿个参数)来推动该领域的发展。为了在这个规模上进行实验，我们引入了“巨大的干净抓取语料库”(C4)，这是一个由从网上抓取的几百gb的干净英文文本组成的数据集。认识到转移学习的主要用途是在数据缺乏的情况下利用预训练模型的可能性，我们也提供了我们的代码、数据集和预训练模型。

The remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every problem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a large set of experiments that explore the field of transfer learning for NLP. At the end of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks. Finally, we provide a summary of our results and wrap up with a look towards the future in Section 4.

本文的其余部分的结构如下:在下一节中，我们将讨论基本模型及其实现，将每个问题形成为文本到文本任务的过程，以及我们所考虑的任务集。在第3节中，我们提出了一组大型的实验来探索NLP的转移学习领域。在本节的最后(第3.7节)，我们结合我们的系统研究的见解，在各种基准上获得最新的结果。最后，我们对结果进行总结，并在第4节中展望未来。

## 2 Setup

Before presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on. We also introduce our approach for treating every problem as a text-to-text task and describe our “Colossal Clean Crawled Corpus” (C4), the Common Crawl-based dataset we created as a source of unlabeled text data. We refer to our model and framework as the “Text-to-Text Transfer Transformer” (T5).

在展示我们大规模实证研究的结果之前，我们回顾了理解我们的结果所需的必要的背景主题，包括Transformer模型架构和我们评估的下游任务。我们还介绍了将每个问题作为文本到文本任务处理的方法，并描述了我们的“巨大的、干净的爬行语料库”(C4)，这是我们创建的通用的基于爬虫的数据集，作为未标记文本数据的来源。我们将我们的模型和框架称为“文本到文本传输转换器”(T5)。

### 2.1 Model

Early results on transfer learning for NLP leveraged recurrent neural networks [Peters et al., 2018; Howard and Ruder, 2018], but it has recently become more common to use models based on the “Transformer” architecture [Vaswani et al., 2017]. The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings [Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018]. Due to its improved performance and increasing ubiquity, all of the models we study are based on the Transformer architecture. Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed. Instead of providing a comprehensive
definition of this model, we refer the interested reader to the original paper [Vaswani et al., 2017] or follow-up tutorials for a more detailed introduction.

NLP杠杆递归神经网络转移学习的早期结果[Peters et al.， 2018;但是最近使用基于“Transformer”架构的模型变得更加普遍[Vaswani et al.， 2017]。该变压器最初被证明是有效的机器翻译，但它后来被用于各种NLP设置[Radford et al.， 2018;Devlin et al.， 2018;麦肯等，2018;Yu等人，2018]。由于其改进的性能和增加的普遍性，我们研究的所有模型都基于Transformer架构。除了下面提到的细节和我们在3.2节中探讨的变体之外，我们并没有明显偏离最初提出的体系结构。我们没有提供该模型的全面定义，而是建议感兴趣的读者参考原始论文[Vaswani et al.， 2017]或后续教程以获得更详细的介绍。

The primary building block of the Transformer is self-attention [Cheng et al., 2016]. Self-attention is a variant of attention [Graves, 2013; Bahdanau et al., 2015] that processes a sequence by replacing each element by a weighted average of the rest of the sequence. The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence [Sutskever et al., 2014; Kalchbrenner et al., 2014] tasks. It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling [Radford et al., 2018; Al-Rfou et al., 2019] or classification and span prediction tasks [Devlin et al., 2018; Yang et al., 2019]. We empirically explore these architectural variants in Section 3.2.

变压器的主要构件是self-attention [Cheng et al.， 2016]。自我关注是注意力的一种变体[Graves, 2013;它通过用序列其余部分的加权平均值替换每个元素来处理序列。最初的Transformer由一个编码器-解码器架构组成，用于从序列到序列[Sutskever et al.， 2014;Kalchbrenner等人，2014]任务。最近，使用由单个Transformer层堆栈组成的模型也变得很普遍，使用不同形式的自我关注来生成适合于语言建模的体系结构[Radford et al.， 2018;或分类和跨度预测任务[Devlin et al.， 2018;Yang等人，2019年]。我们在3.2节中对这些架构变体进行了实证研究。

Overall, our encoder-decoder Transformer implementation closely follows its originally-proposed form [Vaswani et al., 2017]. First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of “blocks”, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization [Ba et al., 2016] is applied to the input of each subcomponent and a residual skip connection [He et al., 2016] adds each subcomponent’s input to its output. Dropout [Srivastava et al., 2014] is applied within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack. The decoder is similar in structure to the encoder except that it includes a standard attention mechanism after each self-attention layer that attends to the output of the encoder. The self-attention mechanism in the decoder also uses a form of autoregressive or causal self-attention, which only allows the model to attend to past outputs. The output of the final decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix. All attention mechanisms in the Transformer are split up into independent “heads” whose outputs are concatenated before being further processed.

总的来说，我们的编码器-解码器转换器的实现非常接近于其最初提出的形式[Vaswani et al.， 2017]。首先，将标记的输入序列映射到嵌入序列，然后将其传递到编码器。编码器由一组“块”组成，每个“块”由两个子组件组成:一个自我注意层和一个小的前馈网络。层归一化[Ba et al.， 2016]应用于每个子组件的输入，剩余的跳跃连接[He et al.， 2016]将每个子组件的输入添加到其输出中。Dropout [Srivastava et al.， 2014]应用于前馈网络、跳跃连接、注意权重，以及整个堆栈的输入和输出。解码器在结构上与编码器相似，除了它在每个关注编码器输出的自我关注层之后包括一个标准的注意机制。解码器中的自我注意机制也使用一种形式的自回归或因果自我注意，这只允许模型注意过去的输出。最后的解码器块的输出被送入一个具有softmax输出的密集层，其权值与输入嵌入矩阵共享。转换器中的所有注意机制被分割成独立的“头”，这些“头”的输出在进一步处理之前被连接起来。

As part of our study, we experiment with the scalability of these models, i.e. how their performance changes as they are made to have more parameters or layers. Training large models can be non-trivial since they might not fit on a single machine and require a great deal of computation. As a result, we use a combination of model and data parallelism and train models on “slices” of Cloud TPU Pods.5 TPU pods are are multi-rack ML supercomputers that contain 1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with supporting CPU host machines. We leverage the Mesh TensorFlow library [Shazeer et al., 2018] for ease of implementation of both model parallelism and data parallelism [Krizhevsky, 2014].

作为我们研究的一部分，我们对这些模型的可伸缩性进行了实验，即当它们具有更多的参数或层时，它们的性能如何变化。训练大型模型可能不是件小事，因为它们可能不适用于一台机器，并且需要大量的计算。因此，我们将模型和数据并行性结合起来，并在云TPU泡头的“切片”上建立训练模型。5个TPU泡头是包含1024个TPU v3芯片的多机架超级计算机，它们通过高速2D网格与支持CPU主机互连。我们利用Mesh TensorFlow库[Shazeer et al.， 2018]来简化模型并行化和数据并行化的实现[Krizhevsky, 2014]。

