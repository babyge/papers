# The Ideal Versus the Real: Revisiting the History of Virtual Machines and Containers

The common perception in both academic literature and industry today is that virtual machines offer better security, whereas containers offer better performance. However, a detailed review of the history of these technologies and the current threats they face reveals a different story. This survey covers key developments in the evolution of virtual machines and containers from the 1950s to today, with an emphasis on countering modern misperceptions with accurate historical details and providing a solid foundation for ongoing research into the future of secure isolation for multitenant infrastructures, such as cloud and container deployments.

当今学术文献和行业的普遍看法是，虚拟机提供了更好的安全性，而容器提供了更好的性能。然而，对这些技术的历史和它们面临的当前威胁的详细回顾揭示了一个不同的故事。这项调查涵盖了从 20世纪50年代中期到今天虚拟机和容器演变的关键进展,重点是用准确的历史细节来对抗现代的误解，并为多租户基础设施 (如云和容器部署) 的安全隔离未来的持续研究提供坚实的基础。

## 1 INTRODUCTION

Many modern computing workloads run in multitenant environments, where each physical machine is split into hundreds or thousands of smaller units of computing, generically called guests. Cloud and containers are currently the leading approaches to implementing multitenant environments. The guests in a cloud deployment are commonly called virtual machines or cloud instances, whereas the guests in a container deployment are commonly called containers. Typically, a single tenant (a user or group of users) is granted access to deploy guests in an orchestrated fashion across a cloud or cluster made up of hundreds or thousands of physical machines located in the same data center or across multiple data centers, to facilitate operational flexibility in areas such as capacity planning, resiliency, and reliable performance under variable load. Each guest runs its own (often minimal) operating system and application workloads, and maintains the illusion of being a physical machine, both to the end users who interact with the services running in the guests, and to developers who are able to build those services using familiar abstractions, such as programming languages, libraries, and operating system features. The illusion, however, is not perfect, because ultimately the guests do share the hardware resources (CPU, memory, cache, devices) of the underlying physical host machine, and consequently also have greater access to the host’s privileged software (kernel, operating system) than a physically distinct machine would have.

许多现代计算工作负载在多租户环境中运行，其中每个物理机器被分割成数百或数千个更小的计算单元，通常称为来宾计算单元。云和容器是当前实现多租户环境的主要方法。云部署中的客户机通常称为虚拟机或云实例，而容器部署中的客户机通常称为容器。通常情况下,一个租户(用户)的用户或组授予访问跨云部署客人精心策划的方式或集群由成百上千的物理机器位于相同的数据中心或跨多个数据中心,便于操作的灵活性在容量规划等领域,弹性、可靠变量负荷条件下的性能。每个客人运行自己的(通常是最小的)操作系统和应用程序工作负载,并维护作为一个物理机器的错觉,对最终用户与服务交互中运行的客人,和开发人员能够使用熟悉的抽象,构建这些服务编程语言、库和操作系统等功能。然而，这种错觉并不完美，因为最终来宾确实共享底层物理主机的硬件资源(CPU、内存、缓存、设备)，因此对主机的特权软件(内核、操作系统)的访问权限也比物理上不同的机器要大。

Ideally, multitenant environments would offer strong isolation of the guest from the host, and between guests on the same host, but reality falls short of the ideal. The approaches that various implementations have taken to isolating guests have different strengths and weaknesses. For example, containers share a kernel with the host, whereas virtual machines may run as a process in the host operating system or a module in the host kernel, so they expose different attack surfaces through different code paths in the host operating system. Fundamentally, however, all existing implementations of virtual machines and containers are leaky abstractions, exposing more of the underlying software and hardware than is necessary, useful, or desirable. New security research in 2018 delivered a further blow to the ideal of isolation in multitenant environments, demonstrating that certain hardware vulnerabilities related to speculative execution—including Spectre, Meltdown, Foreshadow, L1TF, and variants—can easily bypass the software isolation of guests.

理想情况下，多租户环境将提供客户机与主机以及同一主机上的客户机之间的强隔离，但现实情况并不理想。各种实现用来隔离来宾的方法有不同的优点和缺点。例如，容器与主机共享一个内核，而虚拟机可以作为主机操作系统中的进程或主机内核中的模块运行，因此它们通过主机操作系统中的不同代码路径暴露不同的攻击面。然而，从根本上说，所有现有的虚拟机和容器实现都是有漏洞的抽象，暴露的底层软件和硬件比必要的、有用的或需要的要多。2018年的新安全研究进一步打击了在多租户环境中隔离的理想，表明与投机执行相关的某些硬件漏洞——包括Spectre、Meltdown、Foreshadow、L1TF和变体——可以很容易地绕过来宾的软件隔离。

Because multitenancy has proven to be useful and profitable for a large sector of the computing industry, it is likely that a significant percentage of computing workloads will continue to run in multitenant environments for the foreseeable future. This is not a matter of naïveté but of pragmatism: these days, the companies who provide and make use of multitenant environments are generally fully aware of the security risks, but they do so anyway because the benefits—such as flexibility, resiliency, reliability, performance, cost, or any of a dozen other factors—outweigh the risks for their particular use cases and business needs. That being the case, it is worthwhile to take a step back and examine how the past 60 years of evolution led to the current tension between secure ideals and flawed reality, and what lessons from the past might help us build more secure software and hardware for the next 60 years.

由于多租户已被证明对计算行业的大部分部门有用且有利可图，因此在可预见的未来，很可能很大一部分计算工作负载将继续在多租户环境中运行。这不是幼稚的问题，而是实用主义的问题: 如今，提供和利用多租户环境的公司通常充分意识到安全风险,但是他们这样做是因为好处 -- 比如灵活性、弹性、可靠性、性能、成本,或其他十几个因素中的任何一个-超过了其特定用例和业务需求的风险。在这种情况下，值得后退一步，审视过去 60 年的演变是如何导致当前安全理想和有缺陷的现实之间的紧张关系的,以及过去的经验教训可能会帮助我们为未来 60 年构建更安全的软件和硬件。

This survey is divided into sections following the evolutionary paths of the technologies behind virtual machines and containers, generally in chronological order, as illustrated in Figure 1. Section 3 explores the common origins of virtual machines and containers in the late 1950s and early 1960s, driven by the architectural shift toward multitasking and multiprocessing, and motivated by a desire to securely isolate processes, efficiently utilize shared resources, improve portability, and minimize complexity. Section 4 examines the first virtual machines in the mid-1960s to 1970s, which primarily aimed to improve resource utilization in time-sharing systems. Section 5 delves into the capability systems of the early 1960s to 1970s—the precursors of modern containers—which evolved along a parallel track to virtual machines, with similar motivations but different implementations. Section 6 outlines the resurgence of virtual machines in the late 1990s and 2000s. Section 7 traces the emergence of containers in the 2000s and 2010s. Section 8 investigates the impact of recent security research on both virtual machines and containers. Section 9 briefly looks at the relationship between virtual machines and containers and the related terms cloud, serverless, and unikernels.

本调查按照虚拟机和容器背后的技术进化路径分为几部分，通常按时间顺序进行，如图 1 所示。第三节探讨了 20世纪50年代末和 20世纪60年代初中期虚拟机和容器的共同起源，由面向多任务和多处理的架构转变驱动,并出于安全隔离流程、高效利用共享资源、提高可移植性和最大限度地减少复杂性的愿望。第 4 节检查了 20世纪60年代中期到 20世纪70年代中期的第一批虚拟机，这些虚拟机主要旨在提高分时系统中的资源利用率。第五节深入研究了 20世纪60年代初到 20世纪70年代的能力体系 -- 现代容器的前身 -- 它沿着虚拟机的平行轨道发展，有着相似的动机，但实现方式不同。第 6 节概述了 20世纪90年代末和 21世纪00年代中期虚拟机的复苏。第七节追溯了 21世纪00年代和 21世纪10年代容器的出现。第 8 节调查了最近的安全研究对虚拟机和容器的影响。第 9 节简要介绍了虚拟机和容器之间的关系以及相关术语 cloud 、 serverless 和 unikernel。

## 2 TERMINOLOGY

For the sake of clarity, this survey consistently uses certain modern or common terms, even when
discussing literature that used various other terms for the same concepts:

- Container: The term container does not have a single origin, but some early relevant examples of use are Banga et al. [25] in 1999, Lottiaux and Morin [127] in 2001, Morin et al. [145] in 2002, and Price and Tucker [164] in 2004. Early literature on containers confusingly referred to them as a kind of virtualization [45, 48, 104, 142, 164, 182], or even called them virtual machines [182]. As containers grew more popular, the confusion shifted to virtual machines being called containers [37, 220]. This survey uses the term container for multitenant deployment techniques involving process isolation on a shared kernel (in contrast with virtual machine, as defined in the following). However, in practice, the distinction between containers and virtual machines is more of a spectrum than a binary divide. Techniques common to one can be effectively applied to the other, such as using system call filtering with containers, or using seccomp sandboxing or user namespaces with virtual machines.
- Container: Container这个词的起源并不是单一的，但早期使用的相关例子有Banga等人1999年的[25]，2001年的Lottiaux和Morin[127]， 2002年的Morin等人[145]，以及2004年的Price和Tucker[164]。早期关于容器的文献令人困惑地将其称为一种虚拟化[45、48、104、142、164、182]，甚至称其为虚拟机[182]。随着容器越来越流行，混淆转移到被称为容器的虚拟机上[37,220]。本调查使用术语容器来表示涉及共享内核(与下面定义的虚拟机相反)上的进程隔离的多租户部署技术。然而，在实践中，容器和虚拟机之间的区别更多的是频谱，而不是二进制划分。其中一种常见的技术可以有效地应用于另一种，例如对容器使用系统调用过滤，或对虚拟机使用seccomp沙箱或用户名称空间。
- Complexity: There are many dimensions to complexity in computing, but in the context of
multitenant infrastructures, some uniquely relevant dimensions are keeping each guest, the
interactions between guests, and the host’s management of the guests as small and simple
as possible. The implementation technique of isolation supports minimizing complexity by
restricting access to internal knowledge of the guests and host, and providing well-defined
interfaces to reduce the complexity of interactions between them.
- 复杂性:计算的复杂性有很多方面，但是在多租户基础设施的上下文中，一些唯一相关的方面使每个客户机、客户机之间的交互以及主机对客户机的管理尽可能地小和简单。隔离的实现技术通过限制对来宾和主机的内部知识的访问，并提供定义良好的接口来降低它们之间交互的复杂性，从而支持最小化复杂性。
- Guest: The term guest had some early usage in the 1980s for the operating system image running inside a virtual machine [147] but was not common until the early 2000s [26, 197]. This survey uses guest as a general term for operating system images hosted on multitenant infrastructures but occasionally distinguishes between virtual machine guests and container guests.
- Guest:在20世纪80年代，Guest一词最初用于虚拟机中运行的操作系统映像[147]，但直到21世纪初才普遍使用[26,197]。本调查使用guest作为多租户基础设施上托管的操作系统映像的通用术语，但偶尔会区分虚拟机客户机和容器客户机。
- Kernel: A variety of different terms appear in the early literature, including supervisory program [52], supervisor program [20], control program [15, 149, 153], coordinating program [153], nucleus [1, 43], monitor [209], and ultimately kernel around the mid-1970s [123, 161]. This survey uses the modern term kernel.
- Kernel:早期文献中出现了各种不同的术语，包括监督程序[52]、监督程序[20]、控制程序[15,149,153]、协调程序[153]、nucleus[1,43]、monitor[209]，并最终在20世纪70年代中期出现Kernel[123,161]。本调查使用了现代术语内核。
- Performance: There are many dimensions to performance in computing, but in the context of multitenant infrastructures, some uniquely relevant dimensions are the performance impact of added layers of abstraction separating the guest application workload from the host, balanced against the performance benefits of sharing resources between guests and reducing wasted resources from unused capacity. At the level of a single machine, this involves running multiple guests on the samemachine at the same time, with potential for intelligent, dynamic scheduling to extract more work from the same resource pool. Across multiple machines, this involves a larger pool of shared resources, more flexibility to balance work, and options for heterogenous hardware with resource-affinity configurations (e.g., a mixture of some CPU-heavy machines and some storage-heavy machines, with workload allocation determined by resource needs). The implementation technique of breaking down machines into smaller guests and their resources into smaller, sharable units, supports performance by allowing finer-grained and distributed control over resource management.
- 性能:在计算表现是多方面的,但在多租户基础设施的环境中,一些独特的相关维度添加抽象层分离的性能影响客人从主机应用程序工作负载,平衡性能优势客人之间的共享资源,减少资源浪费的未使用的容量。在单个机器的级别上，这涉及到在同一台机器上同时运行多个客户机，从而实现智能的动态调度，从而从同一资源池中提取更多的工作。在多台机器上，这涉及到更大的共享资源池、更灵活地平衡工作以及具有资源亲缘配置的异构硬件的选项(例如，混合使用一些cpu重的机器和一些存储重的机器，工作负载分配由资源需求决定)。通过允许对资源管理进行更细粒度的分布式控制，将机器分解为更小的客户机及其资源分解为更小的、可共享的单元的实现技术支持性能。
- Portability: There are many dimensions to portability in computing, but in the context of multitenant infrastructures, some uniquely relevant dimensions are developing guests in a standardized way—without any special knowledge of the environment where they will be deployed—and abstracting deployment andmanagement across physicalmachines, limiting dependence on low-level hardware details. For example, a container guest can be deployed anywhere in the cluster, or a virtual machine guest can be deployed on any compute machine in the cloud. The implementation techniques of standardizing interfaces so guests are substitutable and hiding implementation and hardware details behind well-defined interfaces both support portability.
- 可移植性:有很多维度在计算可移植性,但在多租户基础设施的环境中,一些独特的相关维度开发客人以标准化不任何特殊的环境中,他们将部署和抽象的知识在物理机器上部署和管理,限制依赖底层硬件细节。例如，容器客户机可以部署在集群中的任何位置，或者虚拟机客户机可以部署在云中的任何计算机上。标准化接口以使来宾具有可替代性的实现技术以及将实现和硬件细节隐藏在定义良好的接口之后的实现技术都支持可移植性。
- Process: The early literature tended to use the terms job [171] or program [20, 52, 153], and process only appeared around the mid-1960s [14, 65]. This survey uses the modern term process. The early use of multiprogramming meaning “multiprocessing” was derived from the early use of program meaning “process.”
- Process:早期文献倾向于使用job[171]或program[20,52,153]等术语，Process大约是在20世纪60年代中期才出现[14,65]。这个调查使用了现代术语过程。早期使用的多道程序设计的意思是“多处理”，源于早期使用的程序意思是“处理”。
- Security: There are many dimensions to security in computing, but in the context of multitenant infrastructures, some uniquely relevant dimensions are limiting access between guests, from guests to the host, and from the host to the guests. The implementation technique of isolation supports security, at both the software level and the hardware level, by reducing the likelihood of a breach and limiting the scope of damage when a breach occurs.
- 安全性:计算中的安全性有很多方面，但是在多租户基础设施的上下文中，一些唯一相关的维度限制来宾之间、来宾到主机之间以及主机到来宾之间的访问。隔离的实现技术在软件级和硬件级都支持安全性，它减少了破坏的可能性，并在发生破坏时限制了破坏的范围。
- Virtual machine: This survey uses the term virtual machine for multitenant deployment techniques involving the replication/emulation of real hardware architectures in software (in contrast with container, as defined earlier). The code responsible for managing virtual machine guests on a physical host machine is often called a hypervisor or virtual machine monitor, both derived from two early terms for the kernel, supervisor and monitor. In many early implementations of virtual machines, the host kernel managed both guests and ordinary processes.
- 虚拟机:本调查使用了术语虚拟机用于多租户部署技术，包括在软件中复制/模拟真实的硬件架构(与前面定义的容器相比)。负责管理物理主机上的虚拟机客户机的代码通常称为hypervisor或虚拟机监视器，它们都源自于内核的两个早期术语:supervisor和monitor。在许多虚拟机的早期实现中，主机内核同时管理来宾进程和普通进程。

## 3 COMMON ORIGINS

The origins of both virtual machines and containers can be traced to a fundamental shift in hardware and software architectures toward the late 1950s. The hardware of the time introduced the concept of multiprogramming, which included both basic multitasking in the form of simple context-switching and basic multiprocessing in the form of dedicated I/O processors and multiple CPUs. Codd [51] attributed the earliest known use of the term multiprogramming to Rochester [171] in 1955, describing the ability of an IBM 705 system to interrupt an I/O process (tape read), run a process (calculation) on the data found, and then return to the I/O process. The concept of multiprogramming evolved over the remainder of the decade through work on the EDSAC [211], UNIVAC LARC [70], STRETCH (IBM 7030) [52, 69], TX-2 [77], and an influential and comprehensive review by Gill [82]. Key trade-offs discussed in the literature on multiprogramming—around security, performance, portability, and complexity—continue to echo through modern literature on virtual machines and containers.

虚拟机和容器的起源可以追溯到上世纪50年代后期硬件和软件体系结构的根本转变。当时的硬件引入了多程序设计的概念，包括以简单上下文切换形式的基本多任务处理和以专用I/O处理器和多个cpu形式的基本多处理。Codd[51]将多程序设计这一术语最早的使用归因于1955年的Rochester[171]，它描述了IBM 705系统中断I/O进程(磁带读取)，对发现的数据运行进程(计算)，然后返回到I/O进程的能力。通过EDSAC[211]、UNIVAC LARC[70]、STRETCH (IBM 7030)[52,69]、TX-2[77]和Gill[82]一篇有影响力的全面综述，多程序设计的概念在接下来的十年中不断发展。关于多编程的文献中讨论的关键权衡——围绕安全性、性能、可移植性和复杂性——在虚拟机和容器方面的现代文献中仍然存在。

### 3.1 Security

Multiprogramming increased the complexity of the system software—due to simultaneous and interleaved processes interacting with other processes and shared hardware resources—and also increased the consequences of misbehaving system software—since any process had the potential to disrupt any other process on the same machine. Codd et al. [52] discussed secure isolation as a requirement for “noninterference” between processes regarding errors, in the core design principles for STRETCH. Codd [51] later expanded on the requirement as a need to prevent processes from making “accidental or fraudulent” changes to another process. Buzen and Gagliardi [43] called out the risk of one process modifying memory allocated to other processes or privileged system operations.

多程序设计增加了系统软件的复杂性——由于同时和交错的进程与其他进程交互并共享硬件资源——也增加了系统软件行为不正常的后果——因为任何进程都有可能破坏同一台机器上的任何其他进程。Codd等人[52]在《STRETCH的核心设计原则》中讨论了安全隔离作为进程之间关于错误的“不干涉”的要求。Codd[51]后来扩展了这一需求，将其作为防止流程对另一个流程进行“意外或欺诈性”更改的需要。Buzen和Gagliardi[43]指出了一个进程修改分配给其他进程的内存或特权系统操作的风险。

In response to the increase in complexity and risk, system software of the time introduced a familiar form of isolation, granting a small privileged kernel of system software unrestricted access to all hardware resources and running processes, as well as responsibility for potentially disruptive operations such as memory and storage allocation, process scheduling, and interrupt handling while restricting access to such features from any software outside the kernel. Codd et al. [52] described the structure and function of the STRETCH kernel in detail, including concurrency, interrupts, memory protection, and time limits (an early form of resource usage control). Amdahl et al. [20] touched on the separation of the kernel in the IBM System/360, including appendices of relevant opcodes and protected storage locations. Opler and Baird [153] weighed trade-offs around having the kernel take responsibility for coordinating the parallel operation of processes and judged the approach to have potential to improve portability of programs not written for parallel operation, as well as potential to minimize complexity for programmers who would no longer be responsible to manually coordinate the parallel operation of each program.

为了应对复杂性和风险的增加,系统软件的时间介绍一种熟悉的隔离,给予一个小特权内核的系统软件不受限制地访问所有硬件资源和运行流程,以及负责潜在破坏性的操作,比如内存和存储分配,进程调度和中断处理而限制访问这些特性从内核之外的任何软件。Codd等人[52]详细描述了扩展内核的结构和功能，包括并发性、中断、内存保护和时间限制(资源使用控制的早期形式)。Amdahl等人[20]谈到了IBM System/360内核的分离，包括相关操作码的附录和受保护的存储位置。奥普莱和Baird[153]权衡取舍的内核负责协调并行操作的流程和评价程序的方法有潜力提高可移植性不写给并行操作,以及潜在的减少复杂性的程序员将不再负责手工协调每个程序的并行操作。

### 3.2 Performance

One of the fundamental goals of adding multiprogramming to hardware and operating systems in the late 1950s was to improve performance through more efficient utilization of available resources by sharing them across parallel processes. Codd et al. [52] described performance as a requirement for “noninterference” between processes regarding “undue delay.” Opler and Baird [153] explored the trade-offs between the performance advantages of increasing utilization through multiprocessing, versus the increased complexity of developing for such systems. Codd [49, 50] published two further papers in 1960 about performance considerations for process scheduling algorithms in multiprogramming. Amdahl et al. [20, p. 89] explored the trade-offs between performance and portability in the architecture design of the IBM System/360. Dennis [64, p. 590] noted the performance advantages of dynamic memory allocation for multiprogramming.

在20世纪50年代后期，将多道程序设计添加到硬件和操作系统的一个基本目标是通过在并行进程之间共享可用资源，从而更有效地利用这些资源来提高性能。Codd等人将性能描述为进程之间关于“不当延迟”的“不干涉”要求。Opler和Baird[153]探索了通过多处理提高利用的性能优势与为这类系统增加开发复杂性之间的权衡。Codd[49,50]在1960年发表了另外两篇关于多道规划中进程调度算法性能考虑的论文。Amdahl等人[20,p. 89]在IBM System/360的架构设计中探索了性能和可移植性之间的权衡。Dennis [64, p. 590]指出了多程序设计的动态内存分配的性能优势。

### 3.3 Portability

In the 1950s, it was common for specialized system software to be developed for each new model of hardware, requiring programs to be rewritten to run on even closely related machines. As the system software and programs grew larger and more complex, the porting effort grew more costly, motivating a desire for programs to be portable across differentmachines. Codd et al. [52] discussed portability as a requirement for “independence of preparation” and “flexible allocation of space and time.” Amdahl et al. [20, p. 97] emphasized portability as one of the primary design goals of the IBM System/360, specifically allowing machine-language programs to run unmodified across six different hardware models, with a variety of different configurations of peripheral devices. Buzen and Gagliardi [43] noted that the introduction of a privileged kernel compounded the problem of portability, since a program might have to be rewritten to run on two different kernels, even when the underlying hardware was compatible or completely identical.

在20世纪50年代，为每一种新的硬件模型开发专门的系统软件是很普遍的，要求程序重写才能在甚至是紧密相关的机器上运行。随着系统软件和程序变得越来越大、越来越复杂，移植工作的成本也越来越高，这就激发了在不同机器之间移植程序的愿望。Codd等人[52]将可移植性作为“独立准备”和“灵活分配空间和时间”的要求进行了讨论。Amdahl等人[20,p. 97]强调可移植性是IBM System/360的主要设计目标之一，特别是允许机器语言程序在六种不同的硬件模型上不加修改地运行，具有各种不同配置的外围设备。Buzen和Gagliardi[43]指出，特权内核的引入加剧了可移植性问题，因为一个程序可能必须重写才能在两个不同的内核上运行，即使底层硬件是兼容的或完全相同的。

### 3.4 Minimizing Complexity

Another early realization after the introduction of multiprogramming was that it was unreasonable to expect the developer of each process to directly manage all of the complexity of interacting with every other process running on the machine, so the privileged kernel approach had the advantage of allowing processes to maintain a more minimal focus on their own internals. Codd et al. [52] described minimizing complexity as a requirement for “minimum information from programmer.” Nearly a decade before Rushby [173] first wrote about the idea of a Trusted Computing Base, Buzen and Gagliardi [43, p. 291] argued for minimizing complexity within the privileged kernel, noting that such separation was effective when the privileged code base was kept small, so it could be maintained in a relatively stable state, with limited changes over time, by a few expert developers.

引入后的另一个早日实现多道程序设计是不合理的期望每个过程的开发人员直接管理所有的复杂性与其他交互过程的计算机上运行,所以有特权的内核方法允许进程保持更多的最小的优势集中在他们自己的内部。Codd等人[52]将复杂性最小化描述为“来自程序员的最小信息”的要求。“近十年之前Rushby[173]首先写一个可信计算基础的想法,Buzen和Gagliardi [43, p . 291]主张减少复杂性在特权内核,指出这种分离是有效的特权代码库时保持小,所以它可以保持在一个相对稳定的状态,有限的变化随着时间的推移,一些专家开发人员。

## 4 EARLY VIRTUAL MACHINES

The early work on virtual machines grew directly out of the work on multiprogramming, continuing the goal of safely sharing the resources of a physical machine across multiple processes. Initially, the idea was no more than a refinement on memory protection between processes, but it expanded into a much bigger idea: that small isolated bundles of shared resources from the host machine could present the illusion of being a physical machine running a full operating system.

虚拟机的早期工作直接从多编程工作发展而来，继续实现跨多个进程安全共享物理机器资源的目标。最初，这个想法只不过是对进程之间的内存保护进行了改进，但它扩展成了一个更大的想法:来自主机的共享资源的小的、独立的bundle可能会给人一种运行完整操作系统的物理机器的错觉。

### 4.1 M44/44X

In 1964, Nelson [149] published an internal research report at IBM outlining plans for an experimental machine based on the IBM 7044, called the M44. The project built on earlier work in multiprogramming, improving process isolation and scheduling in the privileged kernel with an early form of virtualmemory. They called the memory mapped for a particular process a virtualmachine [149, p. 14]. The 44X part of the name stood for the virtual machines (also based on the IBM 7044) running on top of the M44 host machine.

1964年，Nelson[149]发表了一份IBM内部研究报告，概述了基于IBM 7044的实验机器的计划，该机器被称为M44。该项目建立在多编程早期工作的基础上，使用早期形式的虚拟内存改进了特权内核中的进程隔离和调度。他们将映射到特定进程的内存称为虚拟机[149，第14页]。名称中的44X部分代表运行在M44主机上的虚拟机(也是基于IBM 7044)。

Nelson [149, pp. 4–6] identified the performance advantages of dynamically allocated shared resources (especially memory and CPU) as one of the primary motivators for the M44/44X experiments. Portability was another central consideration, allowing software to run unmodified across single process, multiprocess, and debugging contexts [149, pp. 9–10].

Nelson[149，第4-6页]指出，动态分配共享资源(特别是内存和CPU)的性能优势是M44/44X实验的主要动因之一。可移植性是另一个需要考虑的核心问题，允许软件无需修改就可以跨单进程、多进程和调试上下文运行[149，第9-10页]。

The M44/44X lacked almost all of the features we would associate with virtual machines today, but it played an important, although largely forgotten, part in the history of virtual machines. Denning [63] reflected that the M44/44X was central to significant theoretical and experimental advances in memory research around paging, segmentation, and virtual memory in the 1960s.

M44/44X几乎缺少我们今天与虚拟机相关的所有特性，但它在虚拟机的历史上扮演了一个重要的角色，尽管它在很大程度上被遗忘了。Denning[63]认为，M44/44X是20世纪60年代关于分页、分段和虚拟内存研究中重要的理论和实验进展的核心。

### 4.2 Cambridge Monitor System

The IBM System/360 was explicitly designed for portability of software across different models and different hardware configurations [20]. In themid-1960s, IBM’s Control Program-40 Cambridge Monitor System (CP-40/CMS) project running on a modified IBM System/360 (model 40) took the idea a few steps further—initially calling the work a pseudo machine but later adopting the term virtual machine [61, p. 485]. The CP-40/CMS and later CP-67/CMS1 projects improved on earlier approaches to portability, making it possible for software written for a bare metal machine to run unmodified in a virtual machine, which could simulate the appearance of various different hardware configurations [15, pp. 1–2]. It also improved isolation by introducing privilege separation for interrupts [15, pp. 6–7], paged memory within virtual machine guests [43, 155], and simulated devices [1, 43]. IBM’s work on the CP-40/CMS focused on improving performance through efficient utilization of shared memory [15, pp. 3–5] and explictly did not target efficient utilization of CPU through sharing [15, p. 1].Kogut [112] developed a variant of CP-67/CMS to improve performance through dynamic allocation of storage (physical disk) to virtual machines.

IBM System/360被明确地设计为跨不同型号和不同硬件配置[20]的软件可移植性。在20世纪60年代中期，IBM的控制程序-40剑桥监控系统(CP-40/CMS)项目在一个改进的IBM System/360 (model 40)上进一步采用了这个想法——最初称其为一台伪机器，但后来采用了术语虚拟机[61，第485页]。CP-40/CMS和后来的CP-67/CMS1项目对早期的可移植性方法进行了改进，使得为裸机编写的软件无需修改就可以在虚拟机中运行，从而可以模拟各种不同硬件配置的外观[15，第1-2页]。通过引入中断的特权分离[15，第6-7页]、虚拟机客户[43,155]和模拟设备中的分页内存[1,43]，它还改进了隔离。IBM在CP-40/CMS上的工作重点是通过有效利用共享内存来提高性能[15,pp. 3-5]，而明显没有通过共享来有效利用CPU [15, p. 1]。Kogut[112]开发了一种CP-67/CMS的变种，通过向虚拟机动态分配存储(物理磁盘)来提高性能。

### 4.3 VM/370

IBM’s VM/370 running on the System/370 hardware followed in the early 1970s and included virtual memory hardware [61, p. 485]. Madnick and Donovan [130, p. 214] estimated the overhead of the VM/370 at 10% to 15% but deemed the performance trade-off to be worthwhile from a security perspective. Goldberg [85, pp. 39–40] identified the source of overhead as primarily: maintaining state for virtual processors, trapping and emulating privileged instructions, and memory address translation for virtual machine guests (especially when paging was supported in the guests). In retrospect, Creasy [61] noted that efficient execution was never a primary goal of IBM’s work on the CP-40, CP-67, or VM/370 (p. 487), and the focus was instead on efficient utilization of available resources (p. 484).

1970年代早期，IBM的VM/370在System/370硬件上运行，包括虚拟内存硬件[61，第485页]。Madnick和Donovan [130, p. 214]估计VM/370的开销在10%到15%之间，但认为从安全角度来看，性能的权衡是值得的。Goldberg[85，第39-40页]认为开销的来源主要是:维护虚拟处理器的状态、捕获和模拟特权指令，以及虚拟机客户机的内存地址转换(特别是在客户机中支持分页时)。回顾过去，Creasy[61]指出，高效执行从来都不是IBM在CP-40、CP-67或VM/370上工作的主要目标(第487页)，而重点是有效利用可用资源(第484页)。

### 4.4 Trade-Offs

In their formal requirements for virtual machines in the mid-1970s, Popek and Goldberg [162, p. 413] stated that ideally virtual machines should “show at worst only minor decreases in speed” compared to running on bare metal. In 2017, Bugnion et al. [41] explained Popek and Goldberg’s requirements in modern terms, exploring the performance impact for hardware architectures that do not fully meet the requirements.

在20世纪70年代中期对虚拟机的正式要求中，Popek和Goldberg [162, p. 413]指出，与在裸机上运行相比，理想的虚拟机应该“在最坏的情况下只有轻微的速度下降”。2017年，Bugnion等人[41]用现代术语解释了Popek和Goldberg的需求，探讨了不能完全满足需求的硬件架构对性能的影响。

Buzen and Gagliardi [43, p. 291], Madnick and Donovan [130, p. 212], Goldberg [84, p. 75], and Creasy [61, p. 486] all observed that the portability offered by virtual machines was also an advantage for development purposes, since it allowed development and testing of multiple different versions of the kernel/operating systems—and programs targeting those kernels/operating systems—in multiple different virtual hardware configurations, on the same physical machine at the same time.

Buzen和Gagliardi[43，第291页]，Madnick和Donovan[130，第212页]，Goldberg[84，第75页]，和Creasy[61，第486页]都注意到，虚拟机提供的可移植性对于开发目的也是一个优势，因为它允许在同一物理机器上，在不同的虚拟硬件配置中，同时开发和测试多个不同版本的内核/操作系统——以及针对这些内核/操作系统的程序。

Buzen and Gagliardi [43] considered one of the key advantages of the virtual machine approach to be that “virtual machine monitors typically do not require a large amount of code or a high degree of logical complexity.” Popek and Kline [161, p. 294] discussed the advantage of virtual machines being smaller and less complex than a kernel and complete operating system, improving their potential to be secure. Goldberg [85, p. 39] suggested minimizing complexity as a way to improve performance: selectively disabling more expensive features (e.g., memory paging in guests) for virtualmachines thatwould not use the features. Creasy [61, p. 488] discussed the advantages of minimizing interdependencies between virtual machines, giving preference to standard interfaces on the host machine.

Buzen和Gagliardi[43]认为虚拟机方法的关键优势之一是“虚拟机监视器通常不需要大量代码或高度的逻辑复杂性。Popek和Kline[161，第294页]讨论了虚拟机比内核和完整的操作系统更小、更简单的优势，提高了它们的安全潜力。Goldberg[85，第39页]建议将复杂性最小化作为提高性能的一种方法:选择性地禁用不使用这些特性的虚拟机的更昂贵的特性(例如，在客户机中进行内存分页)。Creasy[61，第488页]讨论了最小化虚拟机之间的相互依赖的好处，并优先考虑主机上的标准接口。

A frequently cited group of papers in the early 1970s, by Lauer and Snow [118], Lauer and Wyeth [119], and Srodawa and Bates [185], suggested that virtual machines offered a sufficient level of isolation that it was no longer necessary to maintain a privilege-separated kernel in the host operating system. However, by that point in time, the concept of a privileged kernel was well enough established that the idea of eliminating it was unlikely to be widely accepted. Buzen and Gagliardi [43, p. 297] observed that the proposal depended heavily on the ability of the virtual machine implementation to handle all virtualmemorymapping directly, but since the papers failed to take memory segmentation into account, the approach could not be implemented as initially proposed.

一群频繁引用的论文在1970年代早期,劳尔和雪[118],劳尔和惠氏[119],Srodawa和贝茨[185],认为虚拟机提供了足够的隔离级别,它不再需要维护一个privilege-separated主机操作系统的内核。然而，到那时，特权内核的概念已经足够明确，消除特权内核的想法不太可能被广泛接受。Buzen和Gagliardi [43, p. 297]观察到，该方案严重依赖于虚拟机实现直接处理所有虚拟内存映射的能力，但由于论文没有将内存分割考虑在内，该方法无法像最初提出的那样实现。

### 4.5 Decline

As companies like DEC, Honeywell, HP, Intel, and Xerox introduced smaller hardware to the market in the 1970s, they did not include hardware support for features such as virtual memory and the ability to trap all sensitive instructions, which made it challenging to implement strong isolation using virtual machine techniques on such hardware [66, 78]. Creasy [61, p. 484] observed in the early 1980s that the advent of the personal computer decreased interest in the early forms of virtual machines—which were largely developed for the purpose of isolating users in time-sharing systems on mainframes—but he recognized potential for virtual machines to serve “the future’s network of personal computers.”

随着像DEC、霍尼韦尔、惠普、英特尔和施乐这样的公司在20世纪70年代将小型硬件引入市场，他们没有包括对诸如虚拟内存和捕获所有敏感指令的能力等功能的硬件支持，这使得在此类硬件上使用虚拟机技术实现强隔离具有挑战性[66,78]。Creasy [61, p. 484]在20世纪80年代早期观察到，个人计算机的出现降低了对早期形式的虚拟机的兴趣——虚拟机主要是为了在大型机上的分时系统中隔离用户而开发的——但他认识到虚拟机服务于“未来的个人计算机网络”的潜力。

## 5 EARLY CAPABILITIES

The origin of containers is often attributed [31, 54, 114, 121, 166] to the addition of the chroot system call in the seventh edition of UNIX released by Bell Labs in 1979 [108]. The simple form of filesystem namespace isolation that chroot provides was certainly one influence on the development of containers, although it lacked any concept of isolation for process namespaces [105, 165]. However, containers are not a single technology; they are a collection of technologies combined to provide secure isolation, including namespaces, cgroups, seccomp, and capabilities. Combe et al. [54], Jian and Chen [102], Kovács [114], Priedhorsky and Randles [165], and Raho et al. [166] describe how these different technologies combine to provide secure isolation for containers. It is more accurate to attribute the origin of containers to the earliest of these technologies—capabilities—that began decades before chroot and several years before the first work on virtual machines. Like containers, capabilities took the approach of building secure isolation into the hardware and the operating system, without virtualization.

容器的起源通常被归为[31,54,114,121,166]与1979年贝尔实验室发布的第七版UNIX中增加的chroot系统调用有关[108]。chroot提供的简单形式的文件系统名称空间隔离肯定是容器开发的一个影响因素，尽管它缺乏进程名称空间隔离的任何概念[105,165]。然而，容器不是单一的技术;它们是用于提供安全隔离的技术的集合，包括名称空间、cgroups、seccomp和功能。Combe等人[54]，Jian和Chen [102]， Kovacs [114]， Priedhorsky和Randles [165]， Raho等人[166]描述了这些不同的技术如何结合起来提供容器的安全隔离。更准确的说法是，容器起源于这些技术的早期——能力——在chroot出现之前几十年，在虚拟机出现之前几年。与容器一样，功能采用在硬件和操作系统中构建安全隔离的方法，而不使用虚拟化。

### 5.1 Descriptors

In the early 1960s, inspired by the need to isolate processes, the Burroughs B5000 hardware architecture introduced an improvement to memory protection called descriptors, which flagged whether a particular memory segment held code or data, and protected the system by ensuring it could only execute code (and not data), and could only access data appropriately (a single element scalar, or bounds-checked array) [120, 136]. A process on the B5000 could only access its own code and data segments through a private Program Reference Table, which held the descriptors for the process [120, p. 23]. A descriptor also flagged whether a segment was actively in main memory or needed to be loaded from drum [120, p. 24].

在1960年代早期,受隔离过程的需要,Burroughs B5000硬件架构推出了一个名为“描述符改进内存保护,标记是否一个特定的内存段代码或数据,和保护系统,确保它只能执行代码(而不是数据),并适当地只能访问数据(单个元素标量或bounds-checked数组)(120、136)。B5000上的进程只能通过私有程序引用表访问自己的代码和数据段，这个私有程序引用表包含进程的描述符[120，第23页]。描述符还标记了一个段是否活跃在主存中，或者是否需要从drum中加载[120，第24页]。

### 5.2 Dennis and Van Horn

In the mid-1960s, Dennis and Van Horn [65] introduced the term capability in theoretical work directly inspired by both the Burroughs B5000 andMIT’s Compatible Time-Sharing System (CTSS) [65, p. 154]. Like the B5000 descriptors, capabilities defined the set of memory segments a process was permitted to read, write, or execute [120, p. 42]. These early capabilities introduced several important refinements: a process executed within a protected domain with an associated capability list; multiple processes could share the same capability list; and a process could FORK a parallel process with the same capabilities (but no greater), or create a subprocess with a subset of its own capabilities (but no greater) [120, pp. 42–44]. These theoretical capabilities also had a concept of ownership (by a process or a user) [120, p. 42] and of persistent data “directories” (but not files) that survived beyond the execution of a process and could be private to a user or accessible to any user [120, pp. 44–45].

在20世纪60年代中期，Dennis和Van Horn[65]直接受到Burroughs B5000和mit兼容分时系统(CTSS)的启发，在理论工作中引入了capability一词[65,p. 154]。像B5000描述符一样，能力定义了进程被允许读、写或执行的内存段集[120，第42页]。这些早期的功能引入了几个重要的改进:在受保护的域内执行的流程具有相关的功能列表;多个进程可以共享相同的能力列表;一个流程可以派生出具有相同功能(但不能更大)的并行流程，或者创建具有自己功能子集(但不能更大)的子流程[120，第42-44页]。这些理论上的功能也有所有权的概念(由进程或用户)[120，第42页]和持久性数据“目录”(而不是文件)，这些目录在进程执行之后幸存下来，可以是一个用户私有的，也可以是任何用户可访问的[120，第44-45页]。

Soon after Dennis and Van Horn published their theoretical capabilities, Ackerman and Plummer [14] implemented some aspects of capabilities relating to resource control on a modified PDP-1 at MIT and added a file capability in addition to the directory capability—a precursor to filesystem namespaces.

在Dennis和Van Horn发表了他们的理论能力之后不久，Ackerman和Plummer[14]在MIT的一个经过修改的PDP-1上实现了一些与资源控制相关的能力，并在目录能力之外增加了一个文件能力——文件系统名称空间的前身。

### 5.3 Chicago Magic Number Machine

In 1967, the University of Chicago launched the first attempt at designing and building a generalpurpose hardware and software capability system, which they later called the Chicago Magic Number Machine3 [73, 74]. The Chicago machine pushed the concept of separation between capabilities and data further, to protect against users altering the capabilities that limited their access to memory on the system [120, pp. 49–50]. The machine had a set of physical registers for capabilities, which were distinct from the usual set of registers for data. It also flagged whether each memory segment stored capabilities or data, and prevented processes from performing data operations like reading or writing on capability segments or capability registers. Inter-process communication also sent both a capability segment and a data segment [120, p. 51].

1967年，芝加哥大学首次尝试设计和建造一个通用的硬件和软件能力系统，他们后来称之为芝加哥魔术数字机[73,74]。Chicago机器进一步推动了功能和数据分离的概念，以防止用户更改限制他们访问系统内存的功能[120，第49-50页]。这台机器有一组用于能力的物理寄存器，不同于通常用于数据的寄存器集。它还标记了每个内存段是否存储了功能或数据，并阻止进程执行数据操作，如在功能段或功能寄存器上进行读写。进程间通信也发送能力段和数据段[120，第51页]。

The University of Chicago project ran out of funding and was never completed, but it inspired subsequent work on CAL-TSS [120, p. 49].

芝加哥大学的这个项目资金不足，没有完成，但它启发了随后的CAL-TSS工作[120，第49页]。

### 5.4 CAL-TSS

In 1968, the University of California at Berkeley launched the CAL-TSS project [120, pp. 52–57], which aimed to produce a general-purpose capability-based operating system, to run on a Control Data Corporation 6400 model (RISC architecture) mainframe machine, without any special customization to the hardware. Like previous implementations, CAL-TSS confined a process to a domain, restricting access to hardware registers, memory, executable code, system calls to the kernel, and inter-process communication. The project introduced a concept of unique and nonreusable identifiers for objects, to protect against reuse of dangling pointers to access and modify memory that has been reallocated after being freed.

1968年，加州大学伯克利分校(University of California at Berkeley)启动了CAL-TSS项目[120,pp. 52-57]，目的是生产一种通用的基于能力的操作系统，在Control Data Corporation 6400模型(RISC architecture)主机上运行，而不需要对硬件进行任何特殊定制。与以前的实现一样，CAL-TSS将进程限制在一个域内，限制对硬件寄存器、内存、可执行代码、对内核的系统调用和进程间通信的访问。该项目引入了对象的唯一和不可重用标识符的概念，以防止重用悬空指针来访问和修改释放后重新分配的内存。

The CAL-TSS project encountered difficulties implementing the operating system as designed and was terminated in 1971. Levy [120, p. 57] identified the memory management features of the CDC 6400 as a particularly troublesome obstacle to the implementation. In postmortem analysis, Sturgis [186] and Lampson and Sturgis [116] reflected that CAL-TSS ended up being large, overly complex, and slow, and attributed this primarily to a poor match between the hardware they selected and the design of mapped address spaces, and also to their design choice of distributing privileged code for manipulating global system data across individual processes rather than consolidating it in a privileged kernel.

CAL-TSS项目在实现设计的操作系统时遇到了困难，并于1971年终止。Levy[120，第57页]认为CDC 6400的内存管理特性是实现的一个特别麻烦的障碍。在后期的分析中,Sturgis[186]和兰普森Sturgis[116]反映CAL-TSS最终被大,过于复杂,而缓慢的,,这主要归因于一个糟糕的比赛之间的硬件选择和映射地址空间的设计,以及他们的设计选择的特权代码操纵全局系统数据分发个人过程而不是巩固特权内核。

### 5.5 Plessey System 250

In the early 1970s, the Plessey System 250 [72] was a commercially successful real-time multiprocessing telephone-switch controller. It implemented capabilities for memory protection and process isolation [120, p. 65], and expanded capabilities into the I/O system [120, p. 77].

在20世纪70年代早期，Plessey System 250[72]是一种商业上成功的实时多处理电话开关控制器。它实现了内存保护和进程隔离的功能[120，第65页]，并将功能扩展到I/O系统[120，第77页]。

### 5.6 Provably Secure Operating System

Also in the early 1970s, the Stanford Research Institute began a project to explore the potential of formal proofs applied to a capability-based operating system design, which they called the Provably Secure Operating System (PSOS) [150]. The design was completed in 1980 but never fully formally proven and never implemented [151].

同样在20世纪70年代早期，斯坦福研究所开始了一个项目，探索将形式证明应用于基于能力的操作系统设计的潜力，他们称之为可证明安全操作系统(PSOS)[150]。该设计在1980年完成，但从未完全正式证明和实施[151]。

### 5.7 CAP

In the late 1970s, the University of Cambridge’s CAP machine [148, 210] successfully implemented capabilities as general-purpose hardware combined with a complementary operating system. The CAP introduced a refinement replacing the privileged kernel with an ordinary process, so the special control the “root” process had over the entire system was really just the normal ability of any process to create subprocesses and grant a subset of its own capabilities to those subprocesses [120, pp. 80–81].

在20世纪70年代末，剑桥大学的CAP机器[148,210]成功地实现了通用硬件与互补操作系统结合的功能。帽介绍细化取代特权内核与一个普通的过程,因此,特殊控制“根”的过程在整个系统只是正常的任何进程创建子流程和能力给予这些子流程自身功能的一个子集(120年,第81 - 80页)。

### 5.8 Object Systems

Several software offshoots of the early capability systems generalized the idea by treating processes and shared resources as typed objects with associated capabilities, including Carnegie-Mellon’s Hydra [217, 218], StarOS [103], and Gnosis later renamed as KeyKOS [92].

早期能力系统的几个软件分支通过将进程和共享资源作为具有相关能力的类型对象来推广这一思想，包括Carnegie-Mellon的Hydra[217,218]、StarOS[103]和Gnosis后来被重新命名为KeyKOS[92]。

### 5.9 IBM System/38

In 1978, IBM announced plans for a capability-based hardware architecture, the System/38, which they shipped in 1980 [120, p. 137]. Berstis [32] characterized the primary goal of the System/38 as improving memory protection without sacrificing performance. Houdek et al. [96] described the implementation of capabilities as protected pointers in detail. The System/38 introduced a concept of user profiles associated with protected process domains [32, pp. 249–250], which were vaguely reminiscent of modern user namespaces, although implemented differently. User profiles allowed for revocation of capabilities but at the cost of significantly increased complexity in the implementation [120, pp. 155–156].

1978年，IBM宣布了基于能力的硬件架构System/38的计划，该架构于1980年发布[120，第137页]。Berstis[32]认为System/38的主要目标是在不牺牲性能的情况下提高内存保护。Houdek等人[96]详细描述了作为受保护指针的功能的实现。System/38引入了与受保护进程域相关联的用户配置文件的概念[32,pp. 249-250]，这让人隐约想起现代的用户名称空间，尽管实现方式有所不同。用户配置文件允许撤销功能，但其代价是在实现过程中显著增加了复杂性[120，第155-156页]。

The System/38 was succeeded by the AS/400 in the late 1980s, which removed capability-based addressing [183, p. 119]. The AS/400 later adopted the concept of logical partitioning from the IBM System/370 [176, pp. 1–2], to divide the physical resources of the host machine between multiple guests at the hardware level4 [183, pp. 240, 328].

System/38在1980年代后期被AS/400继任，它删除了基于能力的寻址[183，第119页]。AS/400后来采用了IBM System/370的逻辑分区概念[176,pp. 1-2]，在硬件级别4上在多个客户机之间划分主机的物理资源[183,pp. 240,328]。

### 5.10 Intel iAPX 432

In 1975, Intel began designing the iAPX 432 [2] capability-based hardware architecture, which they originally intended to be their next-generation, market-leading CPU, replacing the 8080 [137, p. 79]. The project finally shipped in 1981, but it was significantly delayed and significantly over budget [137, p. 79].

在1975年，英特尔开始设计iAPX 432[2]基于能力的硬件架构，他们最初打算成为下一代，市场领先的CPU，取代8080 [137,p. 79]。该项目最终在1981年发货，但它被严重推迟，并大大超出预算[137，第79页]。

Mazor [137, p. 75] recorded that performance was not considered as a goal in the design of the iAPX 432. Hansen et al. [91] measured the performance of the iAPX 432 against the Intel 8086, Motorola 68000, and the VAX-11/780 in 1982, with results as poor as 95 times slower on some benchmarks. Norton [152, p. 27] assessed the poor performance and unoptimized compiler offered by the iAPX 432 as the leading cause of its commercial failure. Levy [120, p. 186] blamed the commercial failure on both poor performance and overhyped marketing.

Mazor [137, p. 75]记录了性能在iAPX 432设计中没有被考虑为目标。汉森等人[91]在1982年将iAPX 432与英特尔8086、摩托罗拉68000和VAX-11/780的性能进行了对比，结果在某些基准上甚至慢了95倍。Norton[152，第27页]认为iAPX 432提供的性能差和未优化的编译器是导致其商业失败的主要原因。列维[120，第186页]将商业失败归咎于糟糕的业绩和过度炒作的营销。

In a move that Mazor [137] described as “a crash program . . . to save Intel’s market share” (p. 75), Intel launched a parallel project to develop the 8086 architecture (the first in a long line of x86 CPUs), which became Intel’s leading product line by default rather than by design (p. 79).

在一个行动中，Mazor[137]描述为“一个崩溃计划…为了保住英特尔的市场份额”(第75页)，英特尔启动了一个并行项目来开发8086架构(一长串x86 cpu中的第一个)，它默认而不是通过设计成为英特尔的领先产品线(第79页)。

### 5.11 Trade-Offs

The early capability systems in the 1960s and 1970s sacrificed performance for the sake of security, although Levy speculated in the mid-1980s that this was partly due to “hardware poorly matched to the task” [120, p. 205]. Wilkes [209, pp. 49–59] contrasted the memory protection features of capabilities with other systems of the time, including detailed descriptions of hardware implementations.

早期的能力系统在20世纪60年代和70年代为了安全牺牲了性能，尽管Levy在80年代中期推测这部分是由于“硬件不匹配的任务”[120,205页]。Wilkes [209, pp 49-59]对比了当时其他系统的内存保护功能，包括硬件实现的详细描述。

Levy [120, p. 205] also observed that the early capability systems significantly increased complexity for the sake of security. Patterson and Séquin [157] and Patterson and Ditzel [156] judged this sacrifice as a major reason the capability machines were surpassed by simpler architectures, such as RISC.

Levy[120，第205页]也观察到早期的能力系统为了安全显著地增加了复杂性。Patterson和Sequin[157]和Patterson和Ditzel[156]认为这种牺牲是性能机器被更简单的架构(如RISC)超越的主要原因。

Kirk McKusick recalled that the primary reason Bill Joy ported chroot from UNIX into BSD in 1982 was for portability, so he could build different versions of the system in an isolated build directory [105, p. 11].

Kirk McKusick回忆说，Bill Joy在1982年将chroot从UNIX移植到BSD的主要原因是为了可移植性，这样他就可以在一个独立的构建目录中构建系统的不同版本[105，第11页]。

### 5.12 Decline

As with virtual machines, interest in the early capability systems sharply declined in the 1980s, influenced by several independent factors. Several early attempts to implement capabilities were terminated uncompleted—notably the ChicagoMagic NumberMachine, CAL-TSS, and the PSOS— contributing to a reputation that capability systemswere difficult to implement and perhaps overly ambitious, despite the successful implementations that followed. The commercial failure of Intel’s iAPX 432 raised further doubts on the feasibility of capability-based architectures. In 2003, Neumann and Feiertag [151, p. 6] looked back on the early capability systems, expressing disappointment that “the demand for meaningfully secure systems has remained surprisingly small until recently.”

与虚拟机一样，受几个独立因素的影响，对早期功能系统的兴趣在20世纪80年代急剧下降。一些早期实现能力的尝试没有完成就被终止了——最著名的是芝加哥数字机、CAL-TSS和PSOS——这使得能力系统难以实现，而且可能过于雄心勃勃的名声，尽管后来实现成功了。英特尔iAPX 432的商业失败引发了对基于能力架构的可行性的进一步怀疑。在2003年，Neumann和Feiertag [151, p. 6]回顾了早期的能力系统，表达了对“直到最近对有意义的安全系统的需求仍然惊人的小”的失望。

Perhaps the most significant factor in the decline of capabilities was the rise of the generalpurpose operating system, which was a third important technology that evolved from multiprogramming. MIT’s CTSS [55, 209] laid the foundation for Multics [56], which later inspired UNIX [168] and its robust mutation, the Berkeley Software Distribution (BSD)6 [138, 139]. Saltzer and Schroeder [174, p. 1294] contrasted capabilities with the access control list models adopted by Multics and its descendants, calling out revocation of access as one major area where capabilities fell short.

也许在能力衰退中最重要的因素是通用操作系统的崛起，它是由多道程序设计演变而来的第三项重要技术。麻省理工学院的CTSS[55, 209]奠定了Multics[56]的基础，后者后来激发了UNIX[168]及其强大的变异，伯克利软件发行版(BSD)[138, 139]。Saltzer和Schroeder [174, p. 1294]将性能与Multics及其后代采用的访问控制列表模型进行了对比，指出撤销访问是性能不足的一个主要领域。

Although none of the early capability systems remain in use today, they have not been entirely forgotten. In 2003, Miller et al. [143] reviewed capability systems from a historical perspective, addressing common misconceptions about capabilities related to revocation, confinement, and equivalence to access control lists. Section 7 traces the evolution of a feature called capabilities in the modern Linux Kernel. FreeBSD took a different approach for the feature it calls capabilities and integrated the Capsicum framework [140, p. 30], which was more directly derived from the classic capability systems [21, 199]. In 2012, the CHERI project [200, 202, 203, 215] expanded on the ideas of the Capsicum framework, pushing its capability model down into an RISC-based hardware architecture. Since 2016, Google has been exploring a revival of capability systems with the Fuchsia operating system and Zircon microkernel [87]. In a 2018 plenary session about Spectre/ Meltdown, Hennessy [94] pointed to future potential for capabilities, reflecting that the early capability systems “probably weren’t the right match for what software designers thought they needed and they were too inefficient at the time” but suggested “those are all things we know how to fix now . . . so it’s time, I think, to begin re-examining some of those more sophisticated [protection] mechanisms and see if they’ll work.”

虽然没有一个早期的能力系统仍然在今天使用，他们没有完全被遗忘。在2003年，Miller等人[143]从历史的角度回顾了功能系统，解决了关于功能的撤销、限制和访问控制列表的等价性的常见误解。第7节追溯了现代Linux内核中称为功能的特性的演变。FreeBSD为其称为功能的特性采取了不同的方法，并集成了辣椒框架[140，第30页]，它更直接地源自经典的功能系统[21,199]。2012年，CHERI项目[200,202,203,215]扩展了辣椒框架的思想，将其能力模型下推到基于risc的硬件架构中。自2016年以来，谷歌一直在探索复兴Fuchsia操作系统和锆石微核的能力系统[87]。在2018年的一次全体会议关于幽灵/崩溃,轩尼诗[94]指出未来潜在的能力,反映了早期功能系统,“可能没有合适的匹配软件设计师认为他们需要什么和他们太低效”但建议”这些都是我们现在知道如何修理。所以我认为，现在是重新检查那些更复杂的保护机制，看看它们是否有效的时候了。”

## 6 MODERN VIRTUAL MACHINES

Virtual machines still existed in the 1980s and 1990s but garnered only a bare minimum of activity and interest. IBM’s line of VM products, descended from VM/370, continued to have a small but loyal following [194]. DOS, OS/2, and Windows all offered a limited form of DOS virtual machines during that time, although it might be more fair to categorize those as emulation. The rise of programming languages like Smalltalk and Java re-purposing the term virtual machine—to refer to an abstraction layer of a language runtime rather than a software replication of a real hardware architecture—may be indicative of how dead the original concept of virtual machines was in that period.

虚拟机在20世纪80年代和90年代仍然存在，但只吸引了极少的活动和兴趣。IBM的VM系列产品是VM/370的后代，仍然有一小群忠实的追随者[194]。在此期间，DOS、OS/2和Windows都提供了一种有限形式的DOS虚拟机，尽管将它们归类为仿真可能更公平一些。Smalltalk和Java等编程语言的兴起重新定义了虚拟机这个术语——指的是语言运行时的抽象层，而不是对真实硬件体系结构的软件复制——这可能表明了在那个时期，虚拟机的原始概念是多么的过时。

After nearly two decades, the late 1990s brought a resurgence of interest in virtual machines but for a new purpose adapted to the technology of the time.

近20年后，上世纪90年代末，人们对虚拟机的兴趣死灰复燃，但目的是为了适应当时的技术。

###  6.1 Disco

In 1997, the Disco research project at Stanford University explored reviving virtual machines as an approach to making efficient use of hardware with multiple CPUs (on the order of “tens to hundreds”), and included a lightweight library operating system for guests (SPLASHOS) as an option, in addition to supporting commodity operating systems as guests. Bugnion et al. [39] cited portability (rather than security or performance) as the primary motivation of the Disco project, which proposed virtual machines as a potential way to allow commodity operating systems (Unix, Windows NT, and Linux) to run on NUMA architectures without extensive modifications.

1997年,迪斯科斯坦福大学研究项目探讨恢复虚拟机作为一个方法来有效的利用与多个cpu的硬件(在“数十到数百”)的顺序,并包含一个轻量级图书馆来宾操作系统(SPLASHOS)作为一个选项,除了支持商品操作系统是客人。Bugnion等人的[39]引用可移植性(而不是安全性或性能)作为Disco项目的主要动机，该项目提出虚拟机作为一种潜在的方式，允许普通的操作系统(Unix、Windows NT和Linux)在NUMA架构上运行，而不需要进行大量的修改。

### 6.2 VMware

A year later, the team behind Disco founded VMware to continue their work, and released a workstation product in 1999 [40], quickly followed by two server products (GSX and ESX) in 2001 [18, 175, 197]. VMware faced a challenge in virtualizing the x86 architectures of the time, because the hardware did not support traditional virtualization techniques—specifically the architecture contained some sensitive instructions that were not also privileged—so a virtual machine monitor could not rely on trapping protection exceptions as the sole means of identifying when to execute emulated instructions as a safe replacement, since some potentially harmful instructions would never be trapped [170, p. 131].7 To work around this limitation, VMware combined the trap-andexecute technique with a dynamic binary translation technique [40, p. 12:3], which was faster than full emulation but still allowed the guest operating system to run unmodified [40, pp.12:29–12:36].

一年后，Disco背后的团队建立了VMware继续他们的工作，并在1999年发布了一个工作站产品[40]，紧接着在2001年发布了两个服务器产品(GSX和ESX)[18,175,197]。VMware在虚拟化面临挑战的x86架构,因为传统的硬件不支持虚拟化技术——特别是架构包含一些敏感指令也没有特权,所以虚拟机监视器不能依靠捕捉保护例外作为识别的唯一手段,当执行模拟指令作为一个安全的替代,因为一些潜在的有害指令不会被困(170年p . 131)。为了解决这个限制，VMware将trap-and-execute技术与动态二进制转换技术结合起来[40,p. 12:3]，这比完全模拟要快，但仍然允许来宾操作系统不加修改地运行[40,pp.12:29-12:36]。

### 6.3 Denali

The Denali project at the University of Washington in 2002 [207] introduced the term paravirtualization, 8 another work-around for the lack of hardware virtualization support in x86, which involved altering the instruction set in the virtualized hardware architecture and then porting the guest operating system to run on the altered instruction set [206].

- [207] Andrew Whitaker, Marianne Shaw, and Steven D. Gribble. 2002. Denali: A scalable isolation kernel. In Proceedings of the 10th ACM SIGOPS European Workshop. ACM, New York, NY, 10–15.

华盛顿大学的 Denali 项目 2002年 [207] 引入了半虚拟化这一术语，另一种解决 x86 中缺乏硬件虚拟化支持的方法,其中包括改变虚拟硬件架构中的指令集，然后移植客户操作系统以在改变后的指令集上运行 [206]。

### 6.4 Xen

The Xen project at the University of Cambridge in 2003 [26] also used paravirtualization techniques andmodified guest operating systems but emphasized the importance of preserving the application binary interface (ABI) within the guests so that guest applications could run unmodified. Xen’s greatest technical contribution may have been its approach to precise accounting for resource usage, with the explicit intention to individually bill tenants sharing physical machines [26, p. 176], which was a relatively radical idea at the time and directly led to the creation of Amazon’s Elastic Compute Cloud (EC2) a couple of years later [28].

剑桥大学的 Xen 项目 2003年 [26] 也使用了半虚拟化技术和修改的客户操作系统，但强调了保留应用二进制接口 (ABI) 的重要性。在来宾中，以便来宾应用程序可以未经修改地运行。Xen 最大的技术贡献可能是它对资源使用进行精确核算的方法，明确打算单独向共享物理机器的租户收费 [26，p. 176]，这在当时是一个相对激进的想法，直接导致了亚马逊的弹性计算云 (EC2) 的创建几年后 [28]。

Chisnall [47] provided a detailed account of Xen’s architecture and design goals. Xen’s approach to the problem of untrapped x86 privileged instructions was to substitute a set of hypercalls for unsafe system calls [47, pp. 10–13]. Smith and Nair [181, p. 422] highlighted that Xen was able to run unmodified application binaries within the guest, because it ran the guest in ring 1 of the IA-32 privilege levels and the hypervisor in ring 0, so all privileged instructions were filtered through the hypervisor.

Chisnall [47] 提供了 Xen 的架构和设计目标的详细描述。Xen 处理未捕获 x86 特权指令问题的方法是用一组超调用代替不安全的系统调用 [47，第 10-13 页]。史密斯和奈尔 [181，p. 422] 突出显示 Xen 能够在来宾中运行未经修改的应用程序二进制文件，因为它在 IA-32 权限级别的环 1 中运行来宾，在环 0 中运行虚拟机管理程序,因此，所有特权指令都通过虚拟机管理程序进行过滤。

### 6.5 x86 Hardware Virtualization Extensions

In 2000, Robin and Irvine [170] analyzed the limitations of the x86 architecture as a host for virtual machine implementations, with reference to the earlier work of Goldberg [83] on the architectural features required to support virtualmachines. In themid-2000s, in response to the growing success of virtual machines, and the challenges of implementing them on x86 hardware, Intel and AMD both added hardware support for virtualization in the form of a less privileged execution mode to execute code for the virtual machine guest directly but selectively trap sensitive instructions, eliminating the need for binary translation or paravirtualization. Rosenblum and Garfinkel [172] discussed the motivations behind the added hardware support for virtualization in x86, before the changes were released. Pearce et al. [158, p. 7] contrasted binary translation, paravirtualization, and the features x86 added for hardware-assisted virtualization, clarifying the x86 virtualization extensions were not full virtualization. Adams and Agesen [16] recounted the difficulties VMware encountered while integrating the x86 hardware virtualization extensions and concluded that the new features offered no performance advantage over binary translation.

在 2000，Robin 和 Irvine [170] 分析了 x86 架构作为虚拟机实现主机的局限性,参考 Goldberg [83] 关于支持虚拟机所需的架构特性的早期工作。在 21世纪00年代中期中期，为了应对虚拟机日益增长的成功以及在 x86 硬件上实现虚拟机的挑战,英特尔和 AMD 都以较低特权执行模式的形式增加了对虚拟化的硬件支持，以直接为虚拟机来宾执行代码，但有选择地捕获敏感指令,消除了对二进制翻译或半虚拟化的需要。Rosenblum 和 Garfinkel [172] 在变更发布之前讨论了 x86 中增加硬件支持背后的动机。Pearce 等人 [158，p. 7] 对比了二进制翻译、半虚拟化和 x86 为硬件辅助虚拟化增加的特性，阐明 x86 虚拟化扩展不是完全虚拟化。Adams 和 Agesen [16] 叙述了 VMware 在集成 x86 硬件虚拟化扩展时遇到的困难，并得出结论，新功能没有比二进制翻译提供性能优势。

In 2007, the KVM subsystem for the Linux Kernel provided an API for accessing the x86 hardware virtualization extensions [110]. Since KVM was only a Kernel subsystem, the developers released a fork of QEMU11 as the userspace counterpart of KVM, so the combination of QEMU+KVM provided a full virtual machine implementation, including virtual devices [198, pp.128–129]. Eventually, KVM support was merged into mainline QEMU [122].

在 2007年，Linux 内核的 KVM 子系统提供了用于访问 x86 硬件虚拟化扩展的 API [110]。由于 KVM 只是一个内核子系统，开发人员发布了 QEMU 的一个 fork 作为 KVM 的用户空间对应物，因此 QEMU + KVM 的组合提供了一个完整的虚拟机实现,包括虚拟设备 [198，pp.128-129]。最终，KVM 支持被合并到主线 QEMU [122]。

### 6.6 Hyper-V

In 2008, Microsoft released a beta of Hyper-V [107] for Windows Server. It was built on top of the x86 hardware virtualization extensions and for some virtual devices offered a choice between slower emulation and faster paravirtualization if the guest operating system installed the “Enlightened I/O” extensions. Like Xen’s Dom0, Hyper-V granted special privileges to one guest, called the parent partition, which hosted the virtual devices and handled requests from the other guests. 

In 2010, Bolte et al. [35] incorporated support for Hyper-V into libvirt, so it could be managed through a standardized interface, together with Xen, QEMU+KVM, and VMware ESX.

2008，微软发布了针对 Windows Server 的 Hyper-V [107] 测试版。它是建立在 x86 硬件虚拟化扩展之上的，对于一些虚拟设备，如果客户操作系统安装了 “开明 I/O” 扩展，则可以在较慢的仿真和较快的半虚拟化之间进行选择。像 Xen 的 Dom0 一样，Hyper-V 授予一个来宾特权，称为父分区，它托管虚拟设备并处理来自其他来宾的请求。

在 2010，Bolte 等人 [35] 将对 Hyper-V 的支持纳入了 libvirt 中，因此它可以通过一个标准化的接口，与 Xen 、 QEMU + KVM 和 VMware ESX 一起进行管理。

### 6.7 Trade-Offs

Denali and Xen both used paravirtualization techniques, sacrificing portability to gain performance, but their goals for scale were completely different: Denali considered 10,000 virtual machines to be a good result [208]—achieved through a combination of lightweight guests and a minimal host—whereas Xen argued that 100 virtual machines running full operating systems13 was a more reasonable target [26, p. 165,175]. To some extent, Denali was more in line with modern container implementations than with the virtual machine implementations of its day. Xen has shifted their estimation of required scale upward over the years but still exhibits a tolerance for unnecessary performance degradation. For example, Manco et al. [131] demonstrated that a few small internal changes to the way Xen storesmetadata and creates virtual devices improved virtual machine instantiation time by an order of magnitude—a result 50 to 200 times faster than Docker’s container instantiation—however, those patches are unlikely to ever make it into mainline Xen.

Denali 和 Xen 都使用了半虚拟化技术，牺牲了可移植性以获得性能，但他们对规模的目标完全不同: Denali 认为 10,000 台虚拟机是一个好结果[208]-通过轻量级来宾和最小主机的组合实现-而 Xen 认为运行完整操作系统的 100 台虚拟机是更合理的目标 [26，165,175 页]。在某种程度上，Denali 更符合现代容器实现，而不是当时的虚拟机实现。Xen 多年来已经将他们对所需规模的估计向上移动，但仍然表现出对不必要的性能退化的容忍度。例如,manco 等人 [131] 证明了对 Xen 存储元数据和创建虚拟设备的方式的一些小的内部改变将虚拟机实例化时间提高了一个数量级 -- 结果快了 50 到 200 倍比 Docker 的容器实例化-但是,这些补丁不太可能进入主线 Xen。

Xen and KVM have a reputation for sacrificing performance to gain security; however, several independent lines of research have raised questions as to whether those security gains are real or imagined. Perez-Botero et al. [159] analyzed security vulnerabilities in Xen and KVM between 2008 and 2012, categorizing them by source, vector, and target, and observed that the most common vector of attack was device emulation (Xen 34%, KVM 40%), the majority were triggered from within the virtual machine guest (Xen 71%, KVM 66%), and the majority successfully targeted the hypervisor’s Ring –1 privileges or slightly less privileged control over Dom0 or the host operating system (Xen 80%, KVM 76%). Chandramouli et al. [46] built on the work of Perez-Botero et al. [159], moving toward a more general framework for forensic analysis of vulnerabilities in virtual machine implementations. Ishiguro and Kono [101] evaluated vulnerabilities in Xen and KVMrelated to instruction emulation between 2009 and 2017. They demonstrated that a prototype “instruction firewall” on KVM—which denies emulation of all instructions except the small subset deemed legitimate in the current execution context—could have defended against the known instruction emulation vulnerabilities; however, the patches are unlikely to evermake it intomainline KVM.

Xen 和 KVM 以牺牲性能来获得安全而闻名; 然而，一些独立的研究提出了这些安全收益是真实的还是想象的问题。Perez-Botero 等人 [159] 分析了 Xen 和 KVM 2008年和 2012 的安全漏洞，按来源、矢量和目标对其进行分类,并且观察到最常见的攻击向量是设备仿真 (Xen 34%，KVM 40%)，大多数是从虚拟机客户内部触发的(Xen 71%，KVM 66%)，大多数人成功地将虚拟机管理程序的 Ring-1 特权或对 Dom0 或主机操作系统 (Xen 80%，KVM 76%) 的特权控制作为目标。Chandramouli 等人 [46] 建立在 Perez-Botero 等人的工作之上 [159]，朝着更通用的框架发展，用于对虚拟机实现中的漏洞进行取证分析。Ishiguro 和 Kono [101] 评估了 Xen 和 kvm 中与指令仿真相关的漏洞 2017 和 2009年。他们证明了 KVM 上的原型 “指令防火墙” -- 它拒绝所有指令的仿真，除了在当前执行环境中被认为合法的小子集 -- 可以防御已知的指令仿真漏洞; 然而，这些补丁不太可能进入主线 KVM。

Szefer et al. [191] demonstrated in the NoHype implementation (based on Xen) that eliminating the hypervisor and running virtual machines with more direct access to the hardware improved security by reducing the attack surface and removing virtual machine exit events as potential attack vectors. However, the approach involved a performance trade-off in resource utilization that was not viable for most real deployments: it pre-allocated processor cores, memory, and I/O devices dedicated to specific virtualmachines rather than allowing for oversubscription and dynamic allocation in response to load.

Szefer等人[191]在NoHype实现(基于Xen)中论证了消除虚拟机监控程序和运行更直接访问硬件的虚拟机通过减少攻击面和删除作为潜在攻击载体的虚拟机退出事件来提高安全性。但是，这种方法涉及到资源利用方面的性能权衡，这对于大多数实际部署来说是不可行的:它预先分配处理器核心、内存和I/O设备，专门用于特定的虚拟机，而不允许超订阅和响应负载的动态分配。

One persistent argument in favor of virtual machines has been that virtual machine implementations have fewer lines of code than a kernel or host operating system, and are therefore easier to code review and secure [39, 81, 131, 158, 178], which is the classic trade-off ofminimizing complexity to gain security. However, less code offers only a vague potential for security, and even that potential becomes questionable as modern virtual machine implementations have grown larger and more complex [37, 53, 158, 214].

一个持续争论的虚拟机,虚拟机实现的代码比一个内核少或主机操作系统,因此更容易代码审查和安全(39、81、131、158、178),这是减少复杂性获得安全的经典的权衡。然而，更少的代码只提供了一个模糊的安全潜力，而且随着现代虚拟机实现变得更大、更复杂[37,53,158,214]，甚至这种潜力也变得有问题。

Recent work on virtual machines—such as ukvm [212], LightVM [131], and Kata Containers (formerly Intel Clear Containers) [5]—has shifted back toward an emphasis on improving performance. However, this work appears to be founded on the assumption that the virtual machine implementations under discussion are adequately secure and need only improve performance, which is a dubious assumption at best.

- [212] DanWilliams and Ricardo Koller. 2016. Unikernelmonitors: Extending minimalism outside of the box. In Proceedings of the 8th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud’16). 6.
- [131] Filipe Manco, Costin Lupu, Florian Schmidt, Jose Mendes, Simon Kuenzer, Sumit Sati, Kenichi Yasukata, Costin Raiciu, and Felipe Huici. 2017. MyVMis lighter (and safer) than your container. In Proceedings of the 26th Symposium on Operating Systems Principles (SOSP’17). ACM, New York, NY, 218–233.
- [5] [5] Kata Containers. 2018. Home Page. Retrieved December 18, 2019 from https://katacontainers.io/.

最近在虚拟机方面的工作——如ukvm[212]、LightVM[131]和Kata Containers(以前是Intel Clear Containers)[5]——已经重新将重点放在提高性能上。然而，这项工作似乎是建立在这样一个假设上的，即所讨论的虚拟机实现是足够安全的，并且只需要提高性能，这是一个很可疑的假设。

Two notable departures from this complacent attitude to security are Google’s crosvm [86] and Amazon’s Firecracker [19], which aim to improve both performance and security, by replacing QEMU with a radically smaller and simpler userspace component for KVM, and by choosing Rust as the implementation language for memory safety.14 Firecracker started as a fork of crosvm, but the two projects are collaborating on generalizing the divergence into a set of Rust libraries they can share.

- [86] Google. 2018. Chrome OS Virtual Machine Monitor. Retrieved December 18, 2019 from https://chromium.googlesource.com/chromiumos/platform/crosvm/.

- [19] [19] Amazon. 2019. Firecracker. Retrieved December 18, 2019 from https://firecracker-microvm.github.io/.

两个安全显著偏离这种自满的态度是谷歌crosvm[86]和亚马逊的爆竹[19],旨在提高性能和安全性,通过从根本上取代QEMU小的、简单的用户空间组件KVM,和通过选择生锈的实现语言记忆的安全。爆竹最初是crosvm的一个分支，但这两个项目正在合作，将分歧归纳为一组它们可以共享的锈库。

### 6.8 Decline

Toward the end of the 2000s, the enthusiasm for virtual machines gave way to a growing skepticism. Garfinkel et al. [80] demonstrated that virtual machine environments could reliably be detected on close inspection, reviving the long-running tension between the ideals of strong isolation in virtual machines, and the reality of actual implementations. Buzen and Gagliardi [43] commented on the ideals in the early 1970s, stating “Since a privileged software nucleus has, in principle, no way of determining whether it is running on a virtual or a real machine, it has no way of spying on or altering any other virtual machine that may be coexisting with it in the same system,” but in the same work they acknowledged, “In practice no virtual machine is completely equivalent to its real machine counterpart.”

21世纪00年代末，对虚拟机的热情被日益增长的怀疑所取代。Garfinkel 等人 [80] 证明了在仔细检查时可以可靠地检测到虚拟机环境，恢复了虚拟机中强隔离理想之间的长期运行张力,和实际实现的现实。Buzen 和 Gagliardi [43] 对 20世纪70年代初中期的理想进行了评论，指出 “由于特权软件核心原则上已经,无法确定它是在虚拟还是真实机器上运行,它无法监视或改变在同一个系统中可能与它共存的任何其他虚拟机，”但是在同样的工作中，他们承认,“ 在实践中，没有一个虚拟机是完全等同于其真正的机器对应物。”

In 2010, Bratus et al. [37] criticized the disproportionate focus of systems security research on virtual machines and the resulting neglect of other potentially superior approaches to system security. Vasudevan et al. [195] outlined a set of requirements for protecting the integrity of virtual machines implemented on x86 with hardware virtualization support and evaluated all existing implementations as “unsuitable for use with highly sensitive applications” (p. 141). Colp et al. [53] observed that multitenant environments presented newrisks for virtualmachine implementations, because they required stronger isolation between guests sharing the same host than was necessary when a single tenant owned the entire physical machine.

2010，Bratus 等人 [37] 批评了对虚拟机的系统安全研究的过度关注，以及由此导致的对系统安全的其他潜在优势方法的忽视。Vasudevan 等人 [195] 概述了一组在 x86 上实现的具有硬件虚拟化支持的虚拟机的完整性保护要求，并将所有现有实现评估为 “不适合用于高度敏感的应用程序” (p. 141)。Colp 等人 [53] 观察到多租户环境为虚拟机实现带来了新的风险,因为他们需要在共享同一主机的来宾之间进行更强的隔离，而不是在单个租户拥有整个物理机器时所需要的隔离。

Virtual machines such as Xen, QEMU+KVM, Hyper-V, and VMware are still in active use today, but in recent years they have entirely ceded their reputation as the “hot new thing” to containers.

虚拟机如 Xen，QEMU + KVM，Hyper-V 和 VMware 仍在积极利用今天,但是近年来，他们已经完全放弃了作为 “热门新事物” 的声誉给容器。

## 7 MODERN CONTAINERS

The collection of technologies that make up modern container implementations started coming together years before anyone used the term container. The two decade span surrounding the development of containers corresponded to a major shift in the way information about technological advances was broadcast and consumed. Exploring the socio-economic factors driving this shift is outside the scope of this survey; however, it is worth noting that the academic literature on more recent projects such as Docker and Kubernetes is largely written by outsiders providing external commentary rather than by the primary developers of the technologies. As a result, recent academic publications on containers tend to lack the depth of perspective and insight that was common to earlier publications on virtual machines, capabilities, and security in the Linux Kernel. The dialog driving innovation and improvements to the technology has not disappeared, but it has moved away from the academic literature and into other communication channels.

构成现代容器实现的技术集合在任何人使用术语 “容器” 之前的几年就开始聚集在一起了。围绕集装箱发展的 20 年时间，与传播和消费技术进步信息的方式的重大转变相对应。探索推动这一转变的社会经济因素不在本次调查的范围之内; 然而,值得注意的是，关于 Docker 和 Kubernetes 等最近项目的学术文献主要是由提供外部评论的外部人员编写的，而不是由技术的主要开发人员编写的。因此，最近关于容器的学术出版物往往缺乏早期关于 linux内核中虚拟机、功能和安全性的出版物所共有的视角和洞察力的深度。推动技术创新和改进的对话并没有消失，但它已经从学术文献转移到其他交流渠道。

### 7.1 POSIX Capabilities

In the mid-1990s, the security working group of the POSIX standards project began drafting an extension to the POSIX.1 standard, called POSIX 1003.1e [3, 71, 90], which added a feature called capabilities. The implementation details of POSIX capabilitieswere entirely different than the early capability systems [201, p. 97] but had similarities on a conceptual level: POSIX capabilities were a set of flags associated with a process or file, which determined whether a process was permitted to perform certain actions, a process could exec a subprocess with a subset of its own capabilities, and the specification attempted to support the principle of least privilege [3]. However, the POSIX capabilities did not adopt the concepts of small access domains and no-privilege defaults, which were crucial elements of secure isolation in the early capability systems [62]. The POSIX.1e draft was withdrawn from the process in 1998 and never formally adopted as a standard [90, p. 259], but it formed the basis of the capabilities feature added to the Linux Kernel in 1999 (release 2.2) [4, 132].

在 20世纪90年代中期中期，POSIX 标准项目的安全工作组开始起草对 POSIX.1 标准的扩展，称为 POSIX 1003。1e [3,71，90]，它添加了一个称为能力的特性。POSIX 能力的实现细节与早期的能力系统完全不同 [201，p. 97] 但是在概念层面上有相似之处: POSIX 功能是一组与进程或文件相关联的标志，它决定了进程是否被允许执行某些操作,一个进程可以执行一个具有其自身能力子集的子进程，规范试图支持最小特权原则 [3]。然而，POSIX 能力没有采用小访问域和无特权违约的概念，这是早期能力系统中安全隔离的关键要素 [62]。POSIX.1e 草案于 1998年退出该过程，从未正式作为标准 [90，第 259]，但它构成了 1999年 linux内核 (2.2 版) [132] 中添加的功能特性的基础。

### 7.2 Namespaces and Resource Controls

A second important strand in the evolution of modern container implementations was the isolation of processes via namespaces and resource usage controls. In 2000, FreeBSD added Jails [105], which isolated filesystem namespaces (using chroot) but also isolated processes and network resources in such a way that a process might be granted root privileges inside the jail but blocked from performing operations that would affect anything outside the jail. In 2001, Linux VServer [182] patched the Linux Kernel to add resource usage limits and isolation for filesystems, network addresses, and memory. Around the same time, Virtuozzo (later released as OpenVZ) [98, 135] also patched the Linux Kernel to add resource usage limits and isolation for filesystems, processes, users, devices, and interprocess communication (IPC). In 2003, Nagar et al. [146] proposed a framework for resource usage control and metering called Class-Based Kernel Resource Management (CKRM) and later released it as a set of patches to the Linux Kernel.

现代容器实现发展的第二个重要方面是通过命名空间和资源使用控制隔离进程。在 2000 中，FreeBSD 添加了 Jails [105]，它隔离了文件系统命名空间 (使用 chroot) 但是也隔离了进程和网络资源，这样一个进程可能在监狱内被授予根权限，但被阻止执行会影响监狱外任何事情的操作。在 2001 中，Linux VServer [182] 修补了 Linux 内核以增加资源使用限制和对文件系统、网络地址和内存的隔离。大约在同一时间，Virtuozzo (后来发布为 OpenVZ) [98,135] 也修补了 linux内核，增加了文件系统、进程、用户、设备的资源使用限制和隔离,和进程间通信 (IPC)。2003，Nagar 等 [146] 提出了一个资源使用控制和计量的框架，称为基于类的内核资源管理 (CKRM) 后来将其作为一组修补程序发布到 Linux 内核。

In 2002, the Linux Kernel (release 2.4.19) introduced a filesystem namespaces feature [109].15 In 2006, Biederman [33] proposed expanding the idea of namespace isolation in the Linux Kernel beyond the filesystem to process IDs, IPC, the network stack, and user IDs. The Kernel developers accepted the idea, and the patches to implement the features landed in the Kernel between 2006 and 2013 (releases 2.6.19 to 3.8) [109]. The last set of patches to be completed was user namespaces, which allow an unprivileged user to create a namespace and grant a process full privileges for operations inside that namespace while granting it no privileges for operations outside that namespace [11]. The way user namespaces are nested bears a resemblance to those of the capabilities of Dennis and Van Horn [65], where processes created more restricted subprocesses.

在 2002 中，linux内核 (2.4.19 版) 引入了一个文件系统命名空间特性 [109]。15 2006，Biederman [33] 提出将 linux内核中的命名空间隔离的思想扩展到文件系统之外，以处理 IDs 、 IPC 、网络堆栈和用户 id。内核开发人员接受了这一想法，并且实现这些特性的补丁在 2006年和 2013 (发布了 2.6.19 到 3.8) [109]。最后一组要完成的补丁是用户命名空间,它允许非特权用户创建一个命名空间，并授予该命名空间内操作的进程完全权限，同时不授予该命名空间外操作的权限 [11]。用户名称空间嵌套的方式与 Dennis 和 Van Horn 的能力相似 [65]，其中过程创建了更多受限的子过程。

In 2004, Solaris added Zones [164] (sometimes also called Solaris Containers), which isolated processes into groups that could only observe or signal other processes in the same group, associated each zone with an isolated filesystem namespace, and set limits for shared resource consumption (initially only CPU). Between 2006 and 2007, Rohit Seth and Paul Menageworked on a patch for the Linux Kernel for a feature they called process containers [58]—later renamed to cgroups for “control groups”—which provided resource limiting, prioritization, accounting,16 and control features for processes.

在 2004 中，Solaris 添加了区域 [164] (有时也称为 Solaris 容器)，它将进程隔离成组，这些组只能观察或发送同一组中的其他进程的信号,将每个区域与独立的文件系统命名空间关联，并设置共享资源消耗限制 (最初仅 CPU)。2006 到 2007 之间,rohit Seth 和 Paul Menage 为 linux内核开发了一个补丁，他们称之为进程容器 [58] -- 后来改名为 “控制组” 的 cgroups -- 它提供了资源限制,流程的优先级、会计和控制功能。

### 7.3 Access Control and System Call Filtering

A third set of relevant features in the Linux Kernel evolved around secure isolation of processes through restricted access to system calls. In 2000, Cowan et al. [60] released SubDomain, a Linux Kernel module that added access control checks to a limited set of system calls related to executing processes. In 2001, Loscocco and Smalley [126] published an architectural description of SELinux, which implemented mandatory access control (MAC) for the Linux Kernel. The access control architecture of SELinux was received positively, but the implementation was rejected for being too tightly coupled with the kernel. So, in 2002, Wright et al. [216] proposed the Linux Security Module (LSM) framework as a more general approach to extensible security in the Linux Kernel, which made it possible for security policies to be loaded as Kernel modules. LSM is not an access control mechanism, but it provides a set of hooks where other security extensions such as SELinux or AppArmor can insert access control checks. LSM and a modified version of SELinux based on LSM were both merged into the mainline Linux Kernel in 2003. In 2004 to 2005, SubDomain was rewritten to use LSM and rebranded under the name AppArmor.

Linux内核中的第三组相关特性是围绕通过对系统调用的受限访问来实现进程的安全隔离而发展起来的。2000，Cowan 等人 [60] 发布了 SubDomain，这是一个 linux内核模块，它为有限的一组与执行进程相关的系统调用添加了访问控制检查。2001，loscoco 和 Smalley [126] 发布了 SELinux 的架构描述，实现了 Linux 内核的强制访问控制 (MAC)。SELinux 的访问控制架构得到了肯定的接受，但是由于与内核耦合过紧而拒绝了实现。因此，2002年，Wright 等人 [216] 提出了 Linux 安全模块 (LSM) 框架作为在 linux内核中实现可扩展安全性的更通用的方法,这使得安全策略可以作为内核模块加载。LSM 不是访问控制机制，但它提供了一组钩子，其他安全扩展 (如 SELinux 或 AppArmor) 可以在其中插入访问控制检查。2003年，LSM 和基于 LSM 的 SELinux 的修改版本都被合并到主线 Linux 内核中。在 2004 2005年，子域被重写为使用 LSM 并以名称 AppArmor 重新命名。

In 2005, Arcangeli [22] released a set of patches to the Linux Kernel called seccomp for “secure computing,” which restricted a process so that it could only run an extremely limited set of system calls to exit/return or interact with already open filehandles and terminated a process attempting to run any other system calls. The patches were merged into the mainline Kernel later that year. However, the features of the original seccomp were inadequate and rarely used, and over the years multiple proposals to improve seccomp were unsuccessful. Then, in 2012, Drewry [68] extended seccomp to allow filters for system calls to be dynamically defined using Berkeley Packet Filter (BPF) rules, which provided enough flexibility to make seccomp useful as an isolation technique. In 2013, Krude and Meyer [115] implemented a framework for isolating untrusted workloads on multitenant infrastructures using seccomp system call filter policies written in BPF.

2005，Arcangeli [22] 发布了一组名为 seccomp 的 linux内核补丁，用于 “安全计算”,它限制了一个进程，以便它只能运行一组极其有限的系统调用来退出/返回或与已经打开的文件句柄交互，并终止了试图运行任何其他系统调用的进程。那年晚些时候，这些补丁被合并到主线内核中。然而，原始 seccomp 的特征不充分，很少使用，多年来，多个改进 seccomp 的建议都没有成功。然后，2012年，Drewry [68] 扩展了 seccomp，允许使用 Berkeley Packet Filter (BPF) 规则动态定义系统调用的过滤器,这提供了足够的灵活性，使 seccomp 作为一种隔离技术非常有用。在 2013，Krude 和 Meyer [115] 使用 BPF 中编写的 seccomp 系统调用过滤策略实现了一个隔离多租户基础设施上不受信任的工作负载的框架。

### 7.4 Cluster Management

A fourth relevant strand of technology evolved around resource sharing in large-scale cluster management. In 2001, Lottiaux and Morin [127] used the term container for a form of shared, distributed memory that provided the illusion that multiple nodes in an SMP cluster were sharing kernel resources, including memory, disk, and network. In 2002, the Zap project [154] used the term pod17 for a group of processes sharing a private namespace, which had an isolated view of system resources such as process identifiers and network addresses. These pods were self-contained, so they could be migrated as a unit between physical machines. In the mid-2000s, Google deployed a cluster management solution called Borg [42, 196] into production, to orchestrate the deployment of their vast suite of web applications and services. Although the code for Borg has never been seen outside Google, it was the direct inspiration for the Kubernetes project a decade later [196, p.18:13–18:14]—the Borg alloc became the Kubernetes pod, Borglets became Kubelets, and tasks gave way to containers. Burns et al. [42, p. 70] explained that improving performance through resource utilization was one of the primary motivations for Borg.

第四个相关技术链围绕大规模集群管理中的资源共享展开。在 2001，Lottiaux 和 Morin [127] 将术语 container 用于一种共享的分布式内存形式，它提供了一种错觉，即 SMP 集群中的多个节点共享内核资源，包括内存,磁盘和网络。在 2002 中，Zap 项目 [154] 将术语 pod 用于共享私有命名空间的一组进程，该进程具有系统资源的独立视图，例如进程标识符和网络地址。这些 pod 是独立的，因此它们可以作为一个单元在物理机器之间迁移。在 21世纪00年代中期中期，谷歌在生产中部署了一个名为 Borg [196] 的集群管理解决方案，以协调他们庞大的网络应用和服务套件的部署。虽然博格的代码从未在谷歌之外出现过，但它是十年后 Kubernetes 项目的直接灵感 [196，p.18:13-18:14]-Borg alloc 变成了 Kubernetes pod，Borglets 变成了 Kubelets，任务让位于容器。伯恩斯等人 [42，第 70 页] 解释说，通过资源利用提高绩效是博格的主要动机之一。

### 7.5 Combined Features

The strength of modern containers is not in any one feature but in the combination of multiple features for resource control and isolation. In 2008, Linux Containers (LXC) [6] combined cgroups, namespaces, and capabilities from the Linux Kernel into a tool for building and launching lowlevel system containers. Miller and Chen [142] demonstrated that filesystem isolation between LXC containers could be improved by applying SELinux policies. Xavier et al. [219] and Raho et al. [166] contrasted LXC’s approach to isolation and resource control using standard Linux Kernel features such as cgroups and filesystem, process, IPC, and network namespaces, versus the approaches taken by Linux VServer and OpenVZ using custom patches to the Linux Kernel to provide similar features.

现代容器的优势不在于任何一个特性，而在于用于资源控制和隔离的多个特性的组合。在 2008，Linux Containers (LXC) [6] 将 cgroups 、命名空间和来自 linux内核的功能组合成一个工具，用于构建和启动低级系统容器。Miller 和 Chen [142] 证明了 LXC 容器之间的文件系统隔离可以通过应用 SELinux 策略得到改善。Xavier 等人 [219] 和 Raho 等人 [166] 对比了 LXC 使用标准 linux内核特性的隔离和资源控制方法，例如 cgroups 和文件系统，进程，IPC,和网络命名空间，与 Linux VServer 和 OpenVZ 使用自定义补丁为 Linux 内核提供类似功能所采取的方法相比。

Docker [141] launched in 2013 as a container management platform built on LXC. In 2014, Docker replaced LXC with libcontainer, its own implementation for creating containers, which also used Linux Kernel namespaces, cgroups, and capabilities [99, 166]. Morabito et al. [144] compared the performance of LXC and Docker after the transition to libcontainer and found them to be roughly equivalent on CPU performance, disk I/O, and network I/O; however, LXC performed 30% better on random writes, which may have been related to Docker’s use of a union file system. Raho et al. [166] contrasted the implementations of Docker, QEMU+KVM, and Xen on the ARM hardware architecture. Mattetti et al. [134] experimented with dynamically generating AppArmor rules for Docker containers based on the applicationworkload they contained. Catuogno and Galdi [45] performed a case study of Docker using two different models for security assessment. They built on the work of Reshetova et al. [167] in classifying vulnerabilities by the goal of the attack: denial of service, container compromise, or privilege escalation.

Docker [141] 于 2013年推出，作为基于 LXC 构建的容器管理平台。在 2014 中，Docker 用自己的用于创建容器的实现 libcontainer 取代了 LXC，后者还使用了 linux内核命名空间、 cgroups 和 capability [99，166]。Morabito 等 [144] 比较了 LXC 和 Docker 在过渡到 libcontainer 后的性能，发现它们在 CPU 性能、磁盘 I/O 和网络 I/O 上大致相当; 然而，LXC 在随机写入上的表现要好 30%，这可能与 Docker 使用 union 文件系统有关。Raho 等人 [166] 对比了 Docker 、 QEMU + KVM 和 Xen 在 ARM 硬件架构上的实现。Mattetti 等 [134] 试验了基于包含的应用程序工作负载动态生成 Docker 容器的 AppArmor 规则。Catuogno 和 Galdi [45] 使用两种不同的模型进行了 Docker 的案例研究，用于安全评估。他们建立在 Reshetova 等人 [167] 的工作基础上，通过攻击的目标对漏洞进行分类: 拒绝服务、容器妥协或特权升级。

In 2015, Docker split the container runtime out into a separate project, runc, in support of a vendor-neutral container runtime specification maintained by the Open Container Initiative (OCI). Hykes [100] highlighted that SELinux, AppArmor, and seccomp were all standard supported features in runc. Koller and Williams [113] observed that runc was more minimal than the Docker runtime while still using the same isolation mechanisms from the Linux Kernel, such as namespaces and cgroups. In 2016, Docker and CoreOS merged their container image formats into a vendor-neutral container image format specification, also at OCI [36].

在 2015 中，Docker 将容器运行时拆分为一个单独的项目 runc，以支持由 Open container Initiative (OCI) 维护的供应商中立的容器运行时规范。Hykes [100] 强调了 SELinux 、 AppArmor 和 seccomp 都是 runc 中支持的标准特性。Koller 和 Williams [113] 观察到 runc 比 Docker 运行时更小，同时仍然使用与 linux内核相同的隔离机制，例如命名空间和 cgroups。在 2016，Docker 和 CoreOS 将它们的容器镜像格式合并为供应商中立的容器镜像格式规范，同样在 OCI [36]。

### 7.6 Orchestration

In 2014, Docker began working on Swarm, described as a clustering system for Docker, which they ultimately released late in 2015 [128]. Also in 2014, Google began developing Kubernetes, an orchestration tool for deploying and managing the lifecycle of containers, which they released in the middle of 2015 [38]. Also in 2014, Canonical began developing LXD, a container orchestration tool for LXC containers, which they released in 2016 [89].

在 2014，Docker 开始在 Swarm 上工作，被描述为 Docker 的集群系统，他们最终在 2015年末发布了 [128]。同样在 2014年，Google 开始开发 Kubernetes，这是一种用于部署和管理容器生命周期的编排工具，他们在 2015年发布了这个工具 [38]。同样在 2014年，Canonical 开始开发 LXD，一种用于 LXC 容器的容器编排工具，并于 2016年发布 [89]。

Verma et al. [196] outlined the design goals behind Kubernetes, in the context of lessons learned from Borg. Syed and Fernandez [189, 190] pointed out that the performance advantages of the higher-level container orchestration tools, such as Kubernetes and Docker Swarm, were primarily a matter of improving resource utilization. They also contrasted the portability advantages of managing containers across multiple physical host machines against the increased complexity required for the orchestration tools to advance beyond managing a single machine host. Souppaya et al. [184] systematically reviewed increased security risks and mitigation techniques for container orchestration tools. Bila et al. [34] extended Kubernetes with a vulnerability scanning service and network quarantine for containers.

Verma 等人 [196] 在 Borg 经验教训的背景下概述了 Kubernetes 背后的设计目标。Syed 和 Fernandez [189，190] 指出，Kubernetes 和 Docker Swarm 等更高级别的容器编排工具的性能优势主要在于提高资源利用率。他们还将跨多个物理主机管理容器的可移植性优势与编排工具超越管理单个机器主机所需的复杂性进行了对比。Souppaya 等人 [184] 系统地回顾了容器编排工具增加的安全风险和缓解技术。Bila et al. [34] 扩展 Kubernetes，具有漏洞扫描服务和容器网络隔离。

### 7.7 Trade-Offs

Containers have a reputation for substantially better performance than virtual machines; however, that reputation may not be deserved. In 2015, Felter et al. [76] measured the performance of Docker against QEMU+KVM and determined that neither had significant overhead on CPU and memory usage, but that KVM had a 40% higher overhead in I/O. They observed that the overhead was primarily due to extra cycles on each I/O operation, so the impact could be mitigated for some applications by batching multiple small I/O operations into fewer large I/O operations. In 2017, Kovács [114] compared CPU execution time and network throughput between Docker, LXC, Singularity, KVM, and bare metal, and determined that there was no significant variation between them, as long as Docker and LXC were running in host networking mode, but in Linux bridge mode Docker and LXC exhibited high retransmission rates that negatively impacted their throughput compared to the others. Manco et al. [131] demonstrated that Xen virtual machine instantiation could be 50 to 200 times faster than Docker container instantiation, with a few lowlevel modifications to Xen’s control stack.

容器以性能明显优于虚拟机而闻名; 但是，这种声誉可能不值得。在 2015，Felter 等人 [76] 针对 QEMU + KVM 测量了 Docker 的性能，并确定两者在 CPU 和内存使用上都没有显著的开销,但是 KVM 在 I/O 方面的开销要高 40%。他们观察到开销主要是由于每个 I/O 操作的额外周期,因此，通过将多个小型 I/O 操作批量转换为较少的大型 I/O 操作，可以减轻对某些应用程序的影响。在 2017，kov á cs [114] 比较了 Docker 、 LXC 、奇点、 KVM 和裸机之间的 CPU 执行时间和网络吞吐量，并确定它们之间没有显著变化,只要 Docker 和 LXC 在主机联网模式下运行,但是在 Linux 桥接模式下，Docker 和 LXC 表现出较高的重传速率，与其他模式相比，这对它们的吞吐量产生了负面影响。Manco 等人 [131] 证明了 Xen 虚拟机实例化可以比 Docker 容器实例化快 50 到 200 倍，只需对 Xen 的控制堆栈进行一些低级修改。

Secure isolation technologies have been the core of modern container implementations from the beginning, so it would be reasonable to expect that containers would provide a strong form of isolation. However, early implementations of containers were prone to preventable security vulnerabilities, which may indicate that security was not a primary design consideration, at least not initially. Combe et al. [54] analyzed security vulnerabilities in Docker and libcontainer between 2014 and 2015, and determined that the majority were related to filesystem isolation, which led to privilege escalation when Docker was run as the root user. They also suggested that some of Docker’s sane default configurations for the isolation features of the Linux Kernel could be easily switched to less secure configurations through standard options to the docker command-line tool or the Docker daemon, and so might be prone to user error. Martin et al. [133] surveyed vulnerabilities in Docker images, libcontainer, the Docker daemon, and orchestration tools, as well as the unique security challenges of containers in multitenant infrastructures. In addition to security patches for specific privilege escalation vulnerabilities, there has been ongoing work to integrate support for user namespaces into Docker and Kubernetes,19 so they can run as a non-root user and limit the scope of damage from privilege escalation. However, the user namespaces feature itself has had a series of vulnerabilities20 related to interfaces in the Kernel that were written with the expectation of being restricted to the root user but are now exposed to unprivileged users.

安全隔离技术从一开始就是现代容器实现的核心，因此可以合理地预期容器将提供强大的隔离形式。然而，容器的早期实现容易出现可预防的安全漏洞，这可能表明安全性不是主要的设计考虑因素，至少最初不是。Combe 等人 [54] 在 2014年和 2015 分析了 Docker 和 libcontainer 的安全漏洞，并确定大多数与文件系统隔离有关,当 Docker 作为根用户运行时，这会导致权限提升。他们还建议，通过 Docker 命令行工具或 docker 的标准选项，可以轻松地将一些 Docker 针对 linux内核隔离特性的正常默认配置切换到安全性较低的配置。守护进程因此可能容易出现用户错误。Martin 等 [133] 调查了 Docker 镜像、 libcontainer 、 Docker daemon 和编排工具中的漏洞，以及多租户基础设施中容器特有的安全挑战。除了针对特定的权限提升漏洞的安全补丁之外，还一直在努力将对用户命名空间的支持集成到 Docker 和 Kubernetes 中。因此，他们可以作为非根用户运行，并限制权限提升造成的损害范围。但是，用户命名空间特性本身存在一系列与内核中接口相关的漏洞，这些漏洞的编写预期仅限于根用户，但现在暴露于非特权用户。

One significant difference between virtual machine implementations and container implementations is that containers share a kernel with the host operating system, so efforts to secure the kernel greatly impact the security of containers. Reshetova et al. [167] considered the set of secure isolation features offered by the Linux Kernel as of 2014 (in the context of LXC) and judged them to have caught up with the features of FreeBSD Jails and Solaris Zones but highlighted some areas for improvement in support of containers. These improvements included integratingMAC into the Kernel as “security namespaces,” providing a way to lock down device hotplug features for containers and extending cgroups to support all resource management features supported by rlimits. Gao et al. [79] discussed the risks of certain types of information that containers can currently access from the Linux Kernel via procfs and sysfs—which can be exploited to detect co-resident containers and precisely target power consumption spikes to overload servers—and prototyped a power-based namespace to partition the information for containers.

虚拟机实现和容器实现之间的一个显著区别是容器与主机操作系统共享内核，因此保护内核的努力极大地影响了容器的安全性。Reshetova 等人 [167] 认为 linux内核提供的一组安全隔离特性为 2014年 (在 LXC 的上下文中) 并判断他们已经赶上了 FreeBSD 监狱和 Solaris 区域的功能，但强调了支持容器的一些改进领域。这些改进包括将 mac 集成到内核中作为 “安全名称空间”，提供了一种锁定容器设备热插拔特性的方法，并扩展了 cgroups 以支持 rlimits 支持的所有资源管理特性。Gao 等人 [79] 讨论了容器当前可以通过 procfs 和 sysfs 从 linux内核访问的某些类型信息的风险 -- 这些信息可以被用来检测共同驻留的容器并精确地目标功耗峰值以使服务器过载-并原型化了基于 power 的命名空间以对容器的信息进行分区。

Some more recent approaches to secure isolation for containers have been inspired by virtual machine implementations. Kata Containers (formerly Intel Clear Containers) [5] wraps each Docker container or Kubernetes pod in a QEMU+KVM virtual machine [12]. They realized that QEMU was not ideal for the purpose—since it introduces a substantial performance hit compared to running bare containers, and the majority of the code relates to emulation that is not useful for wrapping containers—so a group at Intel started working on a stripped-down version of QEMU called NEMU [8]. X-Containers [179] used Xen’s paravirtualization features to improve isolation between containers and the host but made an unfortunate trade-off of removing isolation between containers running on the same host. Nabla Containers [7] and gVisor [88] have both taken an approach of improving isolation by heavily filtering system calls from containers to the host kernel, which is a common technique for modern virtual machines.

最近的一些容器安全隔离方法受到虚拟机实现的启发。Kata Containers (以前称为 Intel Clear Containers) [5] 将每个 Docker 容器或 Kubernetes pod 包装在 QEMU + KVM 虚拟机中 [12]。他们意识到 QEMU 并不理想-因为它与运行裸容器相比带来了巨大的性能冲击,大多数代码都与仿真有关，而仿真对于包装容器是没有用的 -- 因此英特尔的一个小组开始开发一个名为 NEMU [8] 的精简版本的 QEMU。X-Containers [179] 使用 Xen 的半虚拟化特性来提高容器和主机之间的隔离，但在删除运行在同一主机上的容器之间的隔离时做出了不幸的权衡。Nabla Containers [7] 和 gVisor [88] 都采取了通过从容器到主机内核的重度过滤系统调用来提高隔离度的方法，这是现代虚拟机的常用技术。

Bratus et al. [37] noted that the “self-protection” techniques employed by container implementations are a necessary path for future research, since even virtual machines depend on those techniques to protect themselves. Hosseinzadeh et al. [95] explored the possibility that container implementationsmight directly adapt earlier work (primarily Berger et al. [30]) for virtual machine implementations to integrate a Trusted Platform Module (TPM) as a virtual device.

Bratus 等人 [37] 指出，容器实现采用的 “自我保护” 技术是未来研究的必要途径，因为甚至虚拟机也依赖于这些技术来保护自己。Hosseinzadeh 等人 [95] 探讨了容器实现可能直接适应虚拟机实现的早期工作 (主要是 Berger 等人 [30]) 以集成可信平台模块 (TPM) 的可能性作为虚拟设备。

Container implementations have a potential advantage over virtual machine implementations in addressing the problem of secure isolation over the long-term, not because any existing implementations are inherently superior but because containers take a modular approach to implementation that permits them to be more flexible over time and across different underlying software and hardware architectures, as new ideas for secure isolation evolve.

容器实现在解决长期安全隔离问题方面比虚拟机实现具有潜在优势不是因为任何现有的实现本质上都是优越的，而是因为容器采用模块化的实现方法，允许它们随着时间的推移和跨不同的底层软件和硬件架构更加灵活,随着安全隔离的新想法的发展。

## 8 SECURITY OUTLOOK

A series of vulnerabilities related to speculative execution and side-channel attacks rose to attention early in 2018. These vulnerabilities collectively upend traditional notions of secure isolation. The current reactionary approach—patching up each vulnerability as it is revealed—works in the short term but is a losing battle in the long term.

2018年初，一系列与投机执行和侧通道攻击相关的漏洞受到关注。这些漏洞颠覆了安全隔离的传统观念。当前的反动做法——修补每一个暴露出来的弱点——短期内有效，但长期来看是一场必败之仗。

Early in 2018, Kocher et al. [111] and Lipp et al. [124] published a set of vulnerabilities, respectively called Spectre and Meltdown, using techniques involving speculative execution and out-oforder execution. Spectre affects Intel, AMD, and ARM [111, p. 3], can be launched from any user process (including JavaScript code run in a browser) [111, p. 3], and grants access to any memory an attacked process could normally access [111, p. 5]. Meltdown affects Intel x86 architecture, can be launched from any user process, and grants full access to any physical memory on the same machine including kernel memory and memory allocated to any other process [124, p. 1]. In July 2018, Schwarz et al. [177] published a remote variant of Spectre, nicknamed NetSpectre, which is launched through packets over the network and grants access to any physical memory accessible to the attacked process. In August 2018, Van Bulck et al. [193] published a variant of Meltdown, nicknamed Foreshadow or more broadly “L1 Terminal Fault” (L1TF), which is launched from unprivileged user space, and grants access to the L1 data cache, including encrypted data from Intel’s Software Guard eXtensions (SGX). In November 2018, Canella et al. [44] reviewed the broad range of speculative execution vulnerabilities and proposed a comprehensive classification of the known variants and mitigations, which also revealed several previously unknown variants.

- [111] Paul Kocher, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, Moritz Lipp, Stefan Mangard, Thomas Prescher, Michael Schwarz, and Yuval Yarom. 2018. Spectre attacks: Exploiting speculative execution. arXiv:1801.01203.
- [124] Moritz Lipp, Michael Schwarz, Daniel Gruss, Thomas Prescher, Werner Haas, Stefan Mangard, Paul Kocher, Daniel Genkin, Yuval Yarom, and Mike Hamburg. 2018. Meltdown. arXiv:1801.01207.
- [177] Michael Schwarz, Martin Schwarzl, Moritz Lipp, and Daniel Gruss. 2018. NetSpectre: Read arbitrary memory over network. arXiv:1807.10535.
- [193] Jo Van Bulck, Marina Minkin, Ofir Weisse, Daniel Genkin, Baris Kasikci, Frank Piessens, Mark Silberstein, Thomas F. Wenisch, Yuval Yarom, and Raoul Strackx. 2018. Foreshadow: Extracting the keys to the Intel SGX kingdom with transient out-of-order execution. In Proceedings of the 27th USENIX Security Symposium (USENIX Security’18). 991–1008.
- [44] Claudio Canella, Jo Van Bulck, Michael Schwarz, Moritz Lipp, Benjamin von Berg, Philipp Ortner, Frank Piessens, Dmitry Evtyushkin, and Daniel Gruss. 2018. A systematic evaluation of transient execution attacks and defenses. arXiv:1811.05441.

2018年初，Kocher等人[111]和Lipp等人[124]发表了一组漏洞，分别称为Spectre和Meltdown，使用的技术包括投机执行和非投机执行。Spectre影响Intel、AMD和ARM[111，第3页]，可以从任何用户进程(包括在浏览器中运行的JavaScript代码)[111，第3页]启动，并授予被攻击进程可以正常访问的任何内存[111，第5页]。Meltdown影响Intel x86架构，可以从任何用户进程启动，并授予对同一机器上任何物理内存的完全访问权，包括内核内存和分配给任何其他进程的内存[124,p. 1]。2018年7月，Schwarz等人[177]发布了Spectre的一个远程变体，昵称为NetSpectre，它通过网络上的数据包发射，授予对被攻击进程可访问的任何物理内存的访问权。2018年8月，Van Bulck等人[193]发表了Meltdown的一种变体，昵称为预测或更广泛地称为“L1终端故障”(L1TF)，它从非特权用户空间启动，授予对L1数据缓存的访问权，包括来自英特尔软件保护扩展器(SGX)的加密数据。2018年11月，Canella等人[44]回顾了投机性执行漏洞的广泛范围，并对已知的变种和缓解措施进行了全面分类，同时也揭示了几个之前未知的变种。

The models of secure isolation employed by virtual machines and containers offer little protection from the speculative execution vulnerabilities. Containers are vulnerable to Meltdown, although virtual machines are not because they run a different kernel than the host [124, p. 12]. Both virtual machines and containers are vulnerable to Spectre [10, pp. 3, 5, 6], NetSpectre [177, p. 11], and L1TF [205], with varying degrees of compromise. Variants of L1TF23 are especially troublesome for virtual machines, because they allow an unprivileged process in the user space of a guest to access any memory on the physical machine, including memory allocated to other guests, the host operating system, and host kernel [13]. Multitenant infrastructures generally allow any tenant to deploy a virtual machine or container on any physical machine in the cloud or cluster, which means that it is viable to exploit these vulnerabilities by simply creating an account with a public provider and deploying malicious guests repeatedly, until one of them lands on a physical host with interesting secrets to steal.

虚拟机和容器使用的安全隔离模型对投机性执行漏洞提供的保护很少。容器很容易崩溃，尽管虚拟机不会，因为它们运行的内核与主机不同[124，第12页]。虚拟机和容器都容易受到Spectre[10，第3，第5，第6]，NetSpectre[177，第11页]和L1TF[205]的攻击，它们都有不同程度的折衷。L1TF23的变体对于虚拟机尤其麻烦，因为它们允许客户机用户空间中的非特权进程访问物理机器上的任何内存，包括分配给其他客户机、主机操作系统和主机内核[13]的内存。多租户基础设施通常允许任何租户任何物理机器上部署一个虚拟机或容器在云中或集群,这意味着它是可行的,利用这些漏洞,只需创建一个帐户与公共提供者和部署恶意客人反复,直到其中一个落在一个物理主机与有趣的秘密窃取。

The techniques behind the speculative execution vulnerabilitieswere not new, but the combined application of the techniques was more sophisticated, and the security impact more severe, than previously considered possible. Although these vulnerabilities were only recently discovered and published by defensive security researchers,24 it is possible that offensive security researchers25 discovered and exploited them much earlier, and continue to exploit additional unpublished variants. Although mitigation patches have typically been applied quickly for the known variants of these vulnerabilities [9, 10], it is not feasible to entirely disable speculative execution [111, p. 11] and out-of-order execution [124, p. 14], which are the primary vectors of the attacks, because the performance penalty is prohibitive, and in some cases the hardware simply has no mechanism to disable the features. The probability of further variants being discovered in the coming years is high. A substantial rethink of the fundamental hardware architecture could potentially eliminate the entire class of vulnerabilities, but in the research, development, and production timelines common to hardware vendors, such a significant change could take decades.

投机性执行漏洞背后的技术并不是新的，但是这些技术的组合应用比以前认为的可能更复杂，安全影响也更严重。尽管防御安全研究人员最近才发现并发布了这些漏洞，但攻击安全研究人员很可能在更早的时候就发现并利用了它们，并继续利用其他未发布的变种。虽然缓解补丁通常很快申请这些漏洞的已知的变异(9、10)是不可行的完全禁用投机执行(111,11页)和无序执行(124,14页),主向量的攻击,因为性能损失是禁止的,在某些情况下,硬件只是没有机制来禁用功能。未来几年发现更多变异的可能性很高。对基础硬件架构进行实质性的重新思考，可能会消除这类漏洞，但在硬件供应商共同的研究、开发和生产时间表中，这样一个重大的改变可能需要几十年的时间。

Two notable alternative hardware architectures, CHERI and RISC-V, were already under development before the flood of speculative execution vulnerabilities were published. CHERI [215] combines concepts from classic capability systems and RISC architectures, with a strong emphasis on memory protection. RISC-V [24] is a RISC-based hardware architecture, aimed at providing an extensible open source instruction set architecture (ISA) used as an industry standard by a broad array of hardware vendors. Neither CHERI nor RISC-V were designed with speculative execution vulnerabilities in mind, but Watson et al. [204] observed that CHERI mitigates some aspects of Spectre and Meltdown but is vulnerable to speculative memory access, whereas Asanović and O’Connor [23] announced that RISC-V is not vulnerable because it does not perform speculative memory access. In August 2018, Google announced that the open source implementation of its Titan project, providing a hardware root of trust, will likely be based on RISC-V [169]. MIT’s Sanctum processor [59] was also based on RISC-V and demonstrated potential for secure hardware partitioning by adding a small secure CPU to the side of the main CPU. Hardware partitioning might provide a way to mitigate the speculative execution vulnerabilities in multitenant environments while avoiding major changes to the kernel and operating system. However, genuinely delivering the level of physical isolation that x86 promised would likely require logical partitioning of the main CPU, RAM, and cache of the machine, so the guests and the host operating system could share resources at the hardware level but be far more restricted at the software level than is currently possible.

两个值得注意的替代硬件架构，CHERI和RISC-V，在投机的执行漏洞泛滥之前就已经在开发中了。CHERI[215]结合了来自经典能力系统和RISC架构的概念，并着重于内存保护。[24]是一种基于risc的硬件架构，旨在提供一种可扩展的开放源码指令集架构(ISA)，被广泛的硬件供应商用作行业标准。谢利和RISC-V设计与投机执行漏洞,但沃森et al。[204]发现谢利,缓解了幽灵的某些方面和崩溃,但容易受到投机性内存访问,而Asanović和奥康纳[23]宣布RISC-V不是脆弱的,因为它不执行投机内存访问。2018年8月，谷歌宣布其Titan项目的开源实现可能基于RISC-V[169]，该项目提供了信任的硬件根目录。MIT的Sanctum处理器[59]也是基于RISC-V，通过在主处理器的侧面添加一个小型安全CPU，显示了安全硬件分区的潜力。硬件分区可以减少多租户环境中的投机性执行漏洞，同时避免对内核和操作系统进行重大更改。然而，真正实现x86所承诺的物理隔离级别可能需要对机器的主CPU、RAM和缓存进行逻辑分区，因此来宾操作系统和主机操作系统可以在硬件级别上共享资源，但在软件级别上受到的限制比目前可能的要大得多。

The problem of providing secure isolation for containers and virtual machines extends beyond simple refinements to their implementations.When the fundamental assumptions of a system are proven false, then any theorems built on those assumptions may also be false. The secure isolation features of the full stack—from the kernel and operating system, through to virtual machines, containers, and application workloads—are all built on false assumptions about the behavior of the hardware and will need to be re-examined.

为容器和虚拟机提供安全隔离的问题超出了对其实现的简单改进。当一个系统的基本假设被证明是错误的，那么任何建立在这些假设上的定理也可能是错误的。完整堆栈的安全隔离特性——从内核和操作系统，到虚拟机、容器和应用程序工作负载——都建立在关于硬件行为的错误假设之上，需要重新检查。

## 9 RELATED IMPLEMENTATIONS

Implementation approaches that adopt the label “cloud” [67, 97, 125, 180] are typically virtual machines with added orchestration features to enhance portability. Cloud implementations also tend to favor lighter-weight guest images, which enhances performance and reduces complexity, although cloud images are generally not quite as minimal as container images.

采用“云”标签的实现方法[67、97、125、180]通常是添加了编排功能以增强可移植性的虚拟机。云实现也倾向于更轻量级的客户映像，这可以提高性能并降低复杂性，尽管云映像通常不像容器映像那样最少。

Implementation approaches that adopt the label “unikernel” [117, 129, 212] takeminimalist guest images to an extreme, by replacing the kernel and operating system of the guest with a set of highly optimized libraries that provide the same functionality. The code for an application workload is compiled together with the small subset of unikernel libraries required by the application, resulting in a very small binary that runs directly as a guest image. Historically, unikernels have sacrificed portability of guest images, by targeting only a limited set of virtual machine implementations as their host, but recent work has begun exploring running unikernels as containers [213]. The unikernel approach also reduces the portability of application code, since unikernel frameworks tend to require the application code to be written in the same language as the unikernel libraries.

采用标签“unikernel”[117,129,212]的实现方法将客户映像发挥到极致，用一组提供相同功能的高度优化的库替换客户的内核和操作系统。应用程序工作负载的代码与应用程序所需的一小部分unikernel库一起编译，生成一个非常小的二进制文件，直接作为客户映像运行。从历史上看，unikernels已经牺牲了客户映像的可移植性，只将有限的一组虚拟机实现作为它们的主机，但是最近的工作已经开始探索将unikernels作为容器来运行[213]。unikernel方法还降低了应用程序代码的可移植性，因为unikernel框架倾向于要求应用程序代码用与unikernel库相同的语言编写。

Implementation approaches that adopt the label “serverless” [17, 93, 106, 113] tend to emphasize portability andminimizing complexity. They rely on the underlying infrastructure—typically some combination of bare metal, virtual machines, and/or containers—for whatever secure isolation and performance they provide.

采用“无服务器”标签的实现方法[17、93、106、113]往往强调可移植性和最小化复杂性。它们依赖底层基础设施(通常是裸机、虚拟机和/或容器的某种组合)来实现它们所提供的任何安全隔离和性能。

## 10 CONCLUSION

A detailed examination of the history of virtual machines and containers reveals that the two have evolved in tandem from the very beginning. It also reveals that both families of technology are facing significant challenges in providing secure isolation for modern multitenant infrastructures. In light of recent vulnerabilities, patching up existing tools is a necessary and valuable activity in the short term but is not sufficient for the long term. In the coming decades, the computing industry as a whole will need to embrace more radical alternatives in both hardware and software. Current researchers and developers can benefit from a deeper understanding of how virtual machines and containers evolved—and the trade-offs made along the way—to make more informed choices for tomorrow, avoid repeating past mistakes, and build on a solid foundation toward new paths of exploration.

对虚拟机和容器历史的详细研究表明，它们从一开始就协同发展。它还揭示了这两种技术在为现代多租户基础设施提供安全隔离方面都面临着重大挑战。鉴于最近的漏洞，修补现有工具在短期内是必要的和有价值的活动，但从长期来看是不够的。在未来的几十年里，计算机行业作为一个整体将需要在硬件和软件上采用更激进的替代品。当前的研究人员和开发人员可以从对虚拟机和容器如何进化的更深入的理解中获益——以及在此过程中所做的权衡——为未来做出更明智的选择，避免重复过去的错误，并为新的探索道路建立坚实的基础。
