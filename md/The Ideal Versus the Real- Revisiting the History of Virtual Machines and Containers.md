# The Ideal Versus the Real: Revisiting the History of Virtual Machines and Containers

The common perception in both academic literature and industry today is that virtual machines offer better security, whereas containers offer better performance. However, a detailed review of the history of these technologies and the current threats they face reveals a different story. This survey covers key developments in the evolution of virtual machines and containers from the 1950s to today, with an emphasis on countering modern misperceptions with accurate historical details and providing a solid foundation for ongoing research into the future of secure isolation for multitenant infrastructures, such as cloud and container deployments.

当今学术文献和行业的普遍看法是，虚拟机提供了更好的安全性，而容器提供了更好的性能。然而，对这些技术的历史和它们面临的当前威胁的详细回顾揭示了一个不同的故事。这项调查涵盖了从 20世纪50年代中期到今天虚拟机和容器演变的关键进展,重点是用准确的历史细节来对抗现代的误解，并为多租户基础设施 (如云和容器部署) 的安全隔离未来的持续研究提供坚实的基础。

## 1 INTRODUCTION

Many modern computing workloads run in multitenant environments, where each physical machine is split into hundreds or thousands of smaller units of computing, generically called guests. Cloud and containers are currently the leading approaches to implementing multitenant environments. The guests in a cloud deployment are commonly called virtual machines or cloud instances, whereas the guests in a container deployment are commonly called containers. Typically, a single tenant (a user or group of users) is granted access to deploy guests in an orchestrated fashion across a cloud or cluster made up of hundreds or thousands of physical machines located in the same data center or across multiple data centers, to facilitate operational flexibility in areas such as capacity planning, resiliency, and reliable performance under variable load. Each guest runs its own (often minimal) operating system and application workloads, and maintains the illusion of being a physical machine, both to the end users who interact with the services running in the guests, and to developers who are able to build those services using familiar abstractions, such as programming languages, libraries, and operating system features. The illusion, however, is not perfect, because ultimately the guests do share the hardware resources (CPU, memory, cache, devices) of the underlying physical host machine, and consequently also have greater access to the host’s privileged software (kernel, operating system) than a physically distinct machine would have.

许多现代计算工作负载在多租户环境中运行，其中每个物理机器被分割成数百或数千个更小的计算单元，通常称为来宾计算单元。云和容器是当前实现多租户环境的主要方法。云部署中的客户机通常称为虚拟机或云实例，而容器部署中的客户机通常称为容器。通常情况下,一个租户(用户)的用户或组授予访问跨云部署客人精心策划的方式或集群由成百上千的物理机器位于相同的数据中心或跨多个数据中心,便于操作的灵活性在容量规划等领域,弹性、可靠变量负荷条件下的性能。每个客人运行自己的(通常是最小的)操作系统和应用程序工作负载,并维护作为一个物理机器的错觉,对最终用户与服务交互中运行的客人,和开发人员能够使用熟悉的抽象,构建这些服务编程语言、库和操作系统等功能。然而，这种错觉并不完美，因为最终来宾确实共享底层物理主机的硬件资源(CPU、内存、缓存、设备)，因此对主机的特权软件(内核、操作系统)的访问权限也比物理上不同的机器要大。

Ideally, multitenant environments would offer strong isolation of the guest from the host, and between guests on the same host, but reality falls short of the ideal. The approaches that various implementations have taken to isolating guests have different strengths and weaknesses. For example, containers share a kernel with the host, whereas virtual machines may run as a process in the host operating system or a module in the host kernel, so they expose different attack surfaces through different code paths in the host operating system. Fundamentally, however, all existing implementations of virtual machines and containers are leaky abstractions, exposing more of the underlying software and hardware than is necessary, useful, or desirable. New security research in 2018 delivered a further blow to the ideal of isolation in multitenant environments, demonstrating that certain hardware vulnerabilities related to speculative execution—including Spectre, Meltdown, Foreshadow, L1TF, and variants—can easily bypass the software isolation of guests.

理想情况下，多租户环境将提供客户机与主机以及同一主机上的客户机之间的强隔离，但现实情况并不理想。各种实现用来隔离来宾的方法有不同的优点和缺点。例如，容器与主机共享一个内核，而虚拟机可以作为主机操作系统中的进程或主机内核中的模块运行，因此它们通过主机操作系统中的不同代码路径暴露不同的攻击面。然而，从根本上说，所有现有的虚拟机和容器实现都是有漏洞的抽象，暴露的底层软件和硬件比必要的、有用的或需要的要多。2018年的新安全研究进一步打击了在多租户环境中隔离的理想，表明与投机执行相关的某些硬件漏洞——包括Spectre、Meltdown、Foreshadow、L1TF和变体——可以很容易地绕过来宾的软件隔离。

Because multitenancy has proven to be useful and profitable for a large sector of the computing industry, it is likely that a significant percentage of computing workloads will continue to run in multitenant environments for the foreseeable future. This is not a matter of naïveté but of pragmatism: these days, the companies who provide and make use of multitenant environments are generally fully aware of the security risks, but they do so anyway because the benefits—such as flexibility, resiliency, reliability, performance, cost, or any of a dozen other factors—outweigh the risks for their particular use cases and business needs. That being the case, it is worthwhile to take a step back and examine how the past 60 years of evolution led to the current tension between secure ideals and flawed reality, and what lessons from the past might help us build more secure software and hardware for the next 60 years.

由于多租户已被证明对计算行业的大部分部门有用且有利可图，因此在可预见的未来，很可能很大一部分计算工作负载将继续在多租户环境中运行。这不是幼稚的问题，而是实用主义的问题: 如今，提供和利用多租户环境的公司通常充分意识到安全风险,但是他们这样做是因为好处 -- 比如灵活性、弹性、可靠性、性能、成本,或其他十几个因素中的任何一个-超过了其特定用例和业务需求的风险。在这种情况下，值得后退一步，审视过去 60 年的演变是如何导致当前安全理想和有缺陷的现实之间的紧张关系的,以及过去的经验教训可能会帮助我们为未来 60 年构建更安全的软件和硬件。

This survey is divided into sections following the evolutionary paths of the technologies behind virtual machines and containers, generally in chronological order, as illustrated in Figure 1. Section 3 explores the common origins of virtual machines and containers in the late 1950s and early 1960s, driven by the architectural shift toward multitasking and multiprocessing, and motivated by a desire to securely isolate processes, efficiently utilize shared resources, improve portability, and minimize complexity. Section 4 examines the first virtual machines in the mid-1960s to 1970s, which primarily aimed to improve resource utilization in time-sharing systems. Section 5 delves into the capability systems of the early 1960s to 1970s—the precursors of modern containers—which evolved along a parallel track to virtual machines, with similar motivations but different implementations. Section 6 outlines the resurgence of virtual machines in the late 1990s and 2000s. Section 7 traces the emergence of containers in the 2000s and 2010s. Section 8 investigates the impact of recent security research on both virtual machines and containers. Section 9 briefly looks at the relationship between virtual machines and containers and the related terms cloud, serverless, and unikernels.

本调查按照虚拟机和容器背后的技术进化路径分为几部分，通常按时间顺序进行，如图 1 所示。第三节探讨了 20世纪50年代末和 20世纪60年代初中期虚拟机和容器的共同起源，由面向多任务和多处理的架构转变驱动,并出于安全隔离流程、高效利用共享资源、提高可移植性和最大限度地减少复杂性的愿望。第 4 节检查了 20世纪60年代中期到 20世纪70年代中期的第一批虚拟机，这些虚拟机主要旨在提高分时系统中的资源利用率。第五节深入研究了 20世纪60年代初到 20世纪70年代的能力体系 -- 现代容器的前身 -- 它沿着虚拟机的平行轨道发展，有着相似的动机，但实现方式不同。第 6 节概述了 20世纪90年代末和 21世纪00年代中期虚拟机的复苏。第七节追溯了 21世纪00年代和 21世纪10年代容器的出现。第 8 节调查了最近的安全研究对虚拟机和容器的影响。第 9 节简要介绍了虚拟机和容器之间的关系以及相关术语 cloud 、 serverless 和 unikernel。

## 2 TERMINOLOGY

For the sake of clarity, this survey consistently uses certain modern or common terms, even when
discussing literature that used various other terms for the same concepts:

- Container: The term container does not have a single origin, but some early relevant examples of use are Banga et al. [25] in 1999, Lottiaux and Morin [127] in 2001, Morin et al. [145] in 2002, and Price and Tucker [164] in 2004. Early literature on containers confusingly referred to them as a kind of virtualization [45, 48, 104, 142, 164, 182], or even called them virtual machines [182]. As containers grew more popular, the confusion shifted to virtual machines being called containers [37, 220]. This survey uses the term container for multitenant deployment techniques involving process isolation on a shared kernel (in contrast with virtual machine, as defined in the following). However, in practice, the distinction between containers and virtual machines is more of a spectrum than a binary divide. Techniques common to one can be effectively applied to the other, such as using system call filtering with containers, or using seccomp sandboxing or user namespaces with virtual machines.

## 3 COMMON ORIGINS

## 4 EARLY VIRTUAL MACHINES

## 5 EARLY CAPABILITIES

## 6 MODERN VIRTUAL MACHINES

Virtual machines still existed in the 1980s and 1990s but garnered only a bare minimum of activity and interest. IBM’s line of VM products, descended from VM/370, continued to have a small but loyal following [194]. DOS, OS/2, and Windows all offered a limited form of DOS virtual machines during that time, although it might be more fair to categorize those as emulation. The rise of programming languages like Smalltalk and Java re-purposing the term virtual machine—to refer to an abstraction layer of a language runtime rather than a software replication of a real hardware architecture—may be indicative of how dead the original concept of virtual machines was in that period.

虚拟机在20世纪80年代和90年代仍然存在，但只吸引了极少的活动和兴趣。IBM的VM系列产品是VM/370的后代，仍然有一小群忠实的追随者[194]。在此期间，DOS、OS/2和Windows都提供了一种有限形式的DOS虚拟机，尽管将它们归类为仿真可能更公平一些。Smalltalk和Java等编程语言的兴起重新定义了虚拟机这个术语——指的是语言运行时的抽象层，而不是对真实硬件体系结构的软件复制——这可能表明了在那个时期，虚拟机的原始概念是多么的过时。

After nearly two decades, the late 1990s brought a resurgence of interest in virtual machines but for a new purpose adapted to the technology of the time.

近20年后，上世纪90年代末，人们对虚拟机的兴趣死灰复燃，但目的是为了适应当时的技术。

###  6.1 Disco

In 1997, the Disco research project at Stanford University explored reviving virtual machines as an approach to making efficient use of hardware with multiple CPUs (on the order of “tens to hundreds”), and included a lightweight library operating system for guests (SPLASHOS) as an option, in addition to supporting commodity operating systems as guests. Bugnion et al. [39] cited portability (rather than security or performance) as the primary motivation of the Disco project, which proposed virtual machines as a potential way to allow commodity operating systems (Unix, Windows NT, and Linux) to run on NUMA architectures without extensive modifications.

1997年,迪斯科斯坦福大学研究项目探讨恢复虚拟机作为一个方法来有效的利用与多个cpu的硬件(在“数十到数百”)的顺序,并包含一个轻量级图书馆来宾操作系统(SPLASHOS)作为一个选项,除了支持商品操作系统是客人。Bugnion等人的[39]引用可移植性(而不是安全性或性能)作为Disco项目的主要动机，该项目提出虚拟机作为一种潜在的方式，允许普通的操作系统(Unix、Windows NT和Linux)在NUMA架构上运行，而不需要进行大量的修改。

### 6.2 VMware

A year later, the team behind Disco founded VMware to continue their work, and released a workstation product in 1999 [40], quickly followed by two server products (GSX and ESX) in 2001 [18, 175, 197]. VMware faced a challenge in virtualizing the x86 architectures of the time, because the hardware did not support traditional virtualization techniques—specifically the architecture contained some sensitive instructions that were not also privileged—so a virtual machine monitor could not rely on trapping protection exceptions as the sole means of identifying when to execute emulated instructions as a safe replacement, since some potentially harmful instructions would never be trapped [170, p. 131].7 To work around this limitation, VMware combined the trap-andexecute technique with a dynamic binary translation technique [40, p. 12:3], which was faster than full emulation but still allowed the guest operating system to run unmodified [40, pp.12:29–12:36].

一年后，Disco背后的团队建立了VMware继续他们的工作，并在1999年发布了一个工作站产品[40]，紧接着在2001年发布了两个服务器产品(GSX和ESX)[18,175,197]。VMware在虚拟化面临挑战的x86架构,因为传统的硬件不支持虚拟化技术——特别是架构包含一些敏感指令也没有特权,所以虚拟机监视器不能依靠捕捉保护例外作为识别的唯一手段,当执行模拟指令作为一个安全的替代,因为一些潜在的有害指令不会被困(170年p . 131)。为了解决这个限制，VMware将trap-and-execute技术与动态二进制转换技术结合起来[40,p. 12:3]，这比完全模拟要快，但仍然允许来宾操作系统不加修改地运行[40,pp.12:29-12:36]。

### 6.3 Denali

The Denali project at the University of Washington in 2002 [207] introduced the term paravirtualization, 8 another work-around for the lack of hardware virtualization support in x86, which involved altering the instruction set in the virtualized hardware architecture and then porting the guest operating system to run on the altered instruction set [206].

华盛顿大学的 Denali 项目 2002年 [207] 引入了半虚拟化这一术语，另一种解决 x86 中缺乏硬件虚拟化支持的方法,其中包括改变虚拟硬件架构中的指令集，然后移植客户操作系统以在改变后的指令集上运行 [206]。

### 6.4 Xen

The Xen project at the University of Cambridge in 2003 [26] also used paravirtualization techniques andmodified guest operating systems but emphasized the importance of preserving the application binary interface (ABI) within the guests so that guest applications could run unmodified. Xen’s greatest technical contribution may have been its approach to precise accounting for resource usage, with the explicit intention to individually bill tenants sharing physical machines [26, p. 176], which was a relatively radical idea at the time and directly led to the creation of Amazon’s Elastic Compute Cloud (EC2) a couple of years later [28].

剑桥大学的 Xen 项目 2003年 [26] 也使用了半虚拟化技术和修改的客户操作系统，但强调了保留应用二进制接口 (ABI) 的重要性。在来宾中，以便来宾应用程序可以未经修改地运行。Xen 最大的技术贡献可能是它对资源使用进行精确核算的方法，明确打算单独向共享物理机器的租户收费 [26，p. 176]，这在当时是一个相对激进的想法，直接导致了亚马逊的弹性计算云 (EC2) 的创建几年后 [28]。

Chisnall [47] provided a detailed account of Xen’s architecture and design goals. Xen’s approach to the problem of untrapped x86 privileged instructions was to substitute a set of hypercalls for unsafe system calls [47, pp. 10–13]. Smith and Nair [181, p. 422] highlighted that Xen was able to run unmodified application binaries within the guest, because it ran the guest in ring 1 of the IA-32 privilege levels and the hypervisor in ring 0, so all privileged instructions were filtered through the hypervisor.

Chisnall [47] 提供了 Xen 的架构和设计目标的详细描述。Xen 处理未捕获 x86 特权指令问题的方法是用一组超调用代替不安全的系统调用 [47，第 10-13 页]。史密斯和奈尔 [181，p. 422] 突出显示 Xen 能够在来宾中运行未经修改的应用程序二进制文件，因为它在 IA-32 权限级别的环 1 中运行来宾，在环 0 中运行虚拟机管理程序,因此，所有特权指令都通过虚拟机管理程序进行过滤。

### 6.5 x86 Hardware Virtualization Extensions

In 2000, Robin and Irvine [170] analyzed the limitations of the x86 architecture as a host for virtual machine implementations, with reference to the earlier work of Goldberg [83] on the architectural features required to support virtualmachines. In themid-2000s, in response to the growing success of virtual machines, and the challenges of implementing them on x86 hardware, Intel and AMD both added hardware support for virtualization in the form of a less privileged execution mode to execute code for the virtual machine guest directly but selectively trap sensitive instructions, eliminating the need for binary translation or paravirtualization. Rosenblum and Garfinkel [172] discussed the motivations behind the added hardware support for virtualization in x86, before the changes were released. Pearce et al. [158, p. 7] contrasted binary translation, paravirtualization, and the features x86 added for hardware-assisted virtualization, clarifying the x86 virtualization extensions were not full virtualization. Adams and Agesen [16] recounted the difficulties VMware encountered while integrating the x86 hardware virtualization extensions and concluded that the new features offered no performance advantage over binary translation.

在 2000，Robin 和 Irvine [170] 分析了 x86 架构作为虚拟机实现主机的局限性,参考 Goldberg [83] 关于支持虚拟机所需的架构特性的早期工作。在 21世纪00年代中期中期，为了应对虚拟机日益增长的成功以及在 x86 硬件上实现虚拟机的挑战,英特尔和 AMD 都以较低特权执行模式的形式增加了对虚拟化的硬件支持，以直接为虚拟机来宾执行代码，但有选择地捕获敏感指令,消除了对二进制翻译或半虚拟化的需要。Rosenblum 和 Garfinkel [172] 在变更发布之前讨论了 x86 中增加硬件支持背后的动机。Pearce 等人 [158，p. 7] 对比了二进制翻译、半虚拟化和 x86 为硬件辅助虚拟化增加的特性，阐明 x86 虚拟化扩展不是完全虚拟化。Adams 和 Agesen [16] 叙述了 VMware 在集成 x86 硬件虚拟化扩展时遇到的困难，并得出结论，新功能没有比二进制翻译提供性能优势。

In 2007, the KVM subsystem for the Linux Kernel provided an API for accessing the x86 hardware virtualization extensions [110]. Since KVM was only a Kernel subsystem, the developers released a fork of QEMU11 as the userspace counterpart of KVM, so the combination of QEMU+KVM provided a full virtual machine implementation, including virtual devices [198, pp.128–129]. Eventually, KVM support was merged into mainline QEMU [122].

在 2007年，Linux 内核的 KVM 子系统提供了用于访问 x86 硬件虚拟化扩展的 API [110]。由于 KVM 只是一个内核子系统，开发人员发布了 QEMU 的一个 fork 作为 KVM 的用户空间对应物，因此 QEMU + KVM 的组合提供了一个完整的虚拟机实现,包括虚拟设备 [198，pp.128-129]。最终，KVM 支持被合并到主线 QEMU [122]。

### 6.6 Hyper-V

In 2008, Microsoft released a beta of Hyper-V [107] for Windows Server. It was built on top of the x86 hardware virtualization extensions and for some virtual devices offered a choice between slower emulation and faster paravirtualization if the guest operating system installed the “Enlightened I/O” extensions. Like Xen’s Dom0, Hyper-V granted special privileges to one guest, called the parent partition, which hosted the virtual devices and handled requests from the other guests. 

In 2010, Bolte et al. [35] incorporated support for Hyper-V into libvirt, so it could be managed through a standardized interface, together with Xen, QEMU+KVM, and VMware ESX.

2008，微软发布了针对 Windows Server 的 Hyper-V [107] 测试版。它是建立在 x86 硬件虚拟化扩展之上的，对于一些虚拟设备，如果客户操作系统安装了 “开明 I/O” 扩展，则可以在较慢的仿真和较快的半虚拟化之间进行选择。像 Xen 的 Dom0 一样，Hyper-V 授予一个来宾特权，称为父分区，它托管虚拟设备并处理来自其他来宾的请求。

在 2010，Bolte 等人 [35] 将对 Hyper-V 的支持纳入了 libvirt 中，因此它可以通过一个标准化的接口，与 Xen 、 QEMU + KVM 和 VMware ESX 一起进行管理。

### 6.7 Trade-Offs

Denali and Xen both used paravirtualization techniques, sacrificing portability to gain performance, but their goals for scale were completely different: Denali considered 10,000 virtual machines to be a good result [208]—achieved through a combination of lightweight guests and a minimal host—whereas Xen argued that 100 virtual machines running full operating systems13 was a more reasonable target [26, p. 165,175]. To some extent, Denali was more in line with modern container implementations than with the virtual machine implementations of its day. Xen has shifted their estimation of required scale upward over the years but still exhibits a tolerance for unnecessary performance degradation. For example, Manco et al. [131] demonstrated that a few small internal changes to the way Xen storesmetadata and creates virtual devices improved virtual machine instantiation time by an order of magnitude—a result 50 to 200 times faster than Docker’s container instantiation—however, those patches are unlikely to ever make it into mainline Xen.

Denali 和 Xen 都使用了半虚拟化技术，牺牲了可移植性以获得性能，但他们对规模的目标完全不同: Denali 认为 10,000 台虚拟机是一个好结果[208]-通过轻量级来宾和最小主机的组合实现-而 Xen 认为运行完整操作系统的 100 台虚拟机是更合理的目标 [26，165,175 页]。在某种程度上，Denali 更符合现代容器实现，而不是当时的虚拟机实现。Xen 多年来已经将他们对所需规模的估计向上移动，但仍然表现出对不必要的性能退化的容忍度。例如,manco 等人 [131] 证明了对 Xen 存储元数据和创建虚拟设备的方式的一些小的内部改变将虚拟机实例化时间提高了一个数量级 -- 结果快了 50 到 200 倍比 Docker 的容器实例化-但是,这些补丁不太可能进入主线 Xen。

Xen and KVM have a reputation for sacrificing performance to gain security; however, several independent lines of research have raised questions as to whether those security gains are real or imagined. Perez-Botero et al. [159] analyzed security vulnerabilities in Xen and KVM between 2008 and 2012, categorizing them by source, vector, and target, and observed that the most common vector of attack was device emulation (Xen 34%, KVM 40%), the majority were triggered from within the virtual machine guest (Xen 71%, KVM 66%), and the majority successfully targeted the hypervisor’s Ring –1 privileges or slightly less privileged control over Dom0 or the host operating system (Xen 80%, KVM 76%). Chandramouli et al. [46] built on the work of Perez-Botero et al. [159], moving toward a more general framework for forensic analysis of vulnerabilities in virtual machine implementations. Ishiguro and Kono [101] evaluated vulnerabilities in Xen and KVMrelated to instruction emulation between 2009 and 2017. They demonstrated that a prototype “instruction firewall” on KVM—which denies emulation of all instructions except the small subset deemed legitimate in the current execution context—could have defended against the known instruction emulation vulnerabilities; however, the patches are unlikely to evermake it intomainline KVM.

Xen 和 KVM 以牺牲性能来获得安全而闻名; 然而，一些独立的研究提出了这些安全收益是真实的还是想象的问题。Perez-Botero 等人 [159] 分析了 Xen 和 KVM 2008年和 2012 的安全漏洞，按来源、矢量和目标对其进行分类,并且观察到最常见的攻击向量是设备仿真 (Xen 34%，KVM 40%)，大多数是从虚拟机客户内部触发的(Xen 71%，KVM 66%)，大多数人成功地将虚拟机管理程序的 Ring-1 特权或对 Dom0 或主机操作系统 (Xen 80%，KVM 76%) 的特权控制作为目标。Chandramouli 等人 [46] 建立在 Perez-Botero 等人的工作之上 [159]，朝着更通用的框架发展，用于对虚拟机实现中的漏洞进行取证分析。Ishiguro 和 Kono [101] 评估了 Xen 和 kvm 中与指令仿真相关的漏洞 2017 和 2009年。他们证明了 KVM 上的原型 “指令防火墙” -- 它拒绝所有指令的仿真，除了在当前执行环境中被认为合法的小子集 -- 可以防御已知的指令仿真漏洞; 然而，这些补丁不太可能进入主线 KVM。

Szefer et al. [191] demonstrated in the NoHype implementation (based on Xen) that eliminating the hypervisor and running virtual machines with more direct access to the hardware improved security by reducing the attack surface and removing virtual machine exit events as potential attack vectors. However, the approach involved a performance trade-off in resource utilization that was not viable for most real deployments: it pre-allocated processor cores, memory, and I/O devices dedicated to specific virtualmachines rather than allowing for oversubscription and dynamic allocation in response to load.

Szefer等人[191]在NoHype实现(基于Xen)中论证了消除虚拟机监控程序和运行更直接访问硬件的虚拟机通过减少攻击面和删除作为潜在攻击载体的虚拟机退出事件来提高安全性。但是，这种方法涉及到资源利用方面的性能权衡，这对于大多数实际部署来说是不可行的:它预先分配处理器核心、内存和I/O设备，专门用于特定的虚拟机，而不允许超订阅和响应负载的动态分配。

One persistent argument in favor of virtual machines has been that virtual machine implementations have fewer lines of code than a kernel or host operating system, and are therefore easier to code review and secure [39, 81, 131, 158, 178], which is the classic trade-off ofminimizing complexity to gain security. However, less code offers only a vague potential for security, and even that potential becomes questionable as modern virtual machine implementations have grown larger and more complex [37, 53, 158, 214].

一个持续争论的虚拟机,虚拟机实现的代码比一个内核少或主机操作系统,因此更容易代码审查和安全(39、81、131、158、178),这是减少复杂性获得安全的经典的权衡。然而，更少的代码只提供了一个模糊的安全潜力，而且随着现代虚拟机实现变得更大、更复杂[37,53,158,214]，甚至这种潜力也变得有问题。

Recent work on virtual machines—such as ukvm [212], LightVM [131], and Kata Containers (formerly Intel Clear Containers) [5]—has shifted back toward an emphasis on improving performance. However, this work appears to be founded on the assumption that the virtual machine implementations under discussion are adequately secure and need only improve performance, which is a dubious assumption at best.

- [212] DanWilliams and Ricardo Koller. 2016. Unikernelmonitors: Extending minimalism outside of the box. In Proceedings of the 8th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud’16). 6.
- [131] Filipe Manco, Costin Lupu, Florian Schmidt, Jose Mendes, Simon Kuenzer, Sumit Sati, Kenichi Yasukata, Costin Raiciu, and Felipe Huici. 2017. MyVMis lighter (and safer) than your container. In Proceedings of the 26th Symposium on Operating Systems Principles (SOSP’17). ACM, New York, NY, 218–233.
- [5] [5] Kata Containers. 2018. Home Page. Retrieved December 18, 2019 from https://katacontainers.io/.

最近在虚拟机方面的工作——如ukvm[212]、LightVM[131]和Kata Containers(以前是Intel Clear Containers)[5]——已经重新将重点放在提高性能上。然而，这项工作似乎是建立在这样一个假设上的，即所讨论的虚拟机实现是足够安全的，并且只需要提高性能，这是一个很可疑的假设。

Two notable departures from this complacent attitude to security are Google’s crosvm [86] and Amazon’s Firecracker [19], which aim to improve both performance and security, by replacing QEMU with a radically smaller and simpler userspace component for KVM, and by choosing Rust as the implementation language for memory safety.14 Firecracker started as a fork of crosvm, but the two projects are collaborating on generalizing the divergence into a set of Rust libraries they can share.

- [86] Google. 2018. Chrome OS Virtual Machine Monitor. Retrieved December 18, 2019 from https://chromium.googlesource.com/chromiumos/platform/crosvm/.

- [19] [19] Amazon. 2019. Firecracker. Retrieved December 18, 2019 from https://firecracker-microvm.github.io/.

两个安全显著偏离这种自满的态度是谷歌crosvm[86]和亚马逊的爆竹[19],旨在提高性能和安全性,通过从根本上取代QEMU小的、简单的用户空间组件KVM,和通过选择生锈的实现语言记忆的安全。爆竹最初是crosvm的一个分支，但这两个项目正在合作，将分歧归纳为一组它们可以共享的锈库。

### 6.8 Decline

Toward the end of the 2000s, the enthusiasm for virtual machines gave way to a growing skepticism. Garfinkel et al. [80] demonstrated that virtual machine environments could reliably be detected on close inspection, reviving the long-running tension between the ideals of strong isolation in virtual machines, and the reality of actual implementations. Buzen and Gagliardi [43] commented on the ideals in the early 1970s, stating “Since a privileged software nucleus has, in principle, no way of determining whether it is running on a virtual or a real machine, it has no way of spying on or altering any other virtual machine that may be coexisting with it in the same system,” but in the same work they acknowledged, “In practice no virtual machine is completely equivalent to its real machine counterpart.”

21世纪00年代末，对虚拟机的热情被日益增长的怀疑所取代。Garfinkel 等人 [80] 证明了在仔细检查时可以可靠地检测到虚拟机环境，恢复了虚拟机中强隔离理想之间的长期运行张力,和实际实现的现实。Buzen 和 Gagliardi [43] 对 20世纪70年代初中期的理想进行了评论，指出 “由于特权软件核心原则上已经,无法确定它是在虚拟还是真实机器上运行,它无法监视或改变在同一个系统中可能与它共存的任何其他虚拟机，”但是在同样的工作中，他们承认,“ 在实践中，没有一个虚拟机是完全等同于其真正的机器对应物。”

In 2010, Bratus et al. [37] criticized the disproportionate focus of systems security research on virtual machines and the resulting neglect of other potentially superior approaches to system security. Vasudevan et al. [195] outlined a set of requirements for protecting the integrity of virtual machines implemented on x86 with hardware virtualization support and evaluated all existing implementations as “unsuitable for use with highly sensitive applications” (p. 141). Colp et al. [53] observed that multitenant environments presented newrisks for virtualmachine implementations, because they required stronger isolation between guests sharing the same host than was necessary when a single tenant owned the entire physical machine.

2010，Bratus 等人 [37] 批评了对虚拟机的系统安全研究的过度关注，以及由此导致的对系统安全的其他潜在优势方法的忽视。Vasudevan 等人 [195] 概述了一组在 x86 上实现的具有硬件虚拟化支持的虚拟机的完整性保护要求，并将所有现有实现评估为 “不适合用于高度敏感的应用程序” (p. 141)。Colp 等人 [53] 观察到多租户环境为虚拟机实现带来了新的风险,因为他们需要在共享同一主机的来宾之间进行更强的隔离，而不是在单个租户拥有整个物理机器时所需要的隔离。

Virtual machines such as Xen, QEMU+KVM, Hyper-V, and VMware are still in active use today, but in recent years they have entirely ceded their reputation as the “hot new thing” to containers.

虚拟机如 Xen，QEMU + KVM，Hyper-V 和 VMware 仍在积极利用今天,但是近年来，他们已经完全放弃了作为 “热门新事物” 的声誉给容器。

## 7 MODERN CONTAINERS

The collection of technologies that make up modern container implementations started coming together years before anyone used the term container. The two decade span surrounding the development of containers corresponded to a major shift in the way information about technological advances was broadcast and consumed. Exploring the socio-economic factors driving this shift is outside the scope of this survey; however, it is worth noting that the academic literature on more recent projects such as Docker and Kubernetes is largely written by outsiders providing external commentary rather than by the primary developers of the technologies. As a result, recent academic publications on containers tend to lack the depth of perspective and insight that was common to earlier publications on virtual machines, capabilities, and security in the Linux Kernel. The dialog driving innovation and improvements to the technology has not disappeared, but it has moved away from the academic literature and into other communication channels.

构成现代容器实现的技术集合在任何人使用术语 “容器” 之前的几年就开始聚集在一起了。围绕集装箱发展的 20 年时间，与传播和消费技术进步信息的方式的重大转变相对应。探索推动这一转变的社会经济因素不在本次调查的范围之内; 然而,值得注意的是，关于 Docker 和 Kubernetes 等最近项目的学术文献主要是由提供外部评论的外部人员编写的，而不是由技术的主要开发人员编写的。因此，最近关于容器的学术出版物往往缺乏早期关于 linux内核中虚拟机、功能和安全性的出版物所共有的视角和洞察力的深度。推动技术创新和改进的对话并没有消失，但它已经从学术文献转移到其他交流渠道。

### 7.1 POSIX Capabilities

In the mid-1990s, the security working group of the POSIX standards project began drafting an extension to the POSIX.1 standard, called POSIX 1003.1e [3, 71, 90], which added a feature called capabilities. The implementation details of POSIX capabilitieswere entirely different than the early capability systems [201, p. 97] but had similarities on a conceptual level: POSIX capabilities were a set of flags associated with a process or file, which determined whether a process was permitted to perform certain actions, a process could exec a subprocess with a subset of its own capabilities, and the specification attempted to support the principle of least privilege [3]. However, the POSIX capabilities did not adopt the concepts of small access domains and no-privilege defaults, which were crucial elements of secure isolation in the early capability systems [62]. The POSIX.1e draft was withdrawn from the process in 1998 and never formally adopted as a standard [90, p. 259], but it formed the basis of the capabilities feature added to the Linux Kernel in 1999 (release 2.2) [4, 132].

在 20世纪90年代中期中期，POSIX 标准项目的安全工作组开始起草对 POSIX.1 标准的扩展，称为 POSIX 1003。1e [3,71，90]，它添加了一个称为能力的特性。POSIX 能力的实现细节与早期的能力系统完全不同 [201，p. 97] 但是在概念层面上有相似之处: POSIX 功能是一组与进程或文件相关联的标志，它决定了进程是否被允许执行某些操作,一个进程可以执行一个具有其自身能力子集的子进程，规范试图支持最小特权原则 [3]。然而，POSIX 能力没有采用小访问域和无特权违约的概念，这是早期能力系统中安全隔离的关键要素 [62]。POSIX.1e 草案于 1998年退出该过程，从未正式作为标准 [90，第 259]，但它构成了 1999年 linux内核 (2.2 版) [132] 中添加的功能特性的基础。

### 7.2 Namespaces and Resource Controls

A second important strand in the evolution of modern container implementations was the isolation of processes via namespaces and resource usage controls. In 2000, FreeBSD added Jails [105], which isolated filesystem namespaces (using chroot) but also isolated processes and network resources in such a way that a process might be granted root privileges inside the jail but blocked from performing operations that would affect anything outside the jail. In 2001, Linux VServer [182] patched the Linux Kernel to add resource usage limits and isolation for filesystems, network addresses, and memory. Around the same time, Virtuozzo (later released as OpenVZ) [98, 135] also patched the Linux Kernel to add resource usage limits and isolation for filesystems, processes, users, devices, and interprocess communication (IPC). In 2003, Nagar et al. [146] proposed a framework for resource usage control and metering called Class-Based Kernel Resource Management (CKRM) and later released it as a set of patches to the Linux Kernel.

现代容器实现发展的第二个重要方面是通过命名空间和资源使用控制隔离进程。在 2000 中，FreeBSD 添加了 Jails [105]，它隔离了文件系统命名空间 (使用 chroot) 但是也隔离了进程和网络资源，这样一个进程可能在监狱内被授予根权限，但被阻止执行会影响监狱外任何事情的操作。在 2001 中，Linux VServer [182] 修补了 Linux 内核以增加资源使用限制和对文件系统、网络地址和内存的隔离。大约在同一时间，Virtuozzo (后来发布为 OpenVZ) [98,135] 也修补了 linux内核，增加了文件系统、进程、用户、设备的资源使用限制和隔离,和进程间通信 (IPC)。2003，Nagar 等 [146] 提出了一个资源使用控制和计量的框架，称为基于类的内核资源管理 (CKRM) 后来将其作为一组修补程序发布到 Linux 内核。

In 2002, the Linux Kernel (release 2.4.19) introduced a filesystem namespaces feature [109].15 In 2006, Biederman [33] proposed expanding the idea of namespace isolation in the Linux Kernel beyond the filesystem to process IDs, IPC, the network stack, and user IDs. The Kernel developers accepted the idea, and the patches to implement the features landed in the Kernel between 2006 and 2013 (releases 2.6.19 to 3.8) [109]. The last set of patches to be completed was user namespaces, which allow an unprivileged user to create a namespace and grant a process full privileges for operations inside that namespace while granting it no privileges for operations outside that namespace [11]. The way user namespaces are nested bears a resemblance to those of the capabilities of Dennis and Van Horn [65], where processes created more restricted subprocesses.

在 2002 中，linux内核 (2.4.19 版) 引入了一个文件系统命名空间特性 [109]。15 2006，Biederman [33] 提出将 linux内核中的命名空间隔离的思想扩展到文件系统之外，以处理 IDs 、 IPC 、网络堆栈和用户 id。内核开发人员接受了这一想法，并且实现这些特性的补丁在 2006年和 2013 (发布了 2.6.19 到 3.8) [109]。最后一组要完成的补丁是用户命名空间,它允许非特权用户创建一个命名空间，并授予该命名空间内操作的进程完全权限，同时不授予该命名空间外操作的权限 [11]。用户名称空间嵌套的方式与 Dennis 和 Van Horn 的能力相似 [65]，其中过程创建了更多受限的子过程。

In 2004, Solaris added Zones [164] (sometimes also called Solaris Containers), which isolated processes into groups that could only observe or signal other processes in the same group, associated each zone with an isolated filesystem namespace, and set limits for shared resource consumption (initially only CPU). Between 2006 and 2007, Rohit Seth and Paul Menageworked on a patch for the Linux Kernel for a feature they called process containers [58]—later renamed to cgroups for “control groups”—which provided resource limiting, prioritization, accounting,16 and control features for processes.

在 2004 中，Solaris 添加了区域 [164] (有时也称为 Solaris 容器)，它将进程隔离成组，这些组只能观察或发送同一组中的其他进程的信号,将每个区域与独立的文件系统命名空间关联，并设置共享资源消耗限制 (最初仅 CPU)。2006 到 2007 之间,rohit Seth 和 Paul Menage 为 linux内核开发了一个补丁，他们称之为进程容器 [58] -- 后来改名为 “控制组” 的 cgroups -- 它提供了资源限制,流程的优先级、会计和控制功能。

### 7.3 Access Control and System Call Filtering

A third set of relevant features in the Linux Kernel evolved around secure isolation of processes through restricted access to system calls. In 2000, Cowan et al. [60] released SubDomain, a Linux Kernel module that added access control checks to a limited set of system calls related to executing processes. In 2001, Loscocco and Smalley [126] published an architectural description of SELinux, which implemented mandatory access control (MAC) for the Linux Kernel. The access control architecture of SELinux was received positively, but the implementation was rejected for being too tightly coupled with the kernel. So, in 2002, Wright et al. [216] proposed the Linux Security Module (LSM) framework as a more general approach to extensible security in the Linux Kernel, which made it possible for security policies to be loaded as Kernel modules. LSM is not an access control mechanism, but it provides a set of hooks where other security extensions such as SELinux or AppArmor can insert access control checks. LSM and a modified version of SELinux based on LSM were both merged into the mainline Linux Kernel in 2003. In 2004 to 2005, SubDomain was rewritten to use LSM and rebranded under the name AppArmor.

Linux内核中的第三组相关特性是围绕通过对系统调用的受限访问来实现进程的安全隔离而发展起来的。2000，Cowan 等人 [60] 发布了 SubDomain，这是一个 linux内核模块，它为有限的一组与执行进程相关的系统调用添加了访问控制检查。2001，loscoco 和 Smalley [126] 发布了 SELinux 的架构描述，实现了 Linux 内核的强制访问控制 (MAC)。SELinux 的访问控制架构得到了肯定的接受，但是由于与内核耦合过紧而拒绝了实现。因此，2002年，Wright 等人 [216] 提出了 Linux 安全模块 (LSM) 框架作为在 linux内核中实现可扩展安全性的更通用的方法,这使得安全策略可以作为内核模块加载。LSM 不是访问控制机制，但它提供了一组钩子，其他安全扩展 (如 SELinux 或 AppArmor) 可以在其中插入访问控制检查。2003年，LSM 和基于 LSM 的 SELinux 的修改版本都被合并到主线 Linux 内核中。在 2004 2005年，子域被重写为使用 LSM 并以名称 AppArmor 重新命名。

In 2005, Arcangeli [22] released a set of patches to the Linux Kernel called seccomp for “secure computing,” which restricted a process so that it could only run an extremely limited set of system calls to exit/return or interact with already open filehandles and terminated a process attempting to run any other system calls. The patches were merged into the mainline Kernel later that year. However, the features of the original seccomp were inadequate and rarely used, and over the years multiple proposals to improve seccomp were unsuccessful. Then, in 2012, Drewry [68] extended seccomp to allow filters for system calls to be dynamically defined using Berkeley Packet Filter (BPF) rules, which provided enough flexibility to make seccomp useful as an isolation technique. In 2013, Krude and Meyer [115] implemented a framework for isolating untrusted workloads on multitenant infrastructures using seccomp system call filter policies written in BPF.

2005，Arcangeli [22] 发布了一组名为 seccomp 的 linux内核补丁，用于 “安全计算”,它限制了一个进程，以便它只能运行一组极其有限的系统调用来退出/返回或与已经打开的文件句柄交互，并终止了试图运行任何其他系统调用的进程。那年晚些时候，这些补丁被合并到主线内核中。然而，原始 seccomp 的特征不充分，很少使用，多年来，多个改进 seccomp 的建议都没有成功。然后，2012年，Drewry [68] 扩展了 seccomp，允许使用 Berkeley Packet Filter (BPF) 规则动态定义系统调用的过滤器,这提供了足够的灵活性，使 seccomp 作为一种隔离技术非常有用。在 2013，Krude 和 Meyer [115] 使用 BPF 中编写的 seccomp 系统调用过滤策略实现了一个隔离多租户基础设施上不受信任的工作负载的框架。

### 7.4 Cluster Management

A fourth relevant strand of technology evolved around resource sharing in large-scale cluster management. In 2001, Lottiaux and Morin [127] used the term container for a form of shared, distributed memory that provided the illusion that multiple nodes in an SMP cluster were sharing kernel resources, including memory, disk, and network. In 2002, the Zap project [154] used the term pod17 for a group of processes sharing a private namespace, which had an isolated view of system resources such as process identifiers and network addresses. These pods were self-contained, so they could be migrated as a unit between physical machines. In the mid-2000s, Google deployed a cluster management solution called Borg [42, 196] into production, to orchestrate the deployment of their vast suite of web applications and services. Although the code for Borg has never been seen outside Google, it was the direct inspiration for the Kubernetes project a decade later [196, p.18:13–18:14]—the Borg alloc became the Kubernetes pod, Borglets became Kubelets, and tasks gave way to containers. Burns et al. [42, p. 70] explained that improving performance through resource utilization was one of the primary motivations for Borg.

第四个相关技术链围绕大规模集群管理中的资源共享展开。在 2001，Lottiaux 和 Morin [127] 将术语 container 用于一种共享的分布式内存形式，它提供了一种错觉，即 SMP 集群中的多个节点共享内核资源，包括内存,磁盘和网络。在 2002 中，Zap 项目 [154] 将术语 pod 用于共享私有命名空间的一组进程，该进程具有系统资源的独立视图，例如进程标识符和网络地址。这些 pod 是独立的，因此它们可以作为一个单元在物理机器之间迁移。在 21世纪00年代中期中期，谷歌在生产中部署了一个名为 Borg [196] 的集群管理解决方案，以协调他们庞大的网络应用和服务套件的部署。虽然博格的代码从未在谷歌之外出现过，但它是十年后 Kubernetes 项目的直接灵感 [196，p.18:13-18:14]-Borg alloc 变成了 Kubernetes pod，Borglets 变成了 Kubelets，任务让位于容器。伯恩斯等人 [42，第 70 页] 解释说，通过资源利用提高绩效是博格的主要动机之一。

### 7.5 Combined Features

The strength of modern containers is not in any one feature but in the combination of multiple features for resource control and isolation. In 2008, Linux Containers (LXC) [6] combined cgroups, namespaces, and capabilities from the Linux Kernel into a tool for building and launching lowlevel system containers. Miller and Chen [142] demonstrated that filesystem isolation between LXC containers could be improved by applying SELinux policies. Xavier et al. [219] and Raho et al. [166] contrasted LXC’s approach to isolation and resource control using standard Linux Kernel features such as cgroups and filesystem, process, IPC, and network namespaces, versus the approaches taken by Linux VServer and OpenVZ using custom patches to the Linux Kernel to provide similar features.

现代容器的优势不在于任何一个特性，而在于用于资源控制和隔离的多个特性的组合。在 2008，Linux Containers (LXC) [6] 将 cgroups 、命名空间和来自 linux内核的功能组合成一个工具，用于构建和启动低级系统容器。Miller 和 Chen [142] 证明了 LXC 容器之间的文件系统隔离可以通过应用 SELinux 策略得到改善。Xavier 等人 [219] 和 Raho 等人 [166] 对比了 LXC 使用标准 linux内核特性的隔离和资源控制方法，例如 cgroups 和文件系统，进程，IPC,和网络命名空间，与 Linux VServer 和 OpenVZ 使用自定义补丁为 Linux 内核提供类似功能所采取的方法相比。

Docker [141] launched in 2013 as a container management platform built on LXC. In 2014, Docker replaced LXC with libcontainer, its own implementation for creating containers, which also used Linux Kernel namespaces, cgroups, and capabilities [99, 166]. Morabito et al. [144] compared the performance of LXC and Docker after the transition to libcontainer and found them to be roughly equivalent on CPU performance, disk I/O, and network I/O; however, LXC performed 30% better on random writes, which may have been related to Docker’s use of a union file system. Raho et al. [166] contrasted the implementations of Docker, QEMU+KVM, and Xen on the ARM hardware architecture. Mattetti et al. [134] experimented with dynamically generating AppArmor rules for Docker containers based on the applicationworkload they contained. Catuogno and Galdi [45] performed a case study of Docker using two different models for security assessment. They built on the work of Reshetova et al. [167] in classifying vulnerabilities by the goal of the attack: denial of service, container compromise, or privilege escalation.

Docker [141] 于 2013年推出，作为基于 LXC 构建的容器管理平台。在 2014 中，Docker 用自己的用于创建容器的实现 libcontainer 取代了 LXC，后者还使用了 linux内核命名空间、 cgroups 和 capability [99，166]。Morabito 等 [144] 比较了 LXC 和 Docker 在过渡到 libcontainer 后的性能，发现它们在 CPU 性能、磁盘 I/O 和网络 I/O 上大致相当; 然而，LXC 在随机写入上的表现要好 30%，这可能与 Docker 使用 union 文件系统有关。Raho 等人 [166] 对比了 Docker 、 QEMU + KVM 和 Xen 在 ARM 硬件架构上的实现。Mattetti 等 [134] 试验了基于包含的应用程序工作负载动态生成 Docker 容器的 AppArmor 规则。Catuogno 和 Galdi [45] 使用两种不同的模型进行了 Docker 的案例研究，用于安全评估。他们建立在 Reshetova 等人 [167] 的工作基础上，通过攻击的目标对漏洞进行分类: 拒绝服务、容器妥协或特权升级。

In 2015, Docker split the container runtime out into a separate project, runc, in support of a vendor-neutral container runtime specification maintained by the Open Container Initiative (OCI). Hykes [100] highlighted that SELinux, AppArmor, and seccomp were all standard supported features in runc. Koller and Williams [113] observed that runc was more minimal than the Docker runtime while still using the same isolation mechanisms from the Linux Kernel, such as namespaces and cgroups. In 2016, Docker and CoreOS merged their container image formats into a vendor-neutral container image format specification, also at OCI [36].

在 2015 中，Docker 将容器运行时拆分为一个单独的项目 runc，以支持由 Open container Initiative (OCI) 维护的供应商中立的容器运行时规范。Hykes [100] 强调了 SELinux 、 AppArmor 和 seccomp 都是 runc 中支持的标准特性。Koller 和 Williams [113] 观察到 runc 比 Docker 运行时更小，同时仍然使用与 linux内核相同的隔离机制，例如命名空间和 cgroups。在 2016，Docker 和 CoreOS 将它们的容器镜像格式合并为供应商中立的容器镜像格式规范，同样在 OCI [36]。

### 7.6 Orchestration

In 2014, Docker began working on Swarm, described as a clustering system for Docker, which they ultimately released late in 2015 [128]. Also in 2014, Google began developing Kubernetes, an orchestration tool for deploying and managing the lifecycle of containers, which they released in the middle of 2015 [38]. Also in 2014, Canonical began developing LXD, a container orchestration tool for LXC containers, which they released in 2016 [89].

在 2014，Docker 开始在 Swarm 上工作，被描述为 Docker 的集群系统，他们最终在 2015年末发布了 [128]。同样在 2014年，Google 开始开发 Kubernetes，这是一种用于部署和管理容器生命周期的编排工具，他们在 2015年发布了这个工具 [38]。同样在 2014年，Canonical 开始开发 LXD，一种用于 LXC 容器的容器编排工具，并于 2016年发布 [89]。

Verma et al. [196] outlined the design goals behind Kubernetes, in the context of lessons learned from Borg. Syed and Fernandez [189, 190] pointed out that the performance advantages of the higher-level container orchestration tools, such as Kubernetes and Docker Swarm, were primarily a matter of improving resource utilization. They also contrasted the portability advantages of managing containers across multiple physical host machines against the increased complexity required for the orchestration tools to advance beyond managing a single machine host. Souppaya et al. [184] systematically reviewed increased security risks and mitigation techniques for container orchestration tools. Bila et al. [34] extended Kubernetes with a vulnerability scanning service and network quarantine for containers.

Verma 等人 [196] 在 Borg 经验教训的背景下概述了 Kubernetes 背后的设计目标。Syed 和 Fernandez [189，190] 指出，Kubernetes 和 Docker Swarm 等更高级别的容器编排工具的性能优势主要在于提高资源利用率。他们还将跨多个物理主机管理容器的可移植性优势与编排工具超越管理单个机器主机所需的复杂性进行了对比。Souppaya 等人 [184] 系统地回顾了容器编排工具增加的安全风险和缓解技术。Bila et al. [34] 扩展 Kubernetes，具有漏洞扫描服务和容器网络隔离。

### 7.7 Trade-Offs

Containers have a reputation for substantially better performance than virtual machines; however, that reputation may not be deserved. In 2015, Felter et al. [76] measured the performance of Docker against QEMU+KVM and determined that neither had significant overhead on CPU and memory usage, but that KVM had a 40% higher overhead in I/O. They observed that the overhead was primarily due to extra cycles on each I/O operation, so the impact could be mitigated for some applications by batching multiple small I/O operations into fewer large I/O operations. In 2017, Kovács [114] compared CPU execution time and network throughput between Docker, LXC, Singularity, KVM, and bare metal, and determined that there was no significant variation between them, as long as Docker and LXC were running in host networking mode, but in Linux bridge mode Docker and LXC exhibited high retransmission rates that negatively impacted their throughput compared to the others. Manco et al. [131] demonstrated that Xen virtual machine instantiation could be 50 to 200 times faster than Docker container instantiation, with a few lowlevel modifications to Xen’s control stack.

容器以性能明显优于虚拟机而闻名; 但是，这种声誉可能不值得。在 2015，Felter 等人 [76] 针对 QEMU + KVM 测量了 Docker 的性能，并确定两者在 CPU 和内存使用上都没有显著的开销,但是 KVM 在 I/O 方面的开销要高 40%。他们观察到开销主要是由于每个 I/O 操作的额外周期,因此，通过将多个小型 I/O 操作批量转换为较少的大型 I/O 操作，可以减轻对某些应用程序的影响。在 2017，kov á cs [114] 比较了 Docker 、 LXC 、奇点、 KVM 和裸机之间的 CPU 执行时间和网络吞吐量，并确定它们之间没有显著变化,只要 Docker 和 LXC 在主机联网模式下运行,但是在 Linux 桥接模式下，Docker 和 LXC 表现出较高的重传速率，与其他模式相比，这对它们的吞吐量产生了负面影响。Manco 等人 [131] 证明了 Xen 虚拟机实例化可以比 Docker 容器实例化快 50 到 200 倍，只需对 Xen 的控制堆栈进行一些低级修改。

Secure isolation technologies have been the core of modern container implementations from the beginning, so it would be reasonable to expect that containers would provide a strong form of isolation. However, early implementations of containers were prone to preventable security vulnerabilities, which may indicate that security was not a primary design consideration, at least not initially. Combe et al. [54] analyzed security vulnerabilities in Docker and libcontainer between 2014 and 2015, and determined that the majority were related to filesystem isolation, which led to privilege escalation when Docker was run as the root user. They also suggested that some of Docker’s sane default configurations for the isolation features of the Linux Kernel could be easily switched to less secure configurations through standard options to the docker command-line tool or the Docker daemon, and so might be prone to user error. Martin et al. [133] surveyed vulnerabilities in Docker images, libcontainer, the Docker daemon, and orchestration tools, as well as the unique security challenges of containers in multitenant infrastructures. In addition to security patches for specific privilege escalation vulnerabilities, there has been ongoing work to integrate support for user namespaces into Docker and Kubernetes,19 so they can run as a non-root user and limit the scope of damage from privilege escalation. However, the user namespaces feature itself has had a series of vulnerabilities20 related to interfaces in the Kernel that were written with the expectation of being restricted to the root user but are now exposed to unprivileged users.

安全隔离技术从一开始就是现代容器实现的核心，因此可以合理地预期容器将提供强大的隔离形式。然而，容器的早期实现容易出现可预防的安全漏洞，这可能表明安全性不是主要的设计考虑因素，至少最初不是。Combe 等人 [54] 在 2014年和 2015 分析了 Docker 和 libcontainer 的安全漏洞，并确定大多数与文件系统隔离有关,当 Docker 作为根用户运行时，这会导致权限提升。他们还建议，通过 Docker 命令行工具或 docker 的标准选项，可以轻松地将一些 Docker 针对 linux内核隔离特性的正常默认配置切换到安全性较低的配置。守护进程因此可能容易出现用户错误。Martin 等 [133] 调查了 Docker 镜像、 libcontainer 、 Docker daemon 和编排工具中的漏洞，以及多租户基础设施中容器特有的安全挑战。除了针对特定的权限提升漏洞的安全补丁之外，还一直在努力将对用户命名空间的支持集成到 Docker 和 Kubernetes 中。因此，他们可以作为非根用户运行，并限制权限提升造成的损害范围。但是，用户命名空间特性本身存在一系列与内核中接口相关的漏洞，这些漏洞的编写预期仅限于根用户，但现在暴露于非特权用户。

One significant difference between virtual machine implementations and container implementations is that containers share a kernel with the host operating system, so efforts to secure the kernel greatly impact the security of containers. Reshetova et al. [167] considered the set of secure isolation features offered by the Linux Kernel as of 2014 (in the context of LXC) and judged them to have caught up with the features of FreeBSD Jails and Solaris Zones but highlighted some areas for improvement in support of containers. These improvements included integratingMAC into the Kernel as “security namespaces,” providing a way to lock down device hotplug features for containers and extending cgroups to support all resource management features supported by rlimits. Gao et al. [79] discussed the risks of certain types of information that containers can currently access from the Linux Kernel via procfs and sysfs—which can be exploited to detect co-resident containers and precisely target power consumption spikes to overload servers—and prototyped a power-based namespace to partition the information for containers.

虚拟机实现和容器实现之间的一个显著区别是容器与主机操作系统共享内核，因此保护内核的努力极大地影响了容器的安全性。Reshetova 等人 [167] 认为 linux内核提供的一组安全隔离特性为 2014年 (在 LXC 的上下文中) 并判断他们已经赶上了 FreeBSD 监狱和 Solaris 区域的功能，但强调了支持容器的一些改进领域。这些改进包括将 mac 集成到内核中作为 “安全名称空间”，提供了一种锁定容器设备热插拔特性的方法，并扩展了 cgroups 以支持 rlimits 支持的所有资源管理特性。Gao 等人 [79] 讨论了容器当前可以通过 procfs 和 sysfs 从 linux内核访问的某些类型信息的风险 -- 这些信息可以被用来检测共同驻留的容器并精确地目标功耗峰值以使服务器过载-并原型化了基于 power 的命名空间以对容器的信息进行分区。

Some more recent approaches to secure isolation for containers have been inspired by virtual machine implementations. Kata Containers (formerly Intel Clear Containers) [5] wraps each Docker container or Kubernetes pod in a QEMU+KVM virtual machine [12]. They realized that QEMU was not ideal for the purpose—since it introduces a substantial performance hit compared to running bare containers, and the majority of the code relates to emulation that is not useful for wrapping containers—so a group at Intel started working on a stripped-down version of QEMU called NEMU [8]. X-Containers [179] used Xen’s paravirtualization features to improve isolation between containers and the host but made an unfortunate trade-off of removing isolation between containers running on the same host. Nabla Containers [7] and gVisor [88] have both taken an approach of improving isolation by heavily filtering system calls from containers to the host kernel, which is a common technique for modern virtual machines.

最近的一些容器安全隔离方法受到虚拟机实现的启发。Kata Containers (以前称为 Intel Clear Containers) [5] 将每个 Docker 容器或 Kubernetes pod 包装在 QEMU + KVM 虚拟机中 [12]。他们意识到 QEMU 并不理想-因为它与运行裸容器相比带来了巨大的性能冲击,大多数代码都与仿真有关，而仿真对于包装容器是没有用的 -- 因此英特尔的一个小组开始开发一个名为 NEMU [8] 的精简版本的 QEMU。X-Containers [179] 使用 Xen 的半虚拟化特性来提高容器和主机之间的隔离，但在删除运行在同一主机上的容器之间的隔离时做出了不幸的权衡。Nabla Containers [7] 和 gVisor [88] 都采取了通过从容器到主机内核的重度过滤系统调用来提高隔离度的方法，这是现代虚拟机的常用技术。

Bratus et al. [37] noted that the “self-protection” techniques employed by container implementations are a necessary path for future research, since even virtual machines depend on those techniques to protect themselves. Hosseinzadeh et al. [95] explored the possibility that container implementationsmight directly adapt earlier work (primarily Berger et al. [30]) for virtual machine implementations to integrate a Trusted Platform Module (TPM) as a virtual device.

Bratus 等人 [37] 指出，容器实现采用的 “自我保护” 技术是未来研究的必要途径，因为甚至虚拟机也依赖于这些技术来保护自己。Hosseinzadeh 等人 [95] 探讨了容器实现可能直接适应虚拟机实现的早期工作 (主要是 Berger 等人 [30]) 以集成可信平台模块 (TPM) 的可能性作为虚拟设备。

Container implementations have a potential advantage over virtual machine implementations in addressing the problem of secure isolation over the long-term, not because any existing implementations are inherently superior but because containers take a modular approach to implementation that permits them to be more flexible over time and across different underlying software and hardware architectures, as new ideas for secure isolation evolve.

容器实现在解决长期安全隔离问题方面比虚拟机实现具有潜在优势不是因为任何现有的实现本质上都是优越的，而是因为容器采用模块化的实现方法，允许它们随着时间的推移和跨不同的底层软件和硬件架构更加灵活,随着安全隔离的新想法的发展。

## 8 SECURITY OUTLOOK

## 9 RELATED IMPLEMENTATIONS

Implementation approaches that adopt the label “cloud” [67, 97, 125, 180] are typically virtual machines with added orchestration features to enhance portability. Cloud implementations also tend to favor lighter-weight guest images, which enhances performance and reduces complexity, although cloud images are generally not quite as minimal as container images.

采用“云”标签的实现方法[67、97、125、180]通常是添加了编排功能以增强可移植性的虚拟机。云实现也倾向于更轻量级的客户映像，这可以提高性能并降低复杂性，尽管云映像通常不像容器映像那样最少。

Implementation approaches that adopt the label “unikernel” [117, 129, 212] takeminimalist guest images to an extreme, by replacing the kernel and operating system of the guest with a set of highly optimized libraries that provide the same functionality. The code for an application workload is compiled together with the small subset of unikernel libraries required by the application, resulting in a very small binary that runs directly as a guest image. Historically, unikernels have sacrificed portability of guest images, by targeting only a limited set of virtual machine implementations as their host, but recent work has begun exploring running unikernels as containers [213]. The unikernel approach also reduces the portability of application code, since unikernel frameworks tend to require the application code to be written in the same language as the unikernel libraries.

采用标签“unikernel”[117,129,212]的实现方法将客户映像发挥到极致，用一组提供相同功能的高度优化的库替换客户的内核和操作系统。应用程序工作负载的代码与应用程序所需的一小部分unikernel库一起编译，生成一个非常小的二进制文件，直接作为客户映像运行。从历史上看，unikernels已经牺牲了客户映像的可移植性，只将有限的一组虚拟机实现作为它们的主机，但是最近的工作已经开始探索将unikernels作为容器来运行[213]。unikernel方法还降低了应用程序代码的可移植性，因为unikernel框架倾向于要求应用程序代码用与unikernel库相同的语言编写。

Implementation approaches that adopt the label “serverless” [17, 93, 106, 113] tend to emphasize portability andminimizing complexity. They rely on the underlying infrastructure—typically some combination of bare metal, virtual machines, and/or containers—for whatever secure isolation and performance they provide.

采用“无服务器”标签的实现方法[17、93、106、113]往往强调可移植性和最小化复杂性。它们依赖底层基础设施(通常是裸机、虚拟机和/或容器的某种组合)来实现它们所提供的任何安全隔离和性能。

## 10 CONCLUSION

A detailed examination of the history of virtual machines and containers reveals that the two have evolved in tandem from the very beginning. It also reveals that both families of technology are facing significant challenges in providing secure isolation for modern multitenant infrastructures. In light of recent vulnerabilities, patching up existing tools is a necessary and valuable activity in the short term but is not sufficient for the long term. In the coming decades, the computing industry as a whole will need to embrace more radical alternatives in both hardware and software. Current researchers and developers can benefit from a deeper understanding of how virtual machines and containers evolved—and the trade-offs made along the way—to make more informed choices for tomorrow, avoid repeating past mistakes, and build on a solid foundation toward new paths of exploration.

