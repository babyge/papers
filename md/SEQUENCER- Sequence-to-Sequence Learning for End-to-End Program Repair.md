# SEQUENCER: Sequence-to-Sequence Learning for End-to-End Program Repair

## Abstract

This paper presents a novel end-to-end approach to program repair based on sequence-to-sequence learning. We devise, implement, and evaluate a technique, called SEQUENCER, for fixing bugs based on sequence-to-sequence learning on source code. This approach uses the copy mechanism to overcome the unlimited vocabulary problem that occurs with big code. Our system is data-driven; we train it on 35,578 samples, carefully curated from commits to open-source repositories. We evaluate SEQUENCER on 4,711 independent real bug fixes, as well on the Defects4J benchmark used in program repair research. SEQUENCER is able to perfectly predict the fixed line for 950/4,711 testing samples, and find correct patches for 14 bugs in Defects4J benchmark. SEQUENCER captures a wide range of repair operators without any domain-specific top-down design.

提出了一种基于序列-序列学习的端到端程序修复方法。我们设计、实现并评估了一种称为“排序器”的技术，该技术基于对源代码的顺序到顺序的学习来修复bug。这种方法使用复制机制来克服大代码中出现的无限词汇表问题。我们的系统是数据驱动的;我们对35,578个样本进行了培训，这些样本都是从提交到开源库中精心挑选出来的。我们在4711个独立的实际bug修复上以及在程序修复研究中使用的缺陷4j基准上评估了音序器。sequalizer能够完美地预测950/ 4711测试样本的固定行数，并为排错测试中的14个bug找到正确的补丁。音序器无需任何领域特定的自顶向下设计，就可以捕获广泛的修复操作符。

## 1 INTRODUCTION

PEOPLE have long dreamed of machines capable of writing computer programs by themselves. Having machines writing a full software system is science-fiction but teaching machines to modify an existing program to fix a bug is within the reach of current software technology; this is called automated program repair [1].

人们一直梦想着能自己编写计算机程序的机器。让机器编写完整的软件系统是科幻小说，但教机器修改现有程序以修复bug是当前软件技术所能做到的;这就是所谓的自动程序修复[1]。

Program repair research is very active and dominated by techniques based on static analysis (e.g., Angelix [2]) and dynamic analysis (e.g., CapGen [3]). While great progress has been achieved, the current state of automated program repair is limited to simple small fixes, mostly one line patches [3], [4]. These techniques are heavily top-down, based on intelligent design and domain-specific knowledge about bug fixing in a given language or a specific application domain. In this paper, we also focus on one line patches, but we aim at doing program repair in a language agnostic generic manner, fully relying on machine learning to capture syntax and grammar rules and produce wellformed, compilable programs. By taking this approach, we aim to provide a foundation for connecting program repair and machine learning, allowing the program repair community to benefit from training with more complete bug datasets and continued improvements to machine learning algorithms and libraries.

程序修复研究非常活跃，并以基于静态分析(例如，Angelix[2])和动态分析(例如，CapGen[3])的技术为主导。虽然已经取得了很大的进展，但目前的自动程序修复状态仅限于简单的小修复，主要是一行补丁[3]、[4]。这些技术很大程度上是自顶向下的，基于智能设计和特定领域的知识，在给定的语言或特定的应用程序领域中修复错误。在本文中，我们也关注单行补丁，但我们的目标是以一种语言无关的通用方式进行程序修复，完全依靠机器学习来捕获语法和语法规则，并生成格式良好的可编译程序。通过采用这种方法，我们的目标是为程序修复和机器学习之间的连接提供一个基础，使程序修复社区能够受益于使用更完整的错误数据集进行的培训，以及对机器学习算法和库的持续改进。

As the foundation for our model, we apply sequence-to-sequence learning [5] to the problem of program repair. Sequence-to-sequence learning is a branch of statistical ma-chine learning, mostly used for machine translation: the algorithm learns to translate text from one language (say French) to another language (say Swedish) by generalizing over large amounts of sentence pairs from French to Swedish. The training data comes from the large amount of text already translated by humans, starting with the Rosetta stone written in 196 BC [6]. The name of the technique is explicit: it is about learning to translate from one sequence of words to another sequence of words.

作为模型的基础，我们将序列-序列学习[5]应用于程序修复问题。序列-序列学习是统计机器学习的一个分支，主要用于机器翻译:该算法通过将大量的句子对从法语到瑞典语进行归纳，学习如何将文本从一种语言(如法语)翻译成另一种语言(如瑞典语)。训练数据来自大量已经由人类翻译的文本，从公元前196年的罗塞塔石碑开始。这项技术的名称很明确:它是关于学习将一个单词序列翻译成另一个单词序列。

Now let us come back to the problem of programming: we want to learn to ’translate’ from one sequence of program tokens (a buggy program) to a different sequence of program tokens (a fixed program). The training data is readily available: we have millions of commits in opensource code repositories. Yet, we still have major challenges to overcome when it comes to using sequence-to-sequence learning on code: 1) the raw (unfiltered) data is rather noisy; one must deploy significant effort to identify and curate commits that focus on a clear task; 2) contrary to natural language, misuse of rare words (identifiers, numbers, etc) is often fatal in programming languages [7]; in natural language some errors may be tolerable because of the intelligence of the human reader while in programming languages the compiler (or interpreter) is strict 3) in natural language, the dependencies are often in the same sentence (“it” refers to “dog” just before) , or within a couple of sentences, while in programming, the dependencies have a longer range: one may use a variable that has been declared dozens of lines before.

现在让我们回到编程问题上:我们想要学习如何将一个程序标记序列(一个有bug的程序)“转换”为另一个程序标记序列(一个固定的程序)。培训数据随时可用:我们在开放源代码库中有数百万次提交。然而，在对代码使用顺序到顺序的学习时，我们仍然有一些主要的挑战需要克服:1)原始(未经过滤的)数据相当嘈杂;一个人必须部署大量的工作来识别和管理集中在一个明确任务上的提交;2)与自然语言相反，在[7]编程语言中，罕见词(标识符、数字等)的误用往往是致命的;在自然语言,因为一些错误可以接受人类读者的情报在编程语言编译器(或翻译)是严格3)在自然语言中,依赖关系通常是在同一个句子(“它”指的是“狗”之前),或在几个句子,在编程中,依赖有更长时间范围:一个可能使用一个变量被宣告数十行之前。

We are now at a tipping point to address those challenges. First, sequence-to-sequence learning has reached a maturity level, both conceptually and from an implementation point of view, that it can be fed with sequences whose characteristics significantly differ from natural language. Second, there has been great recent progress on using various types of language models on source code [8]. Based on this great body of work, we present our approach to using sequence-to-learning for program repair, which we created to repair real bugs from large open-source projects written in the Java programming language.

我们现在正处于应对这些挑战的转折点。首先，从概念和实现的角度来看，序列到序列的学习已经达到了一个成熟的水平，它可以由与自然语言显著不同的特性组成的序列来提供。其次，最近在源代码[8]上使用各种类型的语言模型方面取得了很大的进展。基于这些大量的工作，我们提出了使用顺序学习进行程序修复的方法，我们创建这些方法是为了修复用Java编程语言编写的大型开源项目中的实际bug。

Our end-to-end program repair approach is called SEQUENCER and it works as follows. First, we focus on oneline fixes: we predict the fixed version of a buggy programming line. For this, we create a carefully curated training and testing dataset of one-line commits. Second, we devise a sequence-to-sequence network architecture that is specifically designed to address the two main aforementioned challenges. To address the unlimited vocabulary problem, we use the copy mechanism [9]; this allows SEQUENCER to predict the fixed line, even if the fix contains a token that was too rare (i.e., an API call that appears only in few cases, or a rare identifier used only in one class) to be considered in the vocabulary. This copy mechanism works even if the fixed line should contain tokens which were not in the training set. To address the dependency problem, we construct abstract buggy context from the buggy class, which captures the most important context around the buggy source code and reduces the complexity of the input sequence. This enables us to capture long range dependencies that are required for the fix.

我们的端到端程序修复方法称为序列器，它的工作原理如下。首先，我们关注一行修复:我们预测有bug的编程行的固定版本。为此，我们创建了一个精心策划的单行提交培训和测试数据集。其次，我们设计了一个顺序到顺序的网络架构，专门用于解决前面提到的两个主要挑战。为了解决无限词汇量的问题，我们使用了复制机制[9];这使得音序器能够预测固定的行，即使修正包含了一个非常罕见的标记(例如。在词汇表中考虑的API调用(仅在少数情况下出现，或仅在一个类中使用的罕见标识符)。这个副本机制即使固定线应该包含令牌没有在训练集,解决依赖性问题,我们构建抽象的车从车类上下文,它捕捉最重要的上下文在车源代码和减少输入序列的复杂性。这使我们能够捕获修复所需的长期依赖项。

We evaluate SEQUENCER in two ways. First, we compute accuracy over 4,711 real one-line commits, curated from three open-source projects. The accuracy is measured by the ability of the system to predict the fixed line exactly as originally crafted by the developer, given as input the buggy file and the buggy line number. Our golden configuration is able to perfectly predict the fix for 950/4,711 (20%) of the testing samples. This sets up a baseline for future research in the field. Second, we apply SEQUENCER to the mainstream evaluation benchmark for program repair, Defects4J. Of the 395 total bugs in Defects4J, 75 have one-line replacement repairs; SEQUENCER generates patches which pass the test suite for 19 bugs and patches which are semantically equivalent to the human-generated patch for 14 bugs. To our knowledge, this is the first report ever on using sequence-to-sequence learning for end-to-end program repair, including validation with test cases.

我们评估音序器有两种方式。首先，我们计算出超过4711个真正的单行提交的准确性，这是由三个开源项目策划的。准确性是通过系统预测固定行的能力来衡量的，它与最初由开发人员设计的完全一样，输入错误文件和错误行号。我们的黄金配置能够完美地预测950/ 4711(20%)测试样本的修复。这为该领域未来的研究奠定了基础。其次，我们将音序器应用于程序修复的主流评估基准——缺陷(defect)。在defect ts4j的395个bug中，有75个是单行替换修复;音序器生成的补丁通过了19个bug的测试套件，而这些补丁在语义上相当于人为生成的14个bug的补丁。据我们所知，这是迄今为止关于使用序列到序列学习进行端到端程序修复(包括使用测试用例进行验证)的第一个报告。

Overall, the novelty of this work is as follows. First, we create and share a unique dataset for evaluating learning techniques on one-line program repair. Second, we report on using the copy mechanism on seq-to-seq learning on source code. Third, on the same buggy input dataset, SEQUENCER is able to produce the correct patch for 119% more samples than the closest related work [10].

我们评估音序器有两种方式。首先，我们计算出超过4711个真正的单行提交的准确性，这是由三个开源项目策划的。准确性是通过系统预测固定行的能力来衡量的，它与最初由开发人员设计的完全一样，输入错误文件和错误行号。我们的黄金配置能够完美地预测950/ 4711(20%)测试样本的修复。这为该领域未来的研究奠定了基础。其次，我们将音序器应用于程序修复的主流评估基准——缺陷(defect)。在defect ts4j的395个bug中，有75个是单行替换修复;音序器生成的补丁通过了19个bug的测试套件，而这些补丁在语义上相当于人为生成的14个bug的补丁。据我们所知，这是迄今为止关于使用序列到序列学习进行端到端程序修复(包括使用测试用例进行验证)的第一个报告。

To sum up:  
- Our key contribution is an approach for fixing bugs based on sequence-to-sequence learning on token sequences. This approach uses the copy mechanism to overcome the unlimited vocabulary problem in source code.  
- We present the construction of an abstract buggy context that leverages code context for patch generation. The input program token sequences are at the level of full classes and capture long-range dependencies in the fix to be written.We implement our approach in a publiclyavailable program repair tool called SEQUENCER.  
- We evaluate our approach on 4,711 real bug fixing tasks. Contrary to the closest related work [10], we do not assume bugs to be in small methods only. Our golden trained model is able to perfectly fix 950/4,711 testing samples. To the best-of-our knowledge, this is the best result reported on such a task at the time of writing this paper [10][11][12].  
- We evaluate our approach on the 75 one-line bugs of Defects4J, which is the most widely used benchmark for evaluating programming repair contributions. SEQUENCER is able to find 2,321 patches for these bugs, 761 compile successfully, 61 are plausible (they pass the full test suite) and 18 are semantically equivalent to the patch written by the human developer.  
- We provide a qualitative analysis of 8 interesting repair operators captured by sequence-to-sequence learning on the considered training dataset.

总结:
- 我们的主要贡献是一个基于令牌序列的序列到序列的学习来修复bug的方法。这种方法使用复制机制来克服源代码中的无限词汇表问题。
- 我们提出了一个抽象的有bug的上下文结构，它利用代码上下文来生成补丁。输入程序令牌序列处于完整类的级别，并在要编写的修复程序中捕获长期依赖项。我们在一个公开可用的程序修复工具中实现了我们的方法，这个工具叫做音序器。
- 我们评估了4711个真正的bug修复任务。与最近的相关工作[10]相反，我们不假设bug只是在小方法中。我们的黄金训练模型能够完美地修复950/ 4711测试样本。据我们所知，这是在撰写这篇论文时关于这类任务所报告的最佳结果。
- 我们评估了我们的方法在25行缺陷中的缺陷，这是最广泛使用的评估编程修复贡献的基准。音序器能够找到2,321个补丁，761个编译成功，61个是可信的(它们通过了完整的测试套件)，18个在语义上等同于人类开发人员编写的补丁。
- 我们提供了一个定性的分析，8个有趣的维修操作员捕获的序列到序列的学习对考虑的训练数据集。

## 2 BACKGROUND ON NEURAL MACHINE TRANSLATION
WITH SEQUENCE-TO-SEQUENCE LEARNING

SEQUENCER is based on the idea of receiving buggy code as input and producing fixed code as output. The concept is similar to neural machine translation where the input is a sequence of words in one language and the output is a sequence in another language. In this section, we provide a brief introduction to neural machine translation (NMT).

音序器是基于接收错误代码作为输入，产生固定代码作为输出的思想。这个概念类似于神经机器翻译，即输入是一种语言的单词序列，输出是另一种语言的单词序列。在本节中，我们将简要介绍神经机器翻译(NMT)。

In neural machine translation, the dominant technique is called “sequence-to-sequence learning”, where “sequence” refers to the sequence of words in a sentence. An early example of a sequence-to-sequence network [5] used a recurrent neural network to read in tokens and to generate an output sequence, as shown in Figure 1. Let us consider that the input tokens are denoted xt, and after receiving all of the input tokens a special token is used. The output tokens are denoted yt, and at training time the output tokens are fed into the network to learn proper generation of the next token. In the following equations, ht is the hidden state of a recurrent neural network, Whx is the weight matrix that computes how the input xt affects the hidden state, Whh is the weight matrix related to recurrence (i.e., how the previous hidden state affects the current hidden state), and Wyh is the weight matrix used to predict which token should be output given the hidden state. All weights are learned with supervised learning and back-propagation:

在神经机器翻译中，占主导地位的技术被称为“顺序-顺序学习”，其中“顺序”指的是句子中单词的顺序。序列到序列网络[5]的早期示例使用递归神经网络来读取令牌并生成输出序列，如图1所示。让我们假设输入令牌表示为xt，在接收到所有输入令牌之后，使用一个特殊的令牌。输出令牌记作yt，训练时将输出令牌输入网络，学习下一个令牌的正确生成。式中，ht为递归神经网络的隐状态，Whx为计算输入xt如何影响隐状态的权矩阵，Whh为与递归(即，前一个隐藏状态如何影响当前的隐藏状态)，Wyh是用来预测给定隐藏状态下应该输出哪个令牌的权值矩阵。所有权值通过监督学习和反向传播进行学习:

A softmax function is then used to turn the yt values into probabilities to choose the most likely token from a learned vocabulary. In this example, one can see how the weight matrices capture the learning of common patterns; after processing the input sequence, the hidden state h encodes the most likely initial token to begin the output and each subsequent ht uses the W matrices to predict the most likely next token given the input as well as preceding tokens just produced in the output. The W matrices thus learn the long range dependencies in the full input.

然后使用一个softmax函数将yt值转换为概率，从已学习的词汇表中选择最可能的令牌。在这个例子中，可以看到权重矩阵是如何捕获常见模式的学习的;在处理输入序列之后，隐藏状态h编码最可能的初始令牌开始输出，随后的每个ht使用W矩阵来预测最可能的下一个令牌以及刚刚在输出中产生的前一个令牌。这样，W矩阵就可以学习整个输入中的长期依赖关系。

A problem with the sequence generation described above is that only tokens which are in the training set are available for output as yt. In the case of natural human language, words such as proper names (e.g., Chicago, Stockholm) may be so rare that they do not appear in the training vocabulary, but those words may be necessary for proper output. One successful approach to overcome the vocabulary problem is to use a copy mechanism [9]. The basic intuition behind this approach is that rare words not available in the vocabulary (i.e., unknown words, referred as ), may be directly copied from the input sentence over to the output translated sentence. This relatively simple idea can be successful in many cases - especially when translating sentences containing proper names - where these tokens can be easily copied over.

上面描述的序列生成的一个问题是，只有训练集中的令牌可以作为yt输出。在人类的自然语言中，像专有名词(如芝加哥、斯德哥尔摩)这样的词可能非常罕见，以至于它们不会出现在训练词汇表中，但这些词可能是正确输出所必需的。解决词汇表问题的一个成功方法是使用一个复制机制[9]。这种方法背后的基本直觉是，词汇表中没有罕见的单词(例如:，把不认识的词，简称为)，可以直接从输入的句子中抄到输出的翻译句子中去。这个相对简单的想法在很多情况下都是成功的——尤其是在翻译含有专有名词的句子时——这些标记很容易被复制。
(旭伦注：看不懂的照抄)

For example, let’s consider the task of translating the following English sentence "The car is in Chicago" to French. Let’s also assume that all the tokens in the sentence are in the vocabulary, except "Chicago". An NMT model might output the following sentence: "La voiture est à ". With a copy mechanism, the model would be able to automatically replace the unknown token with one of the tokens from the input sentence, in this case, "Chicago".

例如，让我们考虑将下面的英语句子“the car is in Chicago”翻译成法语的任务。我们还假设句子中的所有标记都在词汇表中，除了“Chicago”。一个NMT模型可能输出以下句子:“La voiture est a”。使用复制机制，模型将能够用输入语句中的一个令牌自动替换未知的令牌，在本例中是“Chicago”。

The copy mechanism can be particularly relevant for source code, where the size of the vocabulary can be several times the size of a natural language corpus [13]. This results from the fact that developers are not constrained by any vocabulary (e.g., English dictionary) when defining names for variables or methods. This leads to an extremely large vocabulary containing many rare tokens, used infrequently only in specific contexts. Thus, the copy mechanism applied to source code allows a system to generate rare out-ofvocabulary identifier names and numeric values as long as they are somewhere in the input. Furthermore, in natural language, a human recipient may be able to use context to cope with one missing word in an automatically translated sentence. In a programming language, the compiler does not make any semantic inference, and the generation has to be complete. For example, if the code to predict is "if (i < num_cars)", then generating "if (i < int)" is not going to work at all. We discuss the mathematics of the copy mechanism in the context of SEQUENCER in Section 3.3.1. Readers interested in more detail are referred to the work by See et al. [9].

复制机制可能与源代码特别相关，其中词汇表的大小可能是自然语言库[13]的几倍。这是因为在为变量或方法定义名称时，开发人员不受任何词汇表(例如，英语词典)的约束。这导致了一个包含许多稀有标记的非常大的词汇表，这些标记只在特定的上下文中使用。因此，应用于源代码的复制机制允许系统生成罕见的词汇表外标识符名称和数值，只要它们位于输入中的某个位置。此外，在自然语言中，人类接受者可能能够使用上下文来处理一个自动翻译的句子中缺少的单词。在编程语言中，编译器不进行任何语义推理，生成必须是完整的。例如，如果要预测的代码是“if (i < num_cars)”，那么生成“if (i < int)”就根本不起作用。我们在第3.3.1节中讨论了音序器上下文中的复制机制的数学。对更多细节感兴趣的读者可以参考See等人的著作。

Tufano et al. [10] proposed using NMT with the goal of learning bug-fixing patches by translating the entire buggy method into the corresponding fixed method. Before the translation, the authors perform a code abstraction process which transforms the source code into an abstracted version, which contains: (i) Java keywords and identifiers; (ii) frequent identifiers and literals (a selection of 300 idioms); (iii) typified IDs (e.g., METHOD_1, VAR_2) that replace identifiers and literals in the code. In Section 6 we highlight differences and improvements introduced in SEQUENCER.

复制机制可能与源代码特别相关，其中词汇表的大小可能是自然语言库[13]的几倍。这是因为在为变量或方法定义名称时，开发人员不受任何词汇表(例如，英语词典)的约束。这导致了一个包含许多稀有标记的非常大的词汇表，这些标记只在特定的上下文中使用。因此，应用于源代码的复制机制允许系统生成罕见的词汇表外标识符名称和数值，只要它们位于输入中的某个位置。此外，在自然语言中，人类接受者可能能够使用上下文来处理一个自动翻译的句子中缺少的单词。在编程语言中，编译器不进行任何语义推理，生成必须是完整的。例如，如果要预测的代码是“if (i < num_cars)”，那么生成“if (i < int)”就根本不起作用。我们在第3.3.1节中讨论了音序器上下文中的复制机制的数学。对更多细节感兴趣的读者可以参考See等人的著作。

Tufano et al. [10] proposed using NMT with the goal of learning bug-fixing patches by translating the entire buggy method into the corresponding fixed method. Before the translation, the authors perform a code abstraction process which transforms the source code into an abstracted version, which contains: (i) Java keywords and identifiers; (ii) frequent identifiers and literals (a selection of 300 idioms); (iii) typified IDs (e.g., METHOD_1, VAR_2) that replace identifiers and literals in the code. In Section 6 we highlight differences and improvements introduced in SEQUENCER.

Tufano等人[10]提出使用NMT，目的是通过将整个bug方法转换为相应的固定方法来学习bug修复补丁。在转换之前，作者执行一个代码抽象过程，该过程将源代码转换为一个抽象版本，该版本包含:(i) Java关键字和标识符;(ii)经常使用的标识符和文字(300个习语);(iii)替换代码中的标识符和文字的类型化id(如METHOD_1、VAR_2)。在第6节中，我们重点介绍了音序器中的差异和改进。

Another approach to addressing the vocabulary size problem in code is to use byte pair encoding (BPE), which has been widely used in NLP and also applied to source code [14]. For SEQUENCER, we did preliminary experiments with BPE to solve the unlimited vocabulary problem, but our early results showed that it is less effective than the copy mechanism.

解决代码中词汇表大小问题的另一种方法是使用字节对编码 (BPE)，它已在 NLP 中广泛使用，也应用于源代码 [14]。对于音序器，我们用 BPE 做了初步的实验来解决无限词汇量的问题，但是我们早期的结果表明它比复制机制的效果差。

## 3 APPROACH TO USING SEQ-TO-SEQ LEARNING FOR REPAIR

SEQUENCER is a sequence-to-sequence deep learning model that aims at automatically fixing bugs by generating oneline patches (i.e., the bug can be fixed by replacing a single buggy line with a single fixed line). We do not consider line deletion because: 1) it does not require a method for token generation (and is thus less interesting to our research) and 2) if desired, SEQUENCER could be combined with the lightweight Kali [11] to include line deletion. We do not consider line addition because spectrum based fault localization, used in most of the related work, is not effective for line addition patches [15]. We note that in 64% of all 395 bugs in Defects4J are fixed by replacing existing source code [16]. Given a Software System with a faulty behavior (i.e., failing test case), state-of-the-art fault localization techniques are used to identify the buggy method and the suspicious buggy lines. Such techniques have been shown to predict the correct buggy line as one of the top 10 candidates in 44% of the time [15]. SEQUENCER then performs a novel Buggy Context Abstraction (Section 3.2) process which intelligently organizes the fault localization data (i.e., buggy classes, methods, and lines) into a representation that is concise and suitable for the deep learning model yet able to preserve valuable information regarding the context of the bug, which will be used to predict the fix. The representation is then fed to a trained sequence-to-sequence model (Section 3.3.1) which performs Patch Inference (Section 3.4) and is capable of generating multiple single-lines of code that represent the potential one-line patches for the bug. Finally, SEQUENCER in the Patch Preparation (Section 3.5) step generates the concrete patches by formatting the code and replacing the suspicious line with the proposed lines. Figure 2 shows the aforementioned steps both for the training phase (left) and inference phase (right). In the remainder of this section we will discuss the common steps as well as those specific for training and inference.

序列发生器是一个序列到序列的深度学习模型，旨在通过生成一行补丁来自动修复错误 (i。e.,这个错误可以通过用一条固定的线替换一条错误线来修复)。我们不考虑行删除，因为: 1) 它不需要令牌生成的方法 (因此对我们的研究来说不那么有趣)，2) 如果需要,音序器可以与轻量级 Kali [11] 相结合，包括线路删除。我们不考虑线路添加，因为在大多数相关工作中使用的基于频谱的故障定位对于线路添加补丁 [15] 无效。我们注意到，在 Defects4J 的所有 64% 个错误中，有 395 是通过替换现有的源代码 [16] 来修复的。给定具有错误行为的软件系统 (i.e.,失败的测试案例)，最先进的故障定位技术被用于识别越野车方法和可疑的越野车线路。这种技术已经被证明可以预测正确的越野车线，成为 44% [15] 的前 10 名候选人之一。排序器然后执行一个新颖的越野车上下文抽象 (第 3.2 节) 过程，该过程智能地组织故障定位数据 (i e.,错误的类、方法和行) 成为一个表示，该表示简洁且适合深度学习模型，但能够保存有关 bug 上下文的有价值的信息,将用于预测修复。然后，该表示被馈送到经过训练的序列到序列模型 (第 3.3.1 节)，该模型执行补丁推理 (第 3.4 节) 并且能够生成多个代表 bug 潜在单行补丁的单行代码。最后，补丁准备 (第 3.5 节) 步骤中的定序器通过格式化代码并用建议的行替换可疑行来生成具体的补丁。图 2 显示了训练阶段 (左) 和推理阶段 (右) 的上述步骤。在本节的剩余部分，我们将讨论常见的步骤以及特定于训练和推理的步骤。

[11] Z. Qi, F. Long, S. Achour, and M. Rinard, “An analysis of patch plausibility and correctness for generate-andvalidate patch generation systems,” in Proceedings of the 2015 International Symposium on Software Testing and Analysis, ser. ISSTA 2015, Baltimore, MD, USA: ACM, 2015, pp. 24–36, ISBN: 978-1-4503-3620-8. DOI: 10.1145/ 2771783.2771791. [Online]. Available: http://doi.acm.org/10.1145/2771783.2771791.

[15] D. Zou, J. Liang, Y. Xiong, M. D. Ernst, and L. Zhang, “An empirical study of fault localization families and their combinations,” IEEE Transactions on Software Engineering, 2019.


