# Language Models are Unsupervised Multitask Learners

## Abstract

Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.

自然语言处理任务，如问题回答、机器翻译、阅读理解和总结，通常是通过在特定任务数据集上的监督学习来完成的。我们证明了，当语言模型在一个叫做WebText的数百万网页的新数据集上训练时，可以在没有任何明确监督的情况下开始学习这些任务。当以文档和问题为条件时，语言模型生成的答案在CoQA数据集上达到55 F1—在不使用127,000+训练示例的情况下匹配或超过4个基线系统中的3个。语言模型的能力对于零起点任务转移的成功至关重要，提高它可以以对数线性的方式提高任务之间的性能。我们最大的模型，GPT-2，是一个1.5B的参数转换器，在零镜头设置下，在8个测试的语言建模数据集中，有7个获得了最先进的结果，但仍然不适合WebText。模型中的样本反映了这些改进，并包含了连贯的文本段落。这些发现为构建语言处理系统提供了一条有希望的道路，该系统可以从自然发生的演示中学习执行任务。

## 1. Introduction

Machine learning systems now excel (in expectation) at tasks they are trained for by using a combination of large datasets, high-capacity models, and supervised learning (Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei et al., 2016). Yet these systems are brittle and sensitive to slight changes in the data distribution (Recht et al., 2018) and task specification (Kirkpatrick et al., 2017). Current systems are better characterized as narrow experts rather than competent generalists. We would like to move towards more general systems which can perform many tasks – eventually without the need to manually create and label a training dataset for each one.

机器学习系统现在通过使用大数据集、高容量模型和监督学习的组合，在它们训练的任务中表现出色(在预期中)(Krizhevsky et al.， 2012) (Sutskever et al.， 2014) (Amodei et al.， 2016)。然而，这些系统对于数据分布(Recht et al.， 2018)和任务规范(Kirkpatrick et al.， 2017)中的细微变化是脆弱和敏感的。当前的系统被更好地描述为狭隘的专家，而不是称职的通才。我们希望向能够执行许多任务的更通用的系统发展——最终不需要为每个任务手动创建和标记训练数据集。

The dominant approach to creating ML systems is to collect a dataset of training examples demonstrating correct behavior for a desired task, train a system to imitate these behaviors, and then test its performance on independent and identically distributed (IID) held-out examples. This has served well to make progress on narrow experts. But the often erratic behavior of captioning models (Lake et al., 2017), reading comprehension systems (Jia & Liang, 2017), and image classifiers (Alcorn et al., 2018) on the diversity and variety of possible inputs highlights some of the shortcomings of this approach.

创建ML系统的主要方法是收集一组训练示例，这些示例演示了所需任务的正确行为，训练系统模仿这些行为，然后在独立且同分布(IID)的保留示例上测试其性能。这有助于在狭隘的专家方面取得进展。但是，字幕模型(Lake等，2017)、阅读理解系统(Jia & Liang, 2017)和图像分类器(Alcorn等，2018)在可能输入的多样性和多样性上的不稳定行为，突显了这种方法的一些缺点。

Our suspicion is that the prevalence of single task training on single domain datasets is a major contributor to the lack of generalization observed in current systems. Progress towards robust systems with current architectures is likely to require training and measuring performance on a wide range of domains and tasks. Recently, several benchmarks have been proposed such as GLUE (Wang et al., 2018) and decaNLP (McCann et al., 2018) to begin studying this.

我们怀疑，在单个域数据集上流行的单一任务训练是当前系统缺乏泛化的主要原因。使用当前体系结构的健壮系统的进展可能需要在广泛的领域和任务上进行培训和测量性能。最近，一些基准被提出，如GLUE (Wang et al.， 2018)和decaNLP (McCann et al.， 2018)开始研究这个。

Multitask learning (Caruana, 1997) is a promising framework for improving general performance. However, multitask training in NLP is still nascent. Recent work reports modest performance improvements (Yogatama et al., 2019) and the two most ambitious efforts to date have trained on a total of 10 and 17 (dataset, objective) pairs respectively (McCann et al., 2018) (Bowman et al., 2018). From a meta-learning perspective, each (dataset, objective) pair is a single training example sampled from the distribution of datasets and objectives. Current ML systems need hundreds to thousands of examples to induce functions which generalize well. This suggests that multitask training many need just as many effective training pairs to realize its promise with current approaches. It will be very difficult to continue to scale the creation of datasets and the design of objectives to the degree that may be required to brute force our way there with current techniques. This motivates exploring additional setups for performing multitask learning.

多任务学习(Caruana, 1997)是一种很有前途的提高综合性能的框架。然而，多任务训练在NLP中还处于起步阶段。最近的工作报告显示了适度的性能改进(Yogatama et al.， 2019)和迄今为止最雄心勃勃的两项工作分别针对10对和17对(数据集，目标)(McCann et al.， 2018) (Bowman et al.， 2018)。从元学习的角度来看，每个(数据集、目标)对都是从数据集和目标的分布中采样的单个训练示例。目前的毫升系统需要数百至数千个例子来归纳归纳功能。这表明，多任务训练需要许多有效的训练对，以实现其承诺与目前的方法。要想继续扩大数据集的创建和目标的设计，达到用现有技术强行达到的程度，将是非常困难的。这激发了对执行多任务学习的额外设置的探索。

The current best performing systems on language tasks utilize a combination of pre-training and supervised finetuning. This approach has a long history with a trend towards more flexible forms of transfer. First, word vectors were learned and used as inputs to task-specific architectures (Mikolov et al., 2013) (Collobert et al., 2011), then the contextual representations of recurrent networks were transferred (Dai & Le, 2015) (Peters et al., 2018), and recent work suggests that task-specific architectures are no longer necessary and transferring many self-attention blocks is sufficient (Radford et al., 2018) (Devlin et al., 2018).

目前在语言任务方面表现最好的系统是结合了培训前的培训和监督下的微调。这一方法历史悠久，其趋势是更灵活的转让形式。首先,词作为输入向量学习和使用特定于任务的架构(Mikolov et al ., 2013) (Collobert et al ., 2011),然后网络转移复发的上下文表示(戴&勒,2015)(Peters等人,2018),和最近的研究表明,特定于任务架构不再是必要和转移许多self-attention块就足够了(雷德福et al ., 2018) (Devlin et al ., 2018)。

These methods still require supervised training in order to perform a task. When only minimal or no supervised data is available, another line of work has demonstrated the promise of language models to perform specific tasks, such as commonsense reasoning (Schwartz et al., 2017) and sentiment analysis (Radford et al., 2017).

为了完成一项任务，这些方法仍然需要监督训练。当只有极少的或没有监督数据可用时，另一个领域的研究表明，语言模型有望执行特定的任务，如常识推理(Schwartz et al.， 2017)和情感分析(Radford et al.， 2017)。

In this paper, we connect these two lines of work and continue the trend of more general methods of transfer. We demonstrate language models can perform down-stream tasks in a zero-shot setting – without any parameter or architecture modification. We demonstrate this approach shows potential by highlighting the ability of language models to perform a wide range of tasks in a zero-shot setting. We achieve promising, competitive, and state of the art results depending on the task.

在本文中，我们将这两种工作联系起来，并延续了更一般的转移方法的趋势。我们证明了语言模型可以在零起点设置下执行下游任务——不需要任何参数或架构修改。我们通过强调语言模型在零镜头设置下执行广泛任务的能力来展示这种方法的潜力。我们根据任务实现有希望的、有竞争力的和最先进的结果。

## 2. Approach

At the core of our approach is language modeling. Language modeling is usually framed as unsupervised distribution estimation from a set of examples $(x_1, x_2, ..., x_n)$ each composed of variable length sequences of symbols $(s_1, s_2, ..., s_n)$. Since language has a natural sequential ordering, it is common to factorize the joint probabilities over symbols as the product of conditional probabilities (Jelinek & Mercer, 1980) (Bengio et al., 2003):

$p(x)=\prod_{i=1}^n p(s_n|s_1,...,s_{n-1})$

This approach allows for tractable sampling from and estimation
of p(x) as well as any conditionals of the form
p(sn􀀀k; :::; snjs1; :::; sn􀀀k􀀀1). In recent years, there have
been significant improvements in the expressiveness of models
that can compute these conditional probabilities, such as
self-attention architectures like the Transformer (Vaswani
et al., 2017).