# Multi-Task Deep Neural Networks for Natural Language Understanding

## Abstract

In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MTDNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of
nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement). We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. The code and pre-trained models are publicly available at https://github.com/namisan/mt-dnn.

在本文中，我们提出了一个多任务深度神经网络 (MT-DNN)，用于跨多个自然语言理解 (NLU) 任务学习表示。MT-DNN 不仅利用了大量的跨任务数据，而且还受益于正规化效应，这种效应会导致更通用的表示，以帮助适应新的任务和域。MTDNN 扩展了 Liu 等人 (2015) 中提出的模型，通过结合一个预先训练的双向变压器语言模型，称为 BERT (Devlin 等人，2018)。MT-DNN 获得了十个 NLU 任务的最新结果，包括 SNLI 、 SciTail 和九个胶水任务中的八个, 将胶水基准推至 82.7% (绝对改善 2.2%)。我们还使用 SNLI 和 SciTail 数据集证明，通过 MT-DNN 学习的表示允许域适应，域内标签比预训练的 BERT 表示少得多。代码和预训练模型在 https://github.com/namisan/mt-dnn 。

## 1 Introduction

Learning vector-space representations of text, e.g., words and sentences, is fundamental to many naturallanguage understanding (NLU) tasks. Two popular approaches are multi-task learning and language model pre-training. In this paper we combine the strengths of both approaches by proposing a new Multi-Task Deep Neural Network (MT-DNN).

学习文本的向量空间表示，例如单词和句子，是许多自然语言理解 (NLU) 任务的基础。两种流行的方法是多任务学习和语言模型预训练。在本文中，我们结合了这两种方法的优点，提出了一种新的多任务深度神经网络 (MT-DNN)。

Multi-Task Learning (MTL) is inspired by human learning activities where people often apply the knowledge learned from previous tasks to help learn a new task (Caruana, 1997; Zhang and Yang, 2017). For example, it is easier for a person who knows how to ski to learn skating than the one who does not. Similarly, it is useful for multiple (related) tasks to be learned jointly so that the knowledge learned in one task can benefit other tasks. Recently, there is a growing interest in applying MTL to representation learning using deep neural networks (DNNs) (Collobert et al., 2011; Liu et al., 2015; Luong et al., 2015; Xu et al., 2018; Guo et al., 2018; Ruder12 et al., 2019) for two reasons. First, supervised learning of DNNs requires large amounts of task-specific labeled data, which is not always available. MTL provides an effective way of leveraging supervised data from many related tasks. Second, the use of multi-task learning profits from a regularization effect via alleviating overfitting to a specific task, thus making the learned representations universal across tasks.

多任务学习 (MTL) 是受人类学习活动的启发，在这些活动中，人们经常应用从以前的任务中学到的知识来帮助学习新的任务 (Caruana，1997; 张和杨, 2017)。例如，对于一个知道如何滑雪的人来说，学习滑冰比不知道的人更容易。同样，联合学习多个 (相关) 任务是有用的，这样在一个任务中学习的知识就可以有益于其他任务。最近，使用深度神经网络 (DNNs) 将 MTL 应用于表示学习的兴趣越来越大 (Collobert 等人，2011; Liu 等人，2015; Luong 等人，2015; 徐等人，2018; 郭等人，2018; Ruder12 等人，2019) 有两个原因。首先，DNNs 的监督学习需要大量特定于任务的标记数据，这并不总是可用的。MTL 提供了一种有效的方法来利用来自许多相关任务的监督数据。其次，多任务学习的使用通过减轻对特定任务的过度适应而受益于正则化效应，从而使学习的表示在任务之间具有普遍性。

In contrast to MTL, language model pretraining has shown to be effective for learning universal language representations by leveraging large amounts of unlabeled data. A recent survey is included in Gao et al. (2018). Some of the most prominent examples are ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2018). These are neural network language models trained on text data using unsupervised objectives. For example, BERT is based on a multi-layer bidirectional Transformer, and is trained on plain text for masked word prediction and next sentence prediction tasks. To apply a pre-trained model to specific NLU tasks, we often need to fine-tune, for each task, the model with additional task-specific layers using task-specific training data. For example, Devlin et al. (2018) shows that BERT can be fine-tuned this way to create state-of-the-art models for a range of NLU tasks, such as question answering and natural language inference.

与 MTL 相比，语言模型预训练通过利用大量未标记数据来学习通用语言表示是有效的。最近的一项调查载于 Gao 等人 (2018)。一些最突出的例子是 ELMo (Peters 等人，2018) 、 GPT (Radford 等人，2018) 和 BERT (Devlin 等人，2018)。这些是使用无监督目标在文本数据上训练的神经网络语言模型。例如，BERT 基于多层双向变压器，并在纯文本上进行训练，以进行蒙面单词预测和下一句预测任务。要将预先训练的模型应用于特定的 NLU 任务，我们通常需要使用任务特定的训练数据对每个任务的模型进行微调，并添加额外的任务特定层。例如，Devlin 等人 (2018) 表明，BERT 可以通过这种方式进行微调，为一系列 NLU 任务创建最先进的模型, 如问答和自然语言推理。

We argue that MTL and language model pretraining are complementary technologies, and can be combined to improve the learning of text representations to boost the performance of various NLU tasks. To this end, we extend the MT-DNN model originally proposed in Liu et al. (2015) by incorporating BERT as its shared text encoding layers. As shown in Figure 1, the lower layers (i.e., text encoding layers) are shared across all tasks, while the top layers are task-specific, combining different types of NLU tasks such as single-sentence classification, pairwise text classification, text similarity, and relevance ranking. Similar to the BERT model, MT-DNN can be adapted to a specific task via fine-tuning. Unlike BERT, MT-DNN uses MTL, in addition to language model pre-training, for learning text representations.

我们认为 MTL 和语言模型预训练是互补的技术，可以结合起来提高文本表示的学习，以提高各种自然语言处理任务的性能。为此，我们扩展了 Liu 等人 (2015) 最初提出的 MT-DNN 模型，将 BERT 作为其共享文本编码层。如图 1 所示，下层 (即文本编码层) 在所有任务中共享，而顶层是特定于任务的, 结合不同类型的 NLU 任务，如单句分类、成对文本分类、文本相似性和相关性排名。类似于 BERT 模型，MT-DNN 可以通过微调来适应特定的任务。与 BERT 不同，MT-DNN 除了语言模型预训练之外，还使用 MTL 来学习文本表示。

MT-DNN obtains new state-of-the-art results on eight out of nine NLU tasks 2 used in the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018), pushing the GLUE benchmark score to 82.7%, amounting to 2.2% absolute improvement over BERT.We further extend the superiority of MT-DNN to the SNLI (Bowman et al., 2015a) and SciTail (Khot et al., 2018) tasks. The representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. For example, our adapted models achieve the accuracy of 91.6% on SNLI and 95.0% on SciTail, outperforming the previous state-ofthe-art performance by 1.5% and 6.7%, respectively. Even with only 0.1% or 1.0% of the original training data, the performance of MT-DNN on both SNLI and SciTail datasets is better than many existing models. All of these clearly demonstrate MT-DNN’s exceptional generalization capability via multi-task learning.

MT-DNN 在通用语言理解评估 (GLUE) 基准 (Wang 等人，2018) 中使用的九个 NLU 任务 2 中，获得了八个新的最新结果, 将胶水基准得分提高到 82.7%，相当于比 BERT 提高了 2.2%。我们进一步将 MT-DNN 的优势扩展到 SNLI (Bowman 等人,2015a) 和 SciTail (Khot 等人，2018) 任务。通过 MT-DNN 学习的表示允许域适应，域内标签比预先训练的 BERT 表示少得多。例如，我们的适应模型在 SNLI 和 SciTail 上的精度分别为 91.6% 和 95.0%，分别比以前最先进的性能高出 1.5% 和 6.7%。即使只有 0.1% 或 1.0% 的原始训练数据，MT-DNN 在 SNLI 和 SciTail 数据集上的性能也优于许多现有模型。所有这些都清楚地展示了 MT-DNN 通过多任务学习的卓越泛化能力。
