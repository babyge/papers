# 深度强化学习：原理与实践

## 强化学习概论

1956年，Bellman提出动态规划方法
1977年，Werbos提出自适应动态规划方法
1988年，Sutton提出时间差分算法
1992年，Watkins提出Q-Learning算法
1994年，Rummery提出Saras算法
1996年，Bersekas提出解决随机过程中优化控制的神经动态规划方法
2006年，Kocsis提出置信上限树算法
2009年，Kewis提出反馈控制自适应动态规划算法
2014年，Sliver提出确定性策略梯度Policy Gradents算法
2015年，Google DeepMind提出Deep Q-Network算法

## 数学基础及环境

## 动态规划法

## 蒙特卡洛法

蒙特卡洛这一名字来源于摩纳哥的城市Monte Carlo。该方法由著名的美国计算机科学家冯诺依曼和S.M.乌拉姆在20世纪40年代研究原子弹的曼哈顿时首先提出。
蒙特卡洛法是一种基于彩样的算法名称，依靠重复随机采样来获得数值结果的计算方法，其核心理念是使用随机性来解决原则上为确定性的问题。通俗而言，蒙特卡洛法采样越多，结果就越近似最优解，即通过多次采样逼近最优解。
蒙特卡洛法能够处理无模型任务，究其原因是无须依赖环境的完备知识，只需收集从环境中进行采样得到的经验轨迹(experience episode)，基于经验轨迹集数据的计算，可求解最优策略。

## 时间差分法

时间差分法利用智能体在环境中时间步之间的时序差，学习由时间间隔产生的差分数据求解强化学习任务。另外，时间差分法结合了动态规划法和蒙特卡洛法的优点，能够更准确、高效地求解强化学习任务，是目前强化学习求解的主要方法。
时间差分控制主要有两种算法：固定策略的sarsa算法和非固定策略的Q-Learning算法。

### 时间差分概述

时间差分法与蒙特卡洛法类似，都是基于采样数据估计当前价值函数。与蒙特卡洛法不同的是，时间差分法采用动态规划法中自举方式计算当前价值函数，而蒙特卡洛是在每次实验结束之后才能计算相应的价值函数。
“自举”表示在当前值函数的计算过程中，会利用到后续的状态值函数或动作值函数，即利用到后续的状态或者<状态-动作>对。

时间差分法主要基于时间序列分析的差分数据进行学习，其分为固定策略on-policy和非固定策略off-policy两种。固定策略时间差分法的代表算法为sarsa算法，非固定策略时间差分法的代表算法为Q-learning算法。

### 时间差分预测

#### 时间差分预测原理

蒙特卡洛法对多次采样后经验轨迹的奖励进行平均，并将平均后的奖励作为累积奖励$G_t$的近似期望。需要特别注意的是，累积奖励的平均计算是在一个经验轨迹收集完成之后开展。式(5.1)为蒙特卡洛中状态值的具体更新过程。

$v(s_t)\leftarrow v(s_t) + \alpha[G_t - v(s_t)]$ （5.1）

式(5.1)利用实际的奖励(Actual Return)$G_t$作为目标Target来更新状态值，并且状态值的更新过程能够增量式地进行。其中$\alpha$为学习率，$G_t$为执行了t个时间步后的实际奖励，是基于某一策略状态值的无偏估计。

在时间差分学习中，算法在估计某一状态值时，使用关于该状态的即时奖励$r_{t+1}$和下一时间步的状态值$v(s_{t+1})$乘以衰减系数$\gamma$进行更新，最简单的时间差分法称为TD(0)，其具体更新过程如式(5.2)所示：

$v(s_t)\leftarrow v(s_t) + \alpha[r_{t+1}+\gamma v(s_{t+1})-v(s_t)]$

其中，
$r_{t+1}+\gamma v(s_{t+1})$为时间差分目标TD Target，表示预测的实际奖励
$\delta_t = r_{t+1}+\gamma v(s_{t+1})-v(s_t)$，用于状态值函数的估计。

另外，关于时间差分目标，主要分为以下两种：
- 普通时间差分目标：即$r_{t+1}+\gamma v(s_{t+1})$，基于下一状态的预测值计算当前奖励预测值，是当前状态实际价值的有偏估计
- 真实时间差分目标，$r_{t+1}+\gamma v_{\pi}(s_{t+1})$，基于下一时间步状态的实际价值计算当前奖励预测值，是当前状态实际价值的无偏估计
  
#### TD算法

### 时间差分控制Sarsa算法

#### Sarsa算法原理

Sarsa算法估计的是动作函数q(s,a)，即估计在策略$\pi$下对于任意状态s上所有可能执行动作a的动作值函数$q_{\pi}(s,a)$

$q(s_t,a_t)\leftarrow q(s_t,a_t) + \alpha[r_{t+1}+\gamma q(s_{t+1},a_{t+1})-q(s_t,a_t)]$

### 时间差分控制Q-learning算法

#### Q-Learning算法原理

$q(s_t,a_t)\leftarrow q(s_t,a_t)+\alpha[r_{t+1}+\lambda max_a q(s_{t+1},a_t)-q(s_t,a_t)]$

Q-Learning算法更新Q值时，时间差分目标使用动作值最大值$max_a q(s_{t+1},a_t)$，并与当前选取动作所使用的策略无关。Sarsa算法只能对给定的策略进行估计，而Q-learning算法选择的动作值Q往往是最优的（偏向于最大值的动作）。

### 期望Sarsa算法

### Double Q-learning算法

## 值函数近似法

在实际应用中，对于状态空间或动作空间都较大的情形，精确获得状态值v(s)或动作值q(s,a)非常困难。此时，可通过寻找状态值函数或动作值函数的近似函数替代对应的原函数。

值函数近似法，是使用梯度下降算法找到目标函数的极小值，并以此设计目标函数来求解近似值函数的权重参数w. 

## 策略梯度法

基于价值的求解算法（如值函数近似法）在实际应用中存在一些不足，如算法难以高效处理连续空间的任务以及最终的求解结果不一定是全局最优解等。

策略梯度法将策略的学习从概率集合P(a|s)变换成策略函数$\pi(a|s)$，并通过求解策略目标函数的极大值，得到最优策略$\pi^*$。这种方法使得智能体能够在不参考价值函数的情况下，直接选择动作，有效地解决了基于价值的强化学习求解方法存在的不足。

## 整合学习与规划

## 深度强化学习

## 深度Q网络

## 深度强化学习算法框架

## 从围棋Alpha Go到Alpha Go Zero
