# When Code Completion Fails: a Case Study on Real-World Completions

## Abstract

Code completion is commonly used by software developers and is integrated into all major IDE’s. Good completion tools can not only save time and effort but may also help avoid incorrect API usage. Many proposed completion tools have shown promising results on synthetic benchmarks, but these benchmarks make no claims about the realism of the completions they test. This lack of grounding in real-world data could hinder our scientific understanding of developer needs and of the efficacy of completion models. This paper presents a case study on 15,000 code completions that were applied by 66 real developers, which we study and contrast with artificial completions to inform future research and tools in this area.We find that synthetic benchmarks misrepresent many aspects of real-world completions; tested completion tools were far less accurate on real-world data.Worse, on the few completions that consumed most of the developers’ time, prediction accuracy was less than 20% – an effect that is invisible in synthetic benchmarks. Our findings have ramifications for future benchmarks, tool design and real-world efficacy: Benchmarks must account for completions that developers use most, such as intra-project APIs; models should be designed to be amenable to intra-project data; and real-world developer trials are essential to quantifying performance on the least predictable completions, which are both most time-consuming and far more typical than artificial data suggests. We publicly release our preprint [https://doi.org/10.5281/zenodo.2565673] and replication data and materials [https://doi.org/10.5281/zenodo.2562249].

代码完成通常被软件开发人员使用，并集成到所有主要的IDE中。好的完成工具不仅可以节省时间和精力，还可以帮助避免错误的API使用。许多已提出的完井工具在综合基准测试中显示出了良好的效果，但这些基准测试并没有对完井的真实性提出任何要求。缺乏真实世界数据的基础可能会妨碍我们对开发人员需求和完成模型的有效性的科学理解。本文通过对66位实际开发人员应用的15,000个代码补全的案例研究，对其进行研究并与人工补全进行对比，为该领域未来的研究和工具提供参考。我们发现，综合基准错误地反映了实际完井的许多方面;测试完的完井工具对真实数据的精确度要低得多。更糟糕的是，在少数消耗了开发人员大部分时间的完井中，预测精度低于20%——这在综合基准测试中是看不见的。我们的发现对未来的基准测试、工具设计和现实世界的效率都有影响:基准测试必须考虑开发人员最常使用的完成率，比如项目内部的api;模型的设计应符合项目内数据的要求;真实世界的开发人员试验对于量化最不可预测的完井效果至关重要，因为完井最耗时，而且比人工数据显示的更典型。我们公开发布我们的预印本[https://doi.org/10.5281/zenodo.2565673]和复制数据和材料[https://doi.org/10.5281/zenodo.2562249]。

## I. INTRODUCTION

Numerous studies have developed tools that can assist programmers in common, but taxing tasks, such as fault localization, program patching, type annotation, and API recommendation [1]–[4]. One of the most clear-cut ways in which tools can reduce the effort of software development for programmers is through code completion. Many tools that suggest plausible completions of the current token, statement, or even function have been proposed [5]–[8] and are part of most Integrated Development Environments (IDEs).

- [1] B. Ray, V. Hellendoorn, S. Godhane, Z. Tu, A. Bacchelli, and P. Devanbu, “On the "naturalness" of buggy code,” in Proceedings of the 38th International Conference on Software Engineering, ser. ICSE ’16. New York, NY, USA: ACM, 2016, pp. 428–439. [Online]. Available: http://doi.acm.org/10.1145/2884781.2884848 
- [2] C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer, “Genprog: A generic method for automatic software repair,” Ieee transactions on software engineering, vol. 38, no. 1, pp. 54–72, 2012. 
- [3] Z. Xu, X. Zhang, L. Chen, K. Pei, and B. Xu, “Python probabilistic type inference with natural language support,” in Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. ACM, 2016, pp. 607–618. 
- [4] A. T. Nguyen and T. N. Nguyen, “Graph-based statistical language model for code,” in Software Engineering (ICSE), 2015 IEEE/ACM 37th IEEE International Conference on, vol. 1. IEEE, 2015, pp. 858–868.
- [5] R. Robbes and M. Lanza, “How program history can improve code completion,” in Proceedings of the 2008 23rd IEEE/ACM International Conference on Automated Software Engineering. IEEE Computer Society, 2008, pp. 317–326. 
- [6] M. Bruch, M. Monperrus, and M. Mezini, “Learning from examples to improve code completion systems,” in Proceedings of the the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering. ACM, 2009, pp. 213–222. 
- [7] S. Proksch, J. Lerch, and M. Mezini, “Intelligent code completion with bayesian networks,” ACM Transactions on Software Engineering and Methodology (TOSEM), vol. 25, no. 1, p. 3, 2015. 
- [8] C. Franks, Z. Tu, P. Devanbu, and V. Hellendoorn, “Cacheca: A cache language model based code suggestion tool,” in Proceedings of the 37th International Conference on Software Engineering - Volume 2, ser. ICSE ’15. Piscataway, NJ, USA: IEEE Press, 2015, pp. 705–708. [Online]. Available: http://dl.acm.org/citation.cfm?id=2819009.2819143

许多研究都开发了一些工具，可以帮助程序员完成常见但繁重的任务，如故障定位、程序补丁、类型注释和API推荐[1]-[4]。对于程序员来说，工具可以减少软件开发工作的最明确的方式之一是通过代码完成。许多建议合理地完成当前令牌、语句甚至函数的工具已经被提出了[5]-[8]，它们是大多数集成开发环境(ide)的一部分。

Nearly without exception, code completion tools are evaluated against synthetic benchmarks, typically produced by adding “holes” to existing code (e.g., removing an identifier) and tasking the model with predicting the correct expression for that location using the remaining context [4], [6], [9]–[12]. Nevertheless, there is reason to be suspicious of synthetic benchmarks: Proksch et al. show that development that happens locally takes place in a very different order and context from what is eventually committed, so that simulations in published code probably do not reflect the developer’s working context [12], [13]. Code completion tool design could greatly benefit from real-world insights; Robbes & Lanza demonstrated this by using observations from a developer study to improve their code completion tool [5].

几乎无一例外，代码补全工具是根据合成基准进行评估的，通常是通过在现有代码中添加“漏洞”(例如，删除标识符)并使用剩余的上下文[4]、[6]、[9]-[12]来预测该位置的正确表达式，从而将模型设置为任务。然而，有理由怀疑合成基准测试:Proksch等人表明，本地发生的开发与最终提交的开发顺序和上下文环境非常不同，因此在已发布的代码中进行的模拟可能不能反映开发人员的工作上下文[12]、[13]。代码完成工具的设计可以极大地受益于现实世界的洞察力;Robbes和Lanza通过使用来自开发人员研究的观察来证明这一点，以改进他们的代码完成工具[5]。

An empirical foundation is needed to establish the characteristics of real-world code completion usage and the objectives that code completion tools should meet to have real-world efficacy. Our case study provides the first step towards such a foundation by analyzing over 15,000 completions that were applied by 66 developers in Visual Studio in a pre-existing dataset. We compare these completions with artificial ones, first in terms of their characteristics and then by replicating them with state-of-the-art code completion models, including structured recommenders, Recurrent Neural Networks, and dynamic n-gram models. We answer the following questions:

需要一个经验基础来建立真实世界的代码完成使用的特征和代码完成工具应该满足的目标，以具有真实世界的效力。我们的案例研究通过分析Visual Studio中66位开发人员在现有数据集中应用的15,000多个完井，为实现这样的基础提供了第一步。我们将这些补全与人工补全进行比较，首先比较它们的特性，然后通过使用最先进的代码补全模型(包括结构化推荐器、递归神经网络和动态n-gram模型)来复制它们。我们回答以下问题:

RQ1: How do code completions in synthetic benchmarks compare to (applied) real-world completions? We find that real-world completions are different from typical artificial ones on both trivial factors, such as primarily addressing tokens with longer names and local variables, and more complex ones, such as often focusing on method invocations and field accesses from within the same project.

RQ1:合成基准中的代码完成率与现实世界中的(应用的)完成率相比如何?我们发现，真实世界中的补全与典型的人工补全在一些琐碎的因素上是不同的，比如主要用更长的名称和局部变量来寻址标记，以及更复杂的一些因素，比如经常关注来自相同项目中的方法调用和字段访问。

RQ2: Do these different characteristics affect code completion models? We show that state-of-the-art models have a much harder time completing real-world queries than those on randomly sampled tokens with similar characteristics. We outline promising factors and outstanding challenges; e.g., models that dynamically integrate local data fare much better than static ones, but intraproject queries remain the hardest category.

RQ2:这些不同的特征会影响代码完成模型吗?我们表明，最先进的模型比那些随机采样的具有相似特征的令牌更难完成真实世界的查询。我们概述了有希望的因素和突出的挑战;例如，动态集成本地数据的模型比静态模型运行得好得多，但是项目内查询仍然是最难的一类。

RQ3: What characteristics of real completions are overlooked in code completion models? Inter alia, our analysis shows that most completions are both applied quickly and fairly predictable, apparently geared at reducing typing effort, but the small remainder (typically involving APIs) consume most of the developers’ time. Existing models perform very poorly on the second category, suggesting that they are optimized for repeating typical patterns but may not provide novel insights.

RQ3:在代码完成模型中忽略了实际完成的哪些特征?除其他外，我们的分析表明，大多数补全都是快速且相当可预测的，显然是为了减少键入工作，但是剩下的小部分(通常涉及api)占用了开发人员的大部分时间。现有的模型在第二种情况下表现很差，这表明它们针对重复典型模式进行了优化，但可能无法提供新的见解。

Our findings inform concrete recommendations for improving code completion benchmarks, tool design, and realworld efficacy. Benchmarks should first and foremost aim to address completions following their frequency of use, which we characterize, showing for example that intra-project API completions are surprisingly relevant. Code completion tools should similarly aim to be flexible, able to integrate local information and be aware of contexts in which they can be (im)precise. Finally, real-world efficacy is increased precisely by focusing on the least accurate predictions in synthetic data: these are far more common and time-consuming in realworld data. Crucially, our work shows that we need more benchmarks of real code completion engines and developer trials to promote impactful research in this field.

我们的发现为改进代码完成基准、工具设计和现实世界的效率提供了具体的建议。基准测试的首要目标应该是在使用频率之后解决完井问题，我们对此进行了描述，并举例说明了项目内部API完成是非常相关的。代码完成工具的目标应该类似地是灵活的，能够集成本地信息，并且能够识别上下文，在上下文中它们可以是(im)精确的。最后，通过关注合成数据中最不准确的预测，可以精确地提高现实世界的效能:这些预测在现实世界的数据中更常见，也更耗时。至关重要的是，我们的工作表明，我们需要更多真实代码完成引擎和开发人员试验的基准，以促进这一领域的有效研究。

## II. EXPERIMENTAL SETUP

Synthetic code completion benchmarks are typically created by taking a complete program and removing a random token, such as an identifier or method invocation [4], [9]–[12], [14]. We want to contrast such synthetic completions with real code completions. To this end, we base our case study on a public dataset containing code completions invoked in Visual Studio and simulate artificial completions on the same data. This section details our data collection and modeling setup. Our processed and replicable code completion data, as well as our model implementations, are publicly available [15].

合成代码完成基准测试通常是通过获取一个完整的程序并删除一个随机令牌(例如标识符或方法调用[4]、[9]-[12]、[14])来创建的。我们希望将这种合成补全与实际代码补全进行对比。为此，我们的案例研究基于一个公共数据集，该数据集包含在Visual Studio中调用的代码补全，并在相同的数据上模拟人工补全。本节详细介绍了我们的数据收集和建模设置。我们处理的和可复制的代码完成数据，以及我们的模型实现，都是公开可用的[15]。

### A. Code Completion Data

Several tools have been developed to track developer as they interact with the Integrated Development Environment (IDE), such as Blaze [16], DFlow [17] and FeedBaG++ [18]. The latter is most appropriate for our purposes; FeedBag++ captures event streams in Visual Studio that include invocations of the code completion tool, as well as events for other actions such as opening new projects, using version control and editing a file. Change related events include snapshots of the file in which the edit/completion took place, stored in the form of simplified syntax trees (SST) – an AST with fully qualified type annotations. We produce our benchmark data by processing a public dataset of developer interactions that were recorded with this tool [19]. In addition, we use a large dataset of C# repositories as the training data [20]. Both datasets have been cleaned and pre-processed as described below to allow replication of a variety of models, including deep learning models, cache-based models, and AST-based models, all of which we include in this work. A full description of the datasets is available in the original papers.

已经开发了一些工具来跟踪开发人员与集成开发环境(IDE)的交互，如Blaze[16]、DFlow[17]和FeedBaG++[18]。后者最适合我们的目的;FeedBag++在Visual Studio中捕获事件流，包括代码完成工具的调用，以及打开新项目、使用版本控制和编辑文件等其他动作的事件。与更改相关的事件包括发生编辑/完成的文件的快照，并以简化语法树(SST)的形式存储—一种具有完全限定类型注释的AST。我们通过处理用这个工具[19]记录的开发人员交互的公共数据集来生成基准数据。另外，我们使用c#知识库的大数据集作为培训数据[20]。如下所述，两个数据集都已被清理和预处理，以允许复制各种模型，包括深度学习模型、基于cache的模型和基于ast的模型，所有这些我们都包括在这项工作中。原始论文中有完整的数据集描述。

1) Benchmark Data: Our benchmark dataset contains event streams from 86 developers (of which 66 had reproducible completions) with varying levels of experience as they interact with C# projects in VisualStudio. This dataset includes many code completion events and general edit events. We extract all completion events and their accompanying code contexts, which contain a syntax tree of the state of the file in which the developer triggered the completion, with a placeholder at the current completion location. We extract the prefix, the characters that were already written towards the completion, and the ultimately selected correct completion. To be compatible with all the models in this study, we only aim to predict the identifier portion (e.g., the method name) of the completion.

基准数据:我们的基准数据集包含来自86个开发人员(其中66个具有可重复完成)的事件流，他们在VisualStudio中与c#项目交互时具有不同级别的经验。该数据集包括许多代码完成事件和一般编辑事件。我们提取所有完成事件及其伴随的代码上下文，其中包含一个语法树，表示开发人员触发完成的文件的状态，在当前完成位置有一个占位符。我们提取前缀、已经向完成写入的字符以及最终选择的正确完成。为了与本研究的所有模型兼容，我们只打算预测完成的标识符部分(例如，方法名)。

Across these developers, we find nearly 200,000 completion events. Of these, ca. 56% were terminated by “filtering” (meaning the developer narrowed down the prefix by typing more characters), 20% were canceled and 24% were ultimately applied. This latter category is most interesting to us, as it presents reproducible completion events. We cannot make strong claims about the reasons for the cancellation events, but it is safe to say that they include at least some cases in which Visual Studio was unable to provide a useful completion (we discuss the ramifications of this in Section IV-A).

在这些开发人员中，我们发现了近200,000个完成事件。其中，约56%通过“过滤”(即开发人员通过输入更多字符来缩小前缀范围)终止，20%被取消，24%最终被应用。后一类是我们最感兴趣的，因为它提供了可重复的完成事件。我们不能对取消事件的原因做出强有力的声明，但是可以肯定的说，它们至少包括了一些Visual Studio无法提供有用的完成的情况(我们将在IV-A部分讨论这种情况的后果)。

Not all completion events contain all the information needed to create reproducible completions. Among the applied cases, ca. 53% did not include locality information about where the completion took place and 32% did not store the selected completion, but rather a generic ‘LookupObject‘ that could not be decoded into the corresponding token after the fact. Accounting for overlap between these categories, we extract 15,245 valid, applied completions belonging to 66 developers for which we can fully replicate the context.

并非所有完井事件都包含创建可重复完井所需的全部信息。在应用的案例中，约53%的案例没有包含关于完成发生在何处的位置信息，32%的案例没有存储所选择的完成，而是一个通用的“LookupObject”，在完成之后不能被解码成相应的令牌。考虑到这些类别之间的重叠，我们提取了15245个有效的、属于66个开发人员的应用补全，我们可以完全复制上下文。

For each completion event, we extract all the surrounding tokens from the stored syntax tree. Ideally, we would also have access to the tokens in the other files in the project, but these are understandably not stored in the dataset. We do have access to any file in which an edit event or completion event takes place, so we approximate the content of the surrounding project by storing the latest snapshots of edited files incrementally, thus creating a restricted view of the project for each developer that improves over time.

对于每个完成事件，我们从存储的语法树中提取所有周围的标记。理想情况下，我们还可以访问项目中其他文件中的令牌，但这些令牌不存储在数据集中是可以理解的。我们访问任何文件中完成一个编辑事件或事件发生,所以我们近似的内容围绕项目存储编辑的最新快照文件增量,从而创建一个限制的项目为每个开发人员改善。

In addition to using the recorded completions as a benchmark, we also simulate a synthetic benchmark on the same data; instead of completing the real missing token, we remove another token at random from the file in which the completion takes place and ask each model to infer it. To allow a more fine-grained comparison, we run this simulation many times per file and store the randomly selected token’s characteristics (e.g., token length, syntactic type); this allows us to approximate several forms of synthetic benchmarks after the fact, specifically: completing all tokens, completing just identifiers, and completing only API calls. Several other types of synthetic data creation have been proposed (e.g., removing sets of related API calls) and may be studied similarly; we selected options representative of most work on code completion.

除了使用所记录的完井数据作为基准外，我们还对相同的数据模拟了一个合成基准;我们没有完成真正丢失的令牌，而是从完成发生的文件中随机删除另一个令牌，并要求每个模型推断它。为了允许更细粒度的比较，我们在每个文件中多次运行这个模拟，并存储随机选择的令牌特征(例如，令牌长度、语法类型);这允许我们在事后近似几种形式的合成基准测试，具体来说:完成所有标记、仅完成标识符和仅完成API调用。已经提出了几种其他类型的合成数据创建(例如，删除相关的API调用集)，可以进行类似的研究;我们选择了代表大多数代码完成工作的选项。

2) Training data: We use a public dataset of 340 C# GitHub repositories, comprising ca. 42M lines of code, to train the different completion tools [20]. This dataset is released with the benchmark data and its code is stored in the same representation (simplified syntax trees). We use a built-in method that comes with the dataset to transform it back into C#, with small adaptations to inline previously nested expressions again and reverse the flattened representations of the simplified syntax trees. This made it easy for us to extract lexical token sequences corresponding to the syntax trees.

培训数据:我们使用公共数据集340 c# GitHub知识库，包括大约42M行的代码，来培训不同的完成工具[20]。该数据集与基准数据一起发布，其代码存储在相同的表示中(简化的语法树)。我们使用数据集自带的内置方法将其转换回c#，同时对之前嵌套的表达式进行了小的调整，并逆转了简化语法树的扁平表示。这使得我们很容易提取与语法树相对应的词汇标记序列。

The resulting token sequences can be used by lexical tokenbased models. To make training with deep learners (see Section II-B) feasible in reasonable time, we select 10% of the files in this dataset at random to create a corpus of 16M tokens. A static vocabulary is estimated on the training data and all events seen less than 10 times are treated as a generic “unknown” token, to produce a vocabulary of 75,913. Both the corpus and the vocabulary are comparable to prior work using deep learners on source code [14] and we use them to train both the n-gram models and deep learners.

结果的令牌序列可以由基于词法令牌的模型使用。To 与 深 培训 学习者 (see Section II-B) 可行 合理 time, 我们 选择 10% 的 文件 在 这个 数据 集 随机 创建 一 个 语料库 的 16M tokens.根据训练数据估计一个静态词汇表，所有出现次数少于10次的事件都被视为一个通用的“未知”标记，从而生成75,913个词汇表。语料库和词汇表都可以与之前使用深度学习者对源代码[14]的工作相媲美，我们使用它们来训练n-gram模型和深度学习者。
